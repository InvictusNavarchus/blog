---
title: "Case Study: Deep Rock Galactic Missions Data"
description: "Analyzing Deep Rock Galactic Daily Missions Data Over July 2024"
author: "invictus"
date: "2024-07-31T08:58:00"
date-modified: "2024-08-05T12:21:00"
image: 'images/preview.png'
draft: false
toc: true
toc-depth: 6
---

## Introduction

> Rocks and stones may break my bones, but beer will never hurt me. ***gee***, ***I'm feeling poetic today***!

Anyone who plays Deep Rock Galactic (DRG) must've heard of that phrase once. It's my personal favorite.

If you're not familiar with the game, DRG is an FPS Sandbox Generated type of game, where you'll play as a miner that lives on space. You'll be diving 1000km to the underground to accomplish a given mission. These missions are randomly generated (or is it?) every 30 minutes.

![](images/548430_20240721100857_1.png)

Interestingly, these missions are the same for all players around the world, as long as you play on the same season.

For every mission, there's a small chance that it'll come with a mutator that changes that nature of the gameplay. One that most sought after is the 'Double XP' mutator, as you can guess, will double the amount of XP you'll obtain.

XP is very important during the early games, because your level will determine what weapons you can unlock and what upgrades you can purchase.

Therefore, one of the reasons that inspired me to conduct this study is to discover the pattern of 'Double XP' mutator. I want to know whether it's completely random or whether I can find out at which hours it usually appear.

I'm a beginner myself in the game, only have about 40 hours gameplay. So, the result of this study will benefit me very much.

### Acknowledgement

This work could've not been done without the massive effort by [rolfosian](https://github.com/rolfosian/), who has written a script to extract the missions data stored on the game. Additionally, he has a site that displays the current active missions updated in real-time, <https://doublexp.net/>. Feel free to check the source code his [github repo](https://github.com/rolfosian/drgmissions).

### Objective

I haven't run it myself on my PC. Fortunately, I discovered that he stores many of the collected missions in json format on the site. Not long after, I wrote simple [python scripts](https://github.com/invictus1032/drg-mission-parser) to download, parse, and convert them to csv.

It was a lot of fun. Now, I want to rewrite the script in R and then finally analyze the data. The objective is to get a full daily missions data over July 2024. And then hopefully to find pattern on 'Double XP'. Additionally, I'll peek on other insights as well, because, why not?

## Collecting Data

Let's fire up our swiss-army knife, `tidyverse`.

```{r}
library(tidyverse)
```

From the rolfosian's project source code, I discovered that the json files are stored on `https://doublexp.net/static/json/bulkmissions/{yyyy-mm-dd}.json.` So let's set that up as the `base_url`

```{r}
json_base_url <- 'https://doublexp.net/static/json/bulkmissions/'
```

The base URL will be used to generate all the download links for each json file. Since the file names are formatted by ISO dates, it's easy to generate using `lubridate`

```{r}
library(lubridate)
start_date <- ymd('2024-07-01')
end_date <- ymd('2024-07-31')
date_list <- seq(start_date, end_date, by = 'day')
```

As simple as that. Now we have a list of date from July 1 to July 31

```{r}
date_list
```

We can simply use `paste0` to put `.json` on them for the extension.

```{r}
filename_list <- paste0(date_list, '.json')
filename_list
```

We can also use `paste0()` to combine them with the base URL to get the download links

```{r}
json_urls <- paste0(json_base_url, filename_list)
```

Now we can use this list to download all the json files from [doublexp.net](https://doublexp.net). Let's put a time recorder on it too, because why not ðŸ˜†.

```{r}
dir.create('json')
library(tictoc)
```

```{r eval=FALSE}
tic('Download all missions json for July 2024')
mapply(download.file, json_urls, paste0('json/', filename_list))
toc()
```

```         
trying URL 'https://doublexp.net/static/json/bulkmissions/2024-07-01.json'
Content type 'application/json' length 261344 bytes (255 KB)
==================================================
downloaded 255 KB
...
https://doublexp.net/static/json/bulkmissions/2024-07-01.json https://doublexp.net/static/json/bulkmissions/2024-07-01.json 
...
Download all missions json for July 2024: 64.759 sec elapsed
```

64 seconds. Not bad. Now let's parse it into a data frame. But first, we need to understand the structure of the JSON.

## Processing Data

#### Parsing one JSON

Due to its unstructured nature, JSON can be messy and hard to parse. Fortunately, the JSON we have here is not too complex. The good thing is, all the JSON are structured the same way. So, we only need to figure out a way to parse one JSON, to parse all of them.

Let's take a look at the JSON file.

```{r}
library(jsonlite)
```

```{r}
json_data <- read_json('./json/2024-07-01.json')
```

```{r}
json_data |> length()
```

```{r}
json_data[c(1:3)] |> glimpse()
```

So, the JSON has 4 levels:

1.  Timestamps
2.  Biomes
3.  The biomes themselves
4.  Mission for each biome (note that the fields under Biomes are also lists)

I've explored the JSON with `View()`, so I got a pretty rough idea of the general structure. I found there are 2 fields with different structure at the end of the lists.

```{r}
json_data[c(49:50)] |> glimpse()
```

The `ver` field is probably just an internal variable for [doublexp.net](doublexp.net). So it's safe to remove it. `dailyDeal` however, is a useful data we can use to analyze the daily deal of the game. But we need to process it differently so it doesn't interfere with the missions parsing.

Let's store in a separate variable, `dailyDeal`

```{r}
dailyDeal <- json_data$dailyDeal
```

Now, we can remove them from the `json_data`. I'm not comfortable mutating our JSON directly, so let's assign it to a new variable.

```{r}
drg_missions <- json_data
drg_missions$dailyDeal <- NULL
drg_missions$ver <- NULL
```

We're good to go. It's time to unravel this JSON to a beautiful data frame.

```{r}
drg_missions <- tibble(drg_missions)
```

Unfortunately, this comes the hard part, unnesting the JSON. `tidyr` provides a powerful set of tools to unnest a data. Yet, I barely able to wrap my head around it.

Basically, there's two main functions we'll use:

1.  `unnest_wider()`: To unpack a list into columns
2.  `unnest_longer():` To unpack a list into rows

First, we'll do `unnest_wider()` to make the timestamps as a new column.

```{r}
drg_missions |>
  unnest_wider(drg_missions)
```

Then, we'll `unnest_longer()` twice to unwrap the Biomes, and the biomes themselves (level 2 and 3).

```{r}
drg_missions |> 
  unnest_wider(drg_missions) |> 
  unnest_longer(Biomes) |> 
  unnest_longer(Biomes)
```

Finally, we'll do `unnests_wider()` to unpack the mission details from each biome (level 4).

```{r}
drg_missions_unnested <- drg_missions |> 
  unnest_wider(drg_missions) |> 
  unnest_longer(Biomes) |> 
  unnest_longer(Biomes) |> 
  unnest_wider(Biomes)

drg_missions_unnested
```

At this point, the data frame is almost done. It looks exactly like how we want it to be. Except, 2 columns are still lists: `MissionWarnings` and `Included_in`. We could do `unnest_longer()` on them, but it'll make duplicate rows, since the only thing different is them. So, the alternative is to use `paste` to join them as one string, separated by comma.

```{r}
drg_missions_unnested |> 
  select(included_in) |> 
  slice(c(1:4))
```

```{r}
drg_missions_unnested <- drg_missions_unnested |> 
  mutate(included_in = map_chr(included_in, ~ paste(.x, collapse = ", ")))

drg_missions_unnested
```

Perfect! Later on, we'll use the `included_in` to filter the missions available to us based on the season.

The lasts thing to convert is the `MissionWarnings`. We could combine them with `paste()` as well, but it's a valuable data we can use to analyse the mission. It's better to treat them as variables and separate it into two columns with `unnest_wider()`.

```{r}
drg_missions_unnested |> 
  unnest_wider(MissionWarnings, names_sep = '_')
```

Perfect! Now, we know how to parse 1 JSON. Let's apply it to all the JSON we have and combine them into one giant tibble.

#### Parsing multiple JSON

To automate the algorithm above, we'd need to condense it to a function in which we can map it to all the JSON files.

```{r}
parse_json_drg_mission <- function(json_path) {
  json_data <- read_json(json_path)
  json_data$dailyDeal <- NULL
  json_data$ver <- NULL
  
  drg_missions <- tibble(drg_missions = json_data)
  drg_missions <-  drg_missions |> 
    unnest_wider(drg_missions) |> 
    unnest_longer(Biomes) |> 
    unnest_longer(Biomes) |> 
    unnest_wider(Biomes)
  
  drg_missions <-  drg_missions |> 
    unnest_wider(MissionWarnings, names_sep = '_') |> 
    mutate(included_in = map_chr(included_in, ~ paste(.x, collapse = ", ")))
  
  return(drg_missions)
}
```

```{r}
parse_json_drg_dailydeal <- function(json_path) {
  json_data <- read_json(json_path)
  dailyDeal <- json_data$dailyDeal
  dailyDeal <- tibble(list(dailyDeal))
  dailyDeal <- dailyDeal |> 
    unnest_wider(everything())
  return(dailyDeal)
}
```

You probably noticed why I reassign `dailyDeal` multiple times instead of just chaining the pipe operator. Please don't ask why ðŸ˜…. I spent more than 30 minutes debugging why the pipe operator messes up the code. In a nutshell, reassigning it preserves the 'dailyDeal' name inside the column, while pipe operator do the opposite.

Previously, it was supposed to be one function, returning a list of 2 tibbles. But that caused a lot of unexpected headache related to one I just mentioned. So I decided to just make separate functions to parse `dailyDeal` and `missions`.

Anyway, here's the output of the functions

```{r}
parse_json_drg_mission('./json/2024-07-01.json')
```

```{r}
parse_json_drg_dailydeal('./json/2024-07-01.json')
```

Working perfectly, finally. Now, we just need to apply these functions to the list of JSON file paths and combine each of them with `bindrows()`.

```{r}

```

```{r}
filepath_list <- paste0('json/',filename_list)
```

```{r eval=FALSE}
tic('combining dailyDeal')
dailyDeal_df_list <- map(filepath_list, parse_json_drg_dailydeal)
toc()

tic('combining missions')
missions_df_list <- map(filepath_list, parse_json_drg_mission)
toc()
```

```         
combining dailyDeal: 1.266 sec elapsed
combining missions: 7.144 sec elapsed
```

Wow, that's quite a significant processing time. Let's cache them to csv files so I don't need to convert them again from scratch if I need to debug.

But first, we need to combine them into one data frames

```{r eval=FALSE}
dailyDeal_df_combined <- dailyDeal_df_list |>
  bind_rows()
missions_df_combined <- missions_df_list |> 
  bind_rows()
```

```{r eval=FALSE}
dailyDeal_df_combined |> write_csv('csv/drg_dailydeal_july_2024.csv')
missions_df_combined |> write_csv('csv/drg_missions_july_2024.csv')
```

Now we can read from these csv files instead.

```{r}
dailyDeal_df_combined <- read_csv('csv/drg_dailydeal_july_2024.csv')
missions_df_combined <- read_csv('csv/drg_missions_july_2024.csv', col_types = 'c')
```

```{r}
dailyDeal_df_combined
```

```{r}
missions_df_combined
```

Nice. Now we have tidy data frames we can work with. The last step before it's 100% ready to be analyzed is to clean it.

#### Cleaning Data

```{r}
missions_df_combined_cleaned <- missions_df_combined
```

##### Non-tidy Data

In his book [*R for Data Science*](https://r4ds.hadley.nz/)*,* Hadley introduces a concept of Tidy Data, which makes data analysis much easier:

> 1.  Each column is a variable.
>
> 2.  Each row is an observation.
>
> 3.  Each cell is a single value.

Fortunately, it looks like we've passed 3 rules, so our data is already tidy.

##### Improper Column Names

It's good enough as it is, but we can make it even better.

1.  **Inconsistencies:** most of the column names are in PascalCase, but `included_in` is in snake_case, and `timestamp` is just all lowercase.
2.  **Grammatically Wrong:** `MissionWarnings_1`, `MissionWarnings_2,` and `Biomes_id` only contain one value in each row, so they should be singular
3.  **Misc:** `Biomes_id` just sounds weird. "id" shouldn't be there.

```{r}
missions_df_combined_cleaned <- missions_df_combined_cleaned |> 
  rename(Timestamp = timestamp,
         IncludedIn = included_in,
         Biome = Biomes_id,
         MissionWarning1 = MissionWarnings_1,
         MissionWarning2 = MissionWarnings_2)
```

```{r}
missions_df_combined_cleaned |> 
  names()
```

##### Irrelevant Data

Previously, I mentioned that we'd only analyze missions data that is usually availabe to most people. Which is the unseasoned ones. In this data, it's the `s0` from `included_in`. So we can filter only the rows with `s0`.

```{r}
missions_df_combined_cleaned <- missions_df_combined_cleaned |> 
  filter(IncludedIn |> str_detect('s0'))
```

```{r}
missions_df_combined_cleaned
```

##### Incorrect Data Types

Since we manually extracted and parsed the data from the JSON files, R didn't assume any data type and just render everything as `chr`. Which mean, a lot of them needs to be changed.

1.  **Factors**\
    `Biomes`, `PrimaryObjective`, `SecondaryObjective`, `MissionMutator`, `MissionWarning1`, and `MissionWarning2` should be factors, since they're categorical variables with limited set of values. `Complexity` and `Length` are a bit tricky since they're ordinal variables, which can be treated as either qualitative or quantitative. Claude Sonnet 3.5, however, argues that since we don't know the exact interval between 1 to 2, or 2 to 3, they should be treated as categorical. Sonnet 3.5 has helped me a lot, so I'll trust her ðŸ¤£.

    > Based on the information provided, your 'Complexity Level' variable is an ordinal categorical variable. Even though it's represented by numbers (1, 2, 3), it's not truly numeric in nature, as the intervals between levels may not be equal.

2.  **Characters\
    **CodeName can stay as `chr` since they don't really hold any values other than being randomly generated names. `included_in` should probably be factors, but the structure isn't standardized yet, and we don't need to deal with them anyway.

3.  **Integers\
    `id`** should be integers. Oh wait, it's already is! I didn't know that.

4.  **Date\
    **This one is obvious. The `Timestamp` column should be in date object format. Otherwise, we can't do any date computation with it.

```{r}
missions_df_combined_cleaned <- missions_df_combined_cleaned |> 
  mutate(across(c(Biome, PrimaryObjective, SecondaryObjective, MissionMutator, MissionWarning1, MissionWarning2, Complexity, Length),
         as_factor)) |> 
  mutate(Timestamp = ymd_hms(Timestamp))

missions_df_combined_cleaned
```

##### Duplicate Data

We also need to scan for duplicates. Assuming the extractor script is robust, there shouldn't be any. But Just in case, we should still check.

First, we can check the frequency count of each id, since it's supposed to be the unique indentifier.

```{r}
missions_df_combined_cleaned |> 
  count(id, sort = TRUE)
```

There's no duplicate id, great. The next column we could check is the `codeName`

```{r}
missions_df_combined_cleaned |> 
  count(CodeName, sort = TRUE)
```

There's quite several repetitions. However, as long as they're in different time, it's safe. So that's our third comparison: `timestamp`

```{r}
missions_df_combined_cleaned |> 
  count(CodeName, Timestamp, sort = TRUE)
```

Yup, all unique. We could also pick one codename and see its time pattern:

```{r}
missions_df_combined_cleaned |> 
  count(CodeName, Timestamp, sort = TRUE) |> 
  filter(CodeName == 'Corrupt Trail')
```

No particular pattern visible, but we can see they're on different days. Only one on the same day, but with 3 hours difference.

Alright, all set. We're good to go.

## Analyzing Data

### Mission Pattern

#### Overall Distribution

Before we analyze the pattern of Double XP's mission mutator, let's first see the overall distribution of mission mutators

```{r}
missions_df_combined_cleaned |> 
  ggplot(aes(MissionMutator)) +
  geom_bar()
```

I'm not sure why `NA` values are also visualized, since they're usually implicitly removed.

```{r}
missions_df_combined_cleaned |> 
  select(MissionMutator) |> 
  filter(!is.na(MissionMutator)) |> 
  ggplot(aes(MissionMutator)) +
  geom_bar()
```

Now the `NA` values are removed, the labels are hard to read due to how many it is. Let's try mitigating this by wrapping the labels to newlines

```{r}
missions_df_combined_cleaned |> 
  select(MissionMutator) |> 
  filter(!is.na(MissionMutator)) |> 
  ggplot(aes(MissionMutator)) +
  geom_bar() +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 5))
```

Now it looks much better. We can reorder the bar descendingly with `fct_infreq()` to see what's the highest and lowest, but honestly, they all don't seem much differ in values, except for *Blood Sugar.*

```{r}
missions_df_combined_cleaned |> 
  select(MissionMutator) |> 
  filter(!is.na(MissionMutator)) |> 
  ggplot(aes(fct_infreq(MissionMutator))) +
  geom_bar() +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 5))
```

See? Well, I guess this is to be expected as it's generated by computers ðŸ˜….

#### Double XP Pattern

Finally, we reach the main objective.

Since we have the complete data with complete timestamp for the entire month, there's various ways we can visualize the pattern.

1.  Hourly
2.  Daily
3.  Weekly

To make our analysis easier, we need to extract each component of the date from the Timestamp, which are: week days (names), days (numbers), and hours

```{r}
double_xp_trend <- missions_df_combined_cleaned |> 
  mutate(WeekDay = wday(Timestamp, label=TRUE),
         Day = day(Timestamp),
         Hour = hour(Timestamp),
         .after = Timestamp) |> 
  filter(MissionMutator == 'Double XP')

```

```{r}
double_xp_trend
```

##### Weekly

Let's compute the average occurrence per day. We can obtain it by first computing the total count each day, and then get the mean from them.

```{r}
double_xp_trend_weekly <- double_xp_trend |> 
  count(Day, WeekDay) |> 
  group_by(WeekDay) |> 
  summarize(mean = mean(n))

double_xp_trend_weekly
```

It looks like there isn't much difference between the day. We can verify it by checking the SD.

```{r}
double_xp_trend_weekly |> 
  pull(mean) |> 
  sd()
```

Yeah, that's minuscule. But still, let's visualize it to get a better picture.

```{r}
double_xp_trend_weekly |>  
  ggplot(aes(WeekDay, mean, group = 1)) +  
  geom_line() +
  geom_label(aes(label = mean)) +
  labs(x='Week Day', y='Averge Occurence (mean)', title = 'Average Occurence (Mean) of Double XP Each Day')
```

So the peak is on Saturday and the trough is on Thursday. Still, if you think about it as a gamer, it really isnt' much of a difference, except maybe for Thursday. 20 Double XP a day is a lot. One game can range from 30 to 60 minutes. Even if you stay up all day, you can't get all of them anyway.

##### Daily

```{r}
double_xp_trend |> 
  count(Day, WeekDay) |> 
  ggplot(aes(Day, n)) +
  geom_line() +
  scale_x_continuous(breaks = c(1:31)) +
  labs(title='Daily Occurences of Double XP')
```

When we zoom out and see the pattern throughout the month, we can see a repeating up and down cycle that's going down nearing the end of the month. I wish we have data from the other months to complete the pattern.

##### Hourly

```{r}
double_xp_trend |> 
  count(Day, Hour) |> 
  group_by(Hour) |> 
  summarize(mean = mean(n))
```

```{r}
double_xp_trend |> 
  count(Hour) |> 
  ggplot(aes(Hour, n)) +
  geom_line() +
  scale_x_continuous(breaks = c(0:23))
```

If we zoom in on the hours and sum all the occurrences on each hour, we can see that the peak is on 10:00 UTC. But is it really that much of a difference compared to the rest? As we did previously, we can use the `mean` instead to see how many occurence is it on average.

```{r}
double_xp_trend_hourly <- double_xp_trend |> 
  count(Day, Hour) |> 
  group_by(Hour) |> 
  summarize(mean = mean(n))

double_xp_trend_hourly |>
  ggplot(aes(Hour, mean)) +
  geom_line() +
  scale_x_continuous(breaks = 0:23)
```

Ohhh, that's unexpected. So on average, the highest is not on 10:00 UTC, but on 01:00 UTC, which was quite low when we see the the total count. This might indicate a presence of outliers.

Anyway, it doesn't matter. The range is only about 1 difference, from 1 to 2. That's meaningless when you consider the game duration per mission. Once you finish one, a new mission will be generated.

## Summary

### Insight 1: Double XP Pattern

Statistically, there are trends and pattern of peaks and troughs, of when Double XP most likely to happen.

Unfortunately, from the gamer's perspective, the difference is not significant enough. On Weekly difference, you don't play game all day. On hourly difference, a new mission will be generated once you played.

In short, there is no strategy that we, as the gamers, can use to find the right time to play. Fortunately, since we have the complete timestamps, we can use it to instead snipe the Double XP missions directly.

### Insight 2: Mutators Occurrences

There's another interesting insight, though. The hourly average is about 1.5. However, we know that the game mission refreshes every 30 minutes. And there's like 20 missions at once. Which mean, on average, there can only be 1 Double XP mission in every cycle.

But remember, we have filtered the data to only include missions that are Double XP. If we have included all mutators, it's highly possible they have similar pattern, considering the frequency of all mutators are highly similar.

## Appendix: Other Findings

Below, you can find other findings that aren't related to Double XP, but might be interesting to know.

### Missions

```{r}
missions_df_combined_cleaned
```

```{r}
missions_df_other_findings <- missions_df_combined_cleaned |> 
  mutate(WeekDay = wday(Timestamp, label=TRUE),
         Day = day(Timestamp),
         Hour = hour(Timestamp),
         .after = Timestamp)
```

#### Biome

```{r}
#| code-fold: true
missions_df_other_findings |>
  count(Day, Biome, name = 'Total_Occurences') |> 
  group_by(Biome) |> 
  summarize(Daily_Occurence = mean(Total_Occurences)) |> 
  arrange(desc(Daily_Occurence)) |> 
  ggplot(aes(Daily_Occurence, fct_rev(fct_infreq(Biome, Daily_Occurence)))) +
  geom_col() +
  labs(title = 'Most Common Biomes', subtitle = 'Based on its Mean of Daily Occurences', x = 'Daily Occurence', y = 'Biomes')
```

#### Primary Objective

```{r}
#| code-fold: true
missions_df_other_findings |> 
  count(Day, PrimaryObjective, name = 'Total_Occurences') |> 
  group_by(PrimaryObjective) |> 
  summarize(Daily_Occurence = mean(Total_Occurences)) |> 
  ggplot(aes(fct_infreq(PrimaryObjective, Daily_Occurence), Daily_Occurence)) +
  geom_col() +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +
  labs(title = 'Most Common Primary Objectives', subtitle = 'Based on its Mean of Daily Occurences', x = 'Primary Objective', y = 'Daily Occurence')
```

#### Secondary Objective

```{r}
#| code-fold: true
missions_df_other_findings |>
  mutate(SecondaryObjective = str_replace(SecondaryObjective, 'ApocaBlooms', 'Apoca Bloom')) |> 
  count(Day, SecondaryObjective, name = 'Total_Occurences') |> 
  group_by(SecondaryObjective) |> 
  summarize(Daily_Occurence = mean(Total_Occurences)) |> 
  ggplot(aes(fct_rev(fct_infreq(SecondaryObjective, Daily_Occurence)), Daily_Occurence)) +
  geom_col() +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +
  labs(title = 'Most Common Secondary Objectives', subtitle = 'Based on its Mean of Daily Occurences', x = 'Secondary Objective', y = 'Daily Occurence')
```

#### Mission Warnings

##### Primary Warning

```{r}
#| code-fold: true
missions_df_other_findings |>
  filter(!is.na(MissionWarning1)) |> 
  count(Day, MissionWarning1, name = 'Total_Occurences') |> 
  group_by(MissionWarning1) |> 
  summarize(Daily_Occurence = mean(Total_Occurences)) |> 
  arrange(desc(Daily_Occurence)) |> 
  head(5) |> 
  ggplot(aes(Daily_Occurence, fct_rev(fct_infreq(MissionWarning1, Daily_Occurence)))) +
  geom_col() +
  labs(title = 'Top Five Most Common Frimary Mission Warnings', subtitle = 'Based on its Mean of Daily Occurences', x = 'Daily Occurence', y = 'Primary Mission Warnings')
```

```{r}
#| code-fold: true
missions_df_other_findings |>
  filter(!is.na(MissionWarning1)) |> 
  count(Day, MissionWarning1, name = 'Total_Occurences') |> 
  group_by(MissionWarning1) |> 
  summarize(Daily_Occurence = mean(Total_Occurences)) |> 
  arrange(Daily_Occurence) |> 
  head(5) |> 
  ggplot(aes(Daily_Occurence, fct_infreq(MissionWarning1, Daily_Occurence))) +
  geom_col() +
  labs(title = 'Top Five Rarest Primary Mission Warnings', subtitle = 'Based on its Mean of Daily Occurences', x = 'Daily Occurence', y = 'Primary Mission Warnings')
```

#### Mission Warnings Combinations

##### Top 10

```{r}
#| code-fold: true
missions_df_other_findings |> 
  filter(!is.na(MissionWarning1), !is.na(MissionWarning2)) |> 
  count(MissionWarning1, MissionWarning2, name = 'Total_Occurence') |> 
  arrange(desc(Total_Occurence))
```

##### Lowest 10

```{r}
#| code-fold: true
missions_df_other_findings |> 
  filter(!is.na(MissionWarning1), !is.na(MissionWarning2)) |> 
  count(MissionWarning1, MissionWarning2, name = 'Total_Occurence') |> 
  arrange(Total_Occurence)
```

### Daily Deal

```{r}
dailyDeal_df_combined
```

#### Overall Trend

```{r}
#| code-fold: true
dailyDeal_df_combined |> 
  ggplot(aes(timestamp,Credits, colour = DealType)) +
  geom_line() +
  labs(title = 'Trend of Daily Deal over July 2024', color='Deal Type', x='Date', y='Credits Offered')
```

#### Common Resource on Sale

```{r}
#| code-fold: true
dailyDeal_df_combined |> 
  count(Resource) |> 
  ggplot(aes(fct_infreq(Resource, n), n)) +
  geom_col() +
  labs(title = 'Most Common Resource on Sale', subtitle = 'Based on total occurences over July 2024', x = 'Resource', y = 'Total Occurence')
```
