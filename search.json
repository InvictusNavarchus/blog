[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey, wassup? Want to hear a fun fact? I discovered the world of Data Science in May 2024. It wasn’t that long, yet I already fell in love with its nature.\nI’ve always been a curious person; seeking the truth, exploring the unknown, and shining light on mysteries. That’s why Data Science suits me perfectly. There’s always mystery to explore and pattern to discover. Even a simple dataset can hide a valuable insight if you look deep enough.\nThat’s when I decided that this is the path I’ll be walking towards. Not just as a career, but as a passion.\nThe path I’m walking on is vast and endless. That’s why I created this blog. To document the journey that awaits me. I hope my blog can also help fellow learners who are also just starting to walk the same path.\n\n\n Back to top"
  },
  {
    "objectID": "posts/imdb-top-10000-movies/EDA_SQL.html",
    "href": "posts/imdb-top-10000-movies/EDA_SQL.html",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDB",
    "section": "",
    "text": "-- url title example\n-- https://www.imdb.com/title/tt0177484/\n-- preview titles\nselect * from title_basics tb \n-- count of titles: 9,123,039\nselect count(*) from title_basics tb\n-- count of each titleType\n--| titleType      | count     | percentage |\n--|---------------|-----------|------------|\n--| tvEpisode     | 6,967,265 | 76.37%     |\n--| short         | 832,301   | 9.12%      |\n--| movie         | 590,707   | 6.47%      |\n--| video         | 251,682   | 2.76%      |\n--| tvSeries      | 225,012   | 2.47%      |\n--| tvMovie       | 127,122   | 1.39%      |\n--| tvMiniSeries  | 46,600    | 0.51%      |\n--| tvSpecial     | 41,436    | 0.45%      |\n--| videoGame     | 32,570    | 0.36%      |\n--| tvShort       | 8,343     | 0.09%      |\n--| tvPilot       | 1         | 0.00%      |\nSELECT titleType,\n    COUNT(*) as count,\n    ROUND((COUNT(*) * 100.0 / 9123039), 2)\n    as percentage\nFROM title_basics tb\nGROUP BY titleType\nORDER BY count DESC;\n-- count title in title_ratings: 1,470,769\nselect count(*) from title_ratings tr;\n-- count of title that has ratings: 1,236,363\nselect count(*)\nfrom title_basics tb \njoin title_ratings tr \n    on tb.tconst = tr.tconst;\n-- find missing titles\n-- found tt0187178 that redirects to tt0177484 which exists in tb\n-- found tt0253688, tt0253697 that actually exist with matching rating\nselect * from title_ratings tr \nleft join title_basics tb \n    on tr.tconst = tb.tconst \nwhere tb.tconst is null\n-- movie counts: 590,707\nselect count(*) from title_basics tb \nwhere titleType = 'movie';\n--newest movies\nselect * from title_basics tb \nwhere titleType = 'movie' and startYear != '\\N'\norder by startYear desc \n--oldest movies\nselect * from title_basics tb \nwhere titleType = 'movie' and startYear != '\\N'\norder by startYear\n-- movie that has ratings: 279,122      \nselect count(*)\nfrom title_basics tb \njoin title_ratings tr \n    on tb.tconst = tr.tconst \nwhere tb.titleType = 'movie';\n-- check sample size for analysis target:\n--- Top 10.000 Movies based on Num Votes and Ratings\n----- Filter release year too?\n\n----- based on numVotes\nselect * \nfrom title_basics tb \ninner join title_ratings tr \n    on tb.tconst = tr.tconst \nwhere titleType = 'movie'\norder by tr.numVotes desc\nlimit 10000\n----- sort by numVotes & average must be 6.8 or higher\nselect * \nfrom title_basics tb \ninner join title_ratings tr \n    on tb.tconst = tr.tconst \nwhere titleType = 'movie' and averageRating &gt;= 7.0\norder by tr.numVotes desc\nlimit 10000\n\n\n\n-- join title basics with title ratings for finding most popoular movies ordered by number of votes\nselect tb.tconst, primaryTitle, genres, startYear, runtimeMinutes, averageRating, numVotes\nfrom title_basics tb \ninner join title_ratings tr \n    on tb.tconst = tr.tconst\nwhere titleType = 'movie'\norder by numVotes desc\nlimit 10000",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with SQL"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/EDA_SQL.html#exploring-data",
    "href": "posts/imdb-top-10000-movies/EDA_SQL.html#exploring-data",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDB",
    "section": "",
    "text": "-- url title example\n-- https://www.imdb.com/title/tt0177484/\n-- preview titles\nselect * from title_basics tb \n-- count of titles: 9,123,039\nselect count(*) from title_basics tb\n-- count of each titleType\n--| titleType      | count     | percentage |\n--|---------------|-----------|------------|\n--| tvEpisode     | 6,967,265 | 76.37%     |\n--| short         | 832,301   | 9.12%      |\n--| movie         | 590,707   | 6.47%      |\n--| video         | 251,682   | 2.76%      |\n--| tvSeries      | 225,012   | 2.47%      |\n--| tvMovie       | 127,122   | 1.39%      |\n--| tvMiniSeries  | 46,600    | 0.51%      |\n--| tvSpecial     | 41,436    | 0.45%      |\n--| videoGame     | 32,570    | 0.36%      |\n--| tvShort       | 8,343     | 0.09%      |\n--| tvPilot       | 1         | 0.00%      |\nSELECT titleType,\n    COUNT(*) as count,\n    ROUND((COUNT(*) * 100.0 / 9123039), 2)\n    as percentage\nFROM title_basics tb\nGROUP BY titleType\nORDER BY count DESC;\n-- count title in title_ratings: 1,470,769\nselect count(*) from title_ratings tr;\n-- count of title that has ratings: 1,236,363\nselect count(*)\nfrom title_basics tb \njoin title_ratings tr \n    on tb.tconst = tr.tconst;\n-- find missing titles\n-- found tt0187178 that redirects to tt0177484 which exists in tb\n-- found tt0253688, tt0253697 that actually exist with matching rating\nselect * from title_ratings tr \nleft join title_basics tb \n    on tr.tconst = tb.tconst \nwhere tb.tconst is null\n-- movie counts: 590,707\nselect count(*) from title_basics tb \nwhere titleType = 'movie';\n--newest movies\nselect * from title_basics tb \nwhere titleType = 'movie' and startYear != '\\N'\norder by startYear desc \n--oldest movies\nselect * from title_basics tb \nwhere titleType = 'movie' and startYear != '\\N'\norder by startYear\n-- movie that has ratings: 279,122      \nselect count(*)\nfrom title_basics tb \njoin title_ratings tr \n    on tb.tconst = tr.tconst \nwhere tb.titleType = 'movie';\n-- check sample size for analysis target:\n--- Top 10.000 Movies based on Num Votes and Ratings\n----- Filter release year too?\n\n----- based on numVotes\nselect * \nfrom title_basics tb \ninner join title_ratings tr \n    on tb.tconst = tr.tconst \nwhere titleType = 'movie'\norder by tr.numVotes desc\nlimit 10000\n----- sort by numVotes & average must be 6.8 or higher\nselect * \nfrom title_basics tb \ninner join title_ratings tr \n    on tb.tconst = tr.tconst \nwhere titleType = 'movie' and averageRating &gt;= 7.0\norder by tr.numVotes desc\nlimit 10000",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with SQL"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/EDA_SQL.html#extracting-data",
    "href": "posts/imdb-top-10000-movies/EDA_SQL.html#extracting-data",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDB",
    "section": "",
    "text": "-- join title basics with title ratings for finding most popoular movies ordered by number of votes\nselect tb.tconst, primaryTitle, genres, startYear, runtimeMinutes, averageRating, numVotes\nfrom title_basics tb \ninner join title_ratings tr \n    on tb.tconst = tr.tconst\nwhere titleType = 'movie'\norder by numVotes desc\nlimit 10000",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with SQL"
    ]
  },
  {
    "objectID": "posts/g4ds-getting-started/index.html",
    "href": "posts/g4ds-getting-started/index.html",
    "title": "Working through G4DS: Getting Started - Part 1",
    "section": "",
    "text": "Previously, I’ve been using Python in Jupyter Notebooks along with its tools such as Pandas, Seaborn, Plotly. I’m not an expert yet. I’m barely familiar with the toolset. Then, I found out about R. I heard many interesting things about: it’s simplicity, it’s advantages over Python, it’s power over Statistic and Data Analysis. Then, I found ggplot2. I instantly fell in love with its grammar of graphics.\nThen, I tried R myself. It’s amazing. I decided, well, I guess I’ll dump Python right there and there. I’m not saying Python is bad at all. It’s amazing. I’ve been using it for years for scripting. I just feel that R is well more suited for data analysis. It’s as if it’s designed for it.\nI’ve learned a bit about ggplot on R4DS. Then I stopped midway and asked myself: “Why don’t I just dive deep into it before continuing?” I already know there’s an amazing book ggplot2: Elegant Graphics for Data Analysis (3e), available for free online.\nAt first, I was just exploring blindly, trying many functions while skimming through the books. It’s fun and all, but I realized that I could make my learning much more effective if I document it. Especially if I use narrative: explaining the steps I take, and my thought process in-between.\nAnd, here I am. It’s barely a month since I started exploring the world of Data Analysis, and I already fell in love with it. That’s why I decided to be serious about it.\nAlright, that’s enough of my story, let’s get started. Feel free to follow along with the book. As the title suggests, we’ll be working on each chapter with each post. Therefore, this post will be dedicated to document my journey on chapter Getting Started"
  },
  {
    "objectID": "posts/g4ds-getting-started/index.html#preface",
    "href": "posts/g4ds-getting-started/index.html#preface",
    "title": "Working through G4DS: Getting Started - Part 1",
    "section": "",
    "text": "Previously, I’ve been using Python in Jupyter Notebooks along with its tools such as Pandas, Seaborn, Plotly. I’m not an expert yet. I’m barely familiar with the toolset. Then, I found out about R. I heard many interesting things about: it’s simplicity, it’s advantages over Python, it’s power over Statistic and Data Analysis. Then, I found ggplot2. I instantly fell in love with its grammar of graphics.\nThen, I tried R myself. It’s amazing. I decided, well, I guess I’ll dump Python right there and there. I’m not saying Python is bad at all. It’s amazing. I’ve been using it for years for scripting. I just feel that R is well more suited for data analysis. It’s as if it’s designed for it.\nI’ve learned a bit about ggplot on R4DS. Then I stopped midway and asked myself: “Why don’t I just dive deep into it before continuing?” I already know there’s an amazing book ggplot2: Elegant Graphics for Data Analysis (3e), available for free online.\nAt first, I was just exploring blindly, trying many functions while skimming through the books. It’s fun and all, but I realized that I could make my learning much more effective if I document it. Especially if I use narrative: explaining the steps I take, and my thought process in-between.\nAnd, here I am. It’s barely a month since I started exploring the world of Data Analysis, and I already fell in love with it. That’s why I decided to be serious about it.\nAlright, that’s enough of my story, let’s get started. Feel free to follow along with the book. As the title suggests, we’ll be working on each chapter with each post. Therefore, this post will be dedicated to document my journey on chapter Getting Started"
  },
  {
    "objectID": "posts/g4ds-getting-started/index.html#introduction",
    "href": "posts/g4ds-getting-started/index.html#introduction",
    "title": "Working through G4DS: Getting Started - Part 1",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n\nThe goal of this chapter is to teach you how to produce useful graphics with ggplot2 as quickly as possible. You’ll learn the basics of ggplot() along with some useful “recipes” to make the most important plots.\n\nLooks like the book will follow a similiar pattern to R4DS and Python4DS. The first chapter will be a rapid overview. No problem.\n\nHere we’ll skip the theory and focus on the practice, and in later chapters you’ll learn how to use the full expressive power of the grammar.\n\nSince there is no theory, I’m guessing there will be a lot of experiments and figuring things out. Sounds fun.\n\nIn this chapter you’ll learn:\n\nAbout the mpg dataset included with ggplot2, Section 2.2.\nThe three key components of every plot: data, aesthetics and geoms, Section 2.3.\nHow to add additional variables to a plot with aesthetics, Section 2.4.\nHow to display additional categorical variables in a plot using small multiples created by faceting, Section 2.5.\nA variety of different geoms that you can use to create different types of plots, Section 2.6.\nHow to modify the axes, Section 2.7.\nThings you can do with a plot object other than display it, like save it to disk, Section 2.8.\n\n\nThat seems a lot for an introduction. Oh right, it’s a rapid overview. That makes sense. I wonder what mpg dataset is."
  },
  {
    "objectID": "posts/g4ds-getting-started/index.html#fuel-economy-data",
    "href": "posts/g4ds-getting-started/index.html#fuel-economy-data",
    "title": "Working through G4DS: Getting Started - Part 1",
    "section": "2.2 Fuel Economy Data",
    "text": "2.2 Fuel Economy Data\n\nIn this chapter, we’ll mostly use one data set that’s bundled with ggplot2: mpg. It includes information about the fuel economy of popular car models in 1999 and 2008, collected by the US Environmental Protection Agency, http://fueleconomy.gov.\n\nAhh, car models. Too bad I’m not familiar with cars. Let’s see if I can follow along.\n\nlibrary(ggplot2)\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\n\nThe variables are mostly self-explanatory:\n\ncty and hwy record miles per gallon (mpg) for city and highway driving.\ndispl is the engine displacement in litres.\ndrv is the drivetrain: front wheel (f), rear wheel (r) or four wheel (4).\nmodel is the model of car. There are 38 models, selected because they had a new edition every year between 1999 and 2008.\nclass is a categorical variable describing the “type” of car: two seater, SUV, compact, etc.\n\n\nOkay, understandable.\n\nThis dataset suggests many interesting questions. How are engine size and fuel economy related? Do certain manufacturers care more about fuel economy than others? Has fuel economy improved in the last ten years?\n\nNow I want to pull up ggplot right away and draw a scatter plot.\n\nWe will try to answer some of these questions, and in the process learn how to create some basic plots with ggplot2.\n\nBut let’s follow the intended workflow for now. There’s plenty of time and other datasets to freely explore.\n\n2.2.1 Exercises\n\n\nList five functions that you could use to get more information about the mpg dataset.\nHow can you find out what other datasets are included with ggplot2?\nApart from the US, most countries use fuel consumption (fuel consumed over fixed distance) rather than fuel economy (distance travelled with fixed amount of fuel). How could you convert cty and hwy into the European standard of l/100km?\nWhich manufacturer has the most models in this dataset? Which model has the most variations? Does your answer change if you remove the redundant specification of drive train (e.g. “pathfinder 4wd”, “a4 quattro”) from the model name?\n\n\nInteresting questions. Let’s try unpacking them one by one.\n\n\nList five functions that you could use to get more information about the mpg dataset.\n\n\nI’m not sure exactly what ‘functions’ the text are referring to? Any functions? I can list several functions from dplyr packages, but I’m not sure that’s what intended because this is book on ggplot. But at the same time, we’re just starting out, so we aren’t supposed to know many ggplot functions, yet.\nTricky. I guess I’ll lean toward ‘any’.\n\nselect()\nfilter()\nmutate()\narrange()\ndistinct()\n\nYeah, whatever, haha. If you know what the answer should’ve been, let me know in the comments.\n\n\nHow can you find out what other datasets are included with ggplot2?\n\n\nUsually, I just do ggplot:: and browse through the suggested auto-completion. The dataset will have a special icon on it.\n\nIn RStudio, you can do similar thing by browsing the Environment tab.\n\nI don’t know how to take HD screenshots. The compression quality doesn’t change anything. So please let me know if you know.\n\n\nApart from the US, most countries use fuel consumption (fuel consumed over fixed distance) rather than fuel economy (distance travelled with fixed amount of fuel). How could you convert cty and hwy into the European standard of l/100km?\n\n\nWait, what. I’m not familiar all of these terms. Let me look it up…\n\nThe text highlights the difference between two ways of measuring fuel efficiency: fuel consumption and fuel economy.\nFuel consumption is the amount of fuel used over a fixed distance, typically measured in liters per 100 kilometers (L/100km) or gallons per 100 miles (gal/100mi). This metric directly indicates how much fuel a vehicle consumes to cover a specific distance.\nOn the other hand, fuel economy is the distance traveled with a fixed amount of fuel, often expressed in kilometers per liter (km/L) or miles per gallon (mpg). This measure represents the efficiency of a vehicle in terms of how far it can go on a given quantity of fuel.\n\nOkay, I’m starting to get the hang of it. However, I believe that researching the difference between the two is beyond the scope of this chapter, let alone this book. So, let’s skip over this. (Read: I’m too lazy to deal with Math, lol)\n\n\nWhich manufacturer has the most models in this dataset? Which model has the most variations? Does your answer change if you remove the redundant specification of drive train (e.g. “pathfinder 4wd”, “a4 quattro”) from the model name?\n\n\nWhat does ‘most models’ supposed to mean here? I’m guessing it means ‘most amounts of models’, regardless of how many cars are there with those models.\nAt first, the group by function from dplyr cames to mind, but the book is about ggplot, not dplyr, so I doubt we should be using that here.\nMy guess is, ggplot should be able to help us see which manufacturer has the most models through visualization.\nAs starters, let’s create a scatterplot with manufacturer as the x and model as the y\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\n\nmpg |&gt; \n  ggplot(aes(x=manufacturer, y=model, color=manufacturer)) + \n  geom_point()\n\n\n\n\n\n\n\n\nNope, it doesn’t help much. I could manually count the dot for each manufacturer, but that’s inefficient. Is there a way to collapse all the points and just make a bar to show many models each manufacturer has? Ahh, we can, if we make manipulation first using dplyr. We’re back to square one.\nAlright, I guess we still have to transform the data\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nI forgot what the data looks like, let’s see…\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nAhh, alright. Let’s select the unique values from the manufacturer and model.\n\nmpg |&gt; \n  distinct(manufacturer, model)\n\n# A tibble: 38 × 2\n   manufacturer model             \n   &lt;chr&gt;        &lt;chr&gt;             \n 1 audi         a4                \n 2 audi         a4 quattro        \n 3 audi         a6 quattro        \n 4 chevrolet    c1500 suburban 2wd\n 5 chevrolet    corvette          \n 6 chevrolet    k1500 tahoe 4wd   \n 7 chevrolet    malibu            \n 8 dodge        caravan 2wd       \n 9 dodge        dakota pickup 4wd \n10 dodge        durango 4wd       \n# ℹ 28 more rows\n\n\nNow it’s much clearer, but it doesn’t immediately show us which manufacturer has the most models. That’s where group by comes in\n\nmodels_count &lt;- mpg |&gt; \n  distinct(manufacturer, model) |&gt; \n  group_by(manufacturer) |&gt; \n  count(name='total_model') |&gt; \n  arrange(desc(total_model))\n\nmodels_count\n\n# A tibble: 15 × 2\n# Groups:   manufacturer [15]\n   manufacturer total_model\n   &lt;chr&gt;              &lt;int&gt;\n 1 toyota                 6\n 2 chevrolet              4\n 3 dodge                  4\n 4 ford                   4\n 5 volkswagen             4\n 6 audi                   3\n 7 nissan                 3\n 8 hyundai                2\n 9 subaru                 2\n10 honda                  1\n11 jeep                   1\n12 land rover             1\n13 lincoln                1\n14 mercury                1\n15 pontiac                1\n\n\nNow we can make a bar plot from this.\n\nmodels_count |&gt; \n  ggplot(aes(y=manufacturer, x=total_model)) +\n  geom_col()\n\n\n\n\n\n\n\n\nI’m not sure why it’s defaulting to alphabetical sorting, but we can see that Toyota has the most models: 6. Well, we can see that from the table anyway.\n\nDoes your answer change if you remove the redundant specification of drive train (e.g. “pathfinder 4wd”, “a4 quattro”) from the model name?\n\nI’m not familiar with cars, so it depends on whether these 2 specification are different. It says ‘redundant’, so I’m guessing they’re the same model. If that’s the case, then yes it would change the answer, because both models will be counted as 1 model."
  },
  {
    "objectID": "posts/g4ds-getting-started/index.html#closing",
    "href": "posts/g4ds-getting-started/index.html#closing",
    "title": "Working through G4DS: Getting Started - Part 1",
    "section": "Closing",
    "text": "Closing\nAlright, the post become much longer than expected. And we’re still halfway through the chapter. We’ll continue the second part on a new post.\nAlso, I heavily underestimated the amount of content the book has. So, unless, there’s something very interesting, we’ll just jump straight to the Exercises.\nThanks for riding along!"
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/index.html",
    "href": "posts/analyzing-manggarai-train-station/index.html",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "Manggarai is the largest train station in Indonesia, a hub for trains from all routes to transit in. This project aims to find the most optimal time for train passengers to go to the station.\n\n\n\n\n\nThe data was collected by querying the API behind the train company official website, KAI Commuter, using Python.\n\n\n\nThe data was transformed into a rectangular format and saved as a CSV file using R.. The column names are translated into English for accessibility and further renamed for clarity.\n\n\n\nThe analysis was conducted using R to perform statistical calculations and create visualizations.\n\n\n\n\n\n\nThere are significantly more trains at around 06:00 - 08:00 in the morning and 16:00-17:00 in the afternoon, than in other times, indicating rush hours\n\n\n\n\n\n\nIf you prefer convenience and want to avoid rush hours, it’s best to avoid the station between 06:00 - 08:00 in the morning and 16:00 - 17:00 in the afternoon.\nConversely, if you don’t mind the crowd and don’t want to wait long for a train to arrive, it’s best to come these rush hours.\n\n\n\n\n\nThe rush hours are inferred from the amount of trains that arrive, which may not accurately represent the situation. However, the reasoning is solid because the increased number of trains likely corresponds to higher passenger volumes.\nThe recommended actions assume that people go to Manggarai station directly. However, most people come to Manggarai through other train stations, which mean they have less control over their arrival time. Fortunately, due to the generally consistent train speeds, people can predict their arrival time at Manggarai based on the departure time from their nearest stations.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/index.html#background-and-objective",
    "href": "posts/analyzing-manggarai-train-station/index.html#background-and-objective",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "Manggarai is the largest train station in Indonesia, a hub for trains from all routes to transit in. This project aims to find the most optimal time for train passengers to go to the station.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/index.html#methodology",
    "href": "posts/analyzing-manggarai-train-station/index.html#methodology",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "The data was collected by querying the API behind the train company official website, KAI Commuter, using Python.\n\n\n\nThe data was transformed into a rectangular format and saved as a CSV file using R.. The column names are translated into English for accessibility and further renamed for clarity.\n\n\n\nThe analysis was conducted using R to perform statistical calculations and create visualizations.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/index.html#key-insights",
    "href": "posts/analyzing-manggarai-train-station/index.html#key-insights",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "There are significantly more trains at around 06:00 - 08:00 in the morning and 16:00-17:00 in the afternoon, than in other times, indicating rush hours",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/index.html#recommended-actions-for-passengers",
    "href": "posts/analyzing-manggarai-train-station/index.html#recommended-actions-for-passengers",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "If you prefer convenience and want to avoid rush hours, it’s best to avoid the station between 06:00 - 08:00 in the morning and 16:00 - 17:00 in the afternoon.\nConversely, if you don’t mind the crowd and don’t want to wait long for a train to arrive, it’s best to come these rush hours.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/index.html#caveats-and-limitations",
    "href": "posts/analyzing-manggarai-train-station/index.html#caveats-and-limitations",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "The rush hours are inferred from the amount of trains that arrive, which may not accurately represent the situation. However, the reasoning is solid because the increased number of trains likely corresponds to higher passenger volumes.\nThe recommended actions assume that people go to Manggarai station directly. However, most people come to Manggarai through other train stations, which mean they have less control over their arrival time. Fortunately, due to the generally consistent train speeds, people can predict their arrival time at Manggarai based on the departure time from their nearest stations.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/setting-up-web-analytics/index.html",
    "href": "posts/setting-up-web-analytics/index.html",
    "title": "Setting Up Web Analytics",
    "section": "",
    "text": "As a blogger about data analysis, it would be strange to not setup web analytics, as it’ll be a valuable data source for analysis."
  },
  {
    "objectID": "posts/setting-up-web-analytics/index.html#setup",
    "href": "posts/setting-up-web-analytics/index.html#setup",
    "title": "Setting Up Web Analytics",
    "section": "Setup",
    "text": "Setup\nFortunately, with Quarto, the setup is simple. Here is an excerpt taken from the docs:\n\nYou can add Google Analytics to your website by adding adding a google-analytics key to your _quarto.yml file. In its simplest form, you can just pass your Google Analytics tracking Id (e.g. UA-xxxxxxx) or Google Tag measurement Id (e.g. G-xxxxxxx) like:\nwebsite: \n  google-analytics: \"UA-XXXXXXXX\"\n\nHowever, there is another analytics I want to use alongside with Google Analytics: Statscounter. It’s only free for 500 visitors, but the report is much cleaner and easier to understand than Google Analytics’s.\nUnfortunately, Quarto doesn’t natively support it, so we have to go by the usual installation method: code insertion, precisely before\n\non every page.\nI tried to look about it on the website-tools page of the docs, but I couldn’t find it. Maybe someone else made it as an extension? Nope, not there either.\nSo, I Googled it and found someone asked a similar question. There, I found the answer. It turned out Quarto natively supports text/file insertion, which i could use for code insertion. It’s on the html-basics page of the docs, right on the #includes section. It’s located at the very bottom of the page, that’s probably why I missed it.\nNow, the _publish.yaml looks something like this:\nformat:\n  html:\n    theme:\n      light: cosmo\n      dark: darkly\n    css: styles.css\n    toc: true\n    include-in-header: \n      - text: |\n            &lt;!-- Default Statcounter code for Invictus Quarto https://invictus.quarto.pub --&gt;\n            &lt;script type=\"text/javascript\"&gt;\n            // actual script goes here\n            &lt;/script&gt;\n            &lt;!-- End of Statcounter Code --&gt;\nWe’re almost done here. There is only one thing left: Cookie Consent.\nYup, we don’t want to collect user’s data without their permission. Fortunately, Quarto also natively supports this. We just to toggle it on.\nwebsite:\n  cookie-consent: true"
  },
  {
    "objectID": "posts/setting-up-web-analytics/index.html#testing",
    "href": "posts/setting-up-web-analytics/index.html#testing",
    "title": "Setting Up Web Analytics",
    "section": "Testing",
    "text": "Testing\nNow let’s fire up the server and make sure if the scripts are loaded.\n\nNow, let’s see what happens when we turn off tracking cookie preference.\n\n\nNow the script becomes text/plain, effectively disabling it. That’s neat."
  },
  {
    "objectID": "posts/setting-up-web-analytics/index.html#closing",
    "href": "posts/setting-up-web-analytics/index.html#closing",
    "title": "Setting Up Web Analytics",
    "section": "Closing",
    "text": "Closing\nWith that, our web analytics setup is done. Ideally, we should setup Privacy Policy as well. We’ll do that some other time. For now, the cookie banner notice is enough\n\nIf there’s anything else you’d like to add, feel free to leave a comment down below. See you!"
  },
  {
    "objectID": "posts/drg-missions-dataset/index.html",
    "href": "posts/drg-missions-dataset/index.html",
    "title": "Case Study: Deep Rock Galactic Missions Data",
    "section": "",
    "text": "Rocks and stones may break my bones, but beer will never hurt me. gee, I’m feeling poetic today!\n\nAnyone who plays Deep Rock Galactic (DRG) must’ve heard of that phrase once. It’s my personal favorite.\nIf you’re not familiar with the game, DRG is an FPS Sandbox Generated type of game, where you’ll play as a miner that lives on space. You’ll be diving 1000km to the underground to accomplish a given mission. These missions are randomly generated (or is it?) every 30 minutes.\n\nInterestingly, these missions are the same for all players around the world, as long as you play on the same season.\nFor every mission, there’s a small chance that it’ll come with a mutator that changes that nature of the gameplay. One that most sought after is the ‘Double XP’ mutator, as you can guess, will double the amount of XP you’ll obtain.\nXP is very important during the early games, because your level will determine what weapons you can unlock and what upgrades you can purchase.\nTherefore, one of the reasons that inspired me to conduct this study is to discover the pattern of ‘Double XP’ mutator. I want to know whether it’s completely random or whether I can find out at which hours it usually appear.\nI’m a beginner myself in the game, only have about 40 hours gameplay. So, the result of this study will benefit me very much.\n\n\nThis work could’ve not been done without the massive effort by rolfosian, who has written a script to extract the missions data stored on the game. Additionally, he has a site that displays the current active missions updated in real-time, https://doublexp.net/. Feel free to check the source code his github repo.\n\n\n\nI haven’t run it myself on my PC. Fortunately, I discovered that he stores many of the collected missions in json format on the site. Not long after, I wrote simple python scripts to download, parse, and convert them to csv.\nIt was a lot of fun. Now, I want to rewrite the script in R and then finally analyze the data. The objective is to get a full daily missions data over July 2024. And then hopefully to find pattern on ‘Double XP’. Additionally, I’ll peek on other insights as well, because, why not?"
  },
  {
    "objectID": "posts/drg-missions-dataset/index.html#introduction",
    "href": "posts/drg-missions-dataset/index.html#introduction",
    "title": "Case Study: Deep Rock Galactic Missions Data",
    "section": "",
    "text": "Rocks and stones may break my bones, but beer will never hurt me. gee, I’m feeling poetic today!\n\nAnyone who plays Deep Rock Galactic (DRG) must’ve heard of that phrase once. It’s my personal favorite.\nIf you’re not familiar with the game, DRG is an FPS Sandbox Generated type of game, where you’ll play as a miner that lives on space. You’ll be diving 1000km to the underground to accomplish a given mission. These missions are randomly generated (or is it?) every 30 minutes.\n\nInterestingly, these missions are the same for all players around the world, as long as you play on the same season.\nFor every mission, there’s a small chance that it’ll come with a mutator that changes that nature of the gameplay. One that most sought after is the ‘Double XP’ mutator, as you can guess, will double the amount of XP you’ll obtain.\nXP is very important during the early games, because your level will determine what weapons you can unlock and what upgrades you can purchase.\nTherefore, one of the reasons that inspired me to conduct this study is to discover the pattern of ‘Double XP’ mutator. I want to know whether it’s completely random or whether I can find out at which hours it usually appear.\nI’m a beginner myself in the game, only have about 40 hours gameplay. So, the result of this study will benefit me very much.\n\n\nThis work could’ve not been done without the massive effort by rolfosian, who has written a script to extract the missions data stored on the game. Additionally, he has a site that displays the current active missions updated in real-time, https://doublexp.net/. Feel free to check the source code his github repo.\n\n\n\nI haven’t run it myself on my PC. Fortunately, I discovered that he stores many of the collected missions in json format on the site. Not long after, I wrote simple python scripts to download, parse, and convert them to csv.\nIt was a lot of fun. Now, I want to rewrite the script in R and then finally analyze the data. The objective is to get a full daily missions data over July 2024. And then hopefully to find pattern on ‘Double XP’. Additionally, I’ll peek on other insights as well, because, why not?"
  },
  {
    "objectID": "posts/drg-missions-dataset/index.html#collecting-data",
    "href": "posts/drg-missions-dataset/index.html#collecting-data",
    "title": "Case Study: Deep Rock Galactic Missions Data",
    "section": "Collecting Data",
    "text": "Collecting Data\nLet’s fire up our swiss-army knife, tidyverse.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nFrom the rolfosian’s project source code, I discovered that the json files are stored on https://doublexp.net/static/json/bulkmissions/{yyyy-mm-dd}.json. So let’s set that up as the base_url\n\njson_base_url &lt;- 'https://doublexp.net/static/json/bulkmissions/'\n\nThe base URL will be used to generate all the download links for each json file. Since the file names are formatted by ISO dates, it’s easy to generate using lubridate\n\nlibrary(lubridate)\nstart_date &lt;- ymd('2024-07-01')\nend_date &lt;- ymd('2024-07-31')\ndate_list &lt;- seq(start_date, end_date, by = 'day')\n\nAs simple as that. Now we have a list of date from July 1 to July 31\n\ndate_list\n\n [1] \"2024-07-01\" \"2024-07-02\" \"2024-07-03\" \"2024-07-04\" \"2024-07-05\"\n [6] \"2024-07-06\" \"2024-07-07\" \"2024-07-08\" \"2024-07-09\" \"2024-07-10\"\n[11] \"2024-07-11\" \"2024-07-12\" \"2024-07-13\" \"2024-07-14\" \"2024-07-15\"\n[16] \"2024-07-16\" \"2024-07-17\" \"2024-07-18\" \"2024-07-19\" \"2024-07-20\"\n[21] \"2024-07-21\" \"2024-07-22\" \"2024-07-23\" \"2024-07-24\" \"2024-07-25\"\n[26] \"2024-07-26\" \"2024-07-27\" \"2024-07-28\" \"2024-07-29\" \"2024-07-30\"\n[31] \"2024-07-31\"\n\n\nWe can simply use paste0 to put .json on them for the extension.\n\nfilename_list &lt;- paste0(date_list, '.json')\nfilename_list\n\n [1] \"2024-07-01.json\" \"2024-07-02.json\" \"2024-07-03.json\" \"2024-07-04.json\"\n [5] \"2024-07-05.json\" \"2024-07-06.json\" \"2024-07-07.json\" \"2024-07-08.json\"\n [9] \"2024-07-09.json\" \"2024-07-10.json\" \"2024-07-11.json\" \"2024-07-12.json\"\n[13] \"2024-07-13.json\" \"2024-07-14.json\" \"2024-07-15.json\" \"2024-07-16.json\"\n[17] \"2024-07-17.json\" \"2024-07-18.json\" \"2024-07-19.json\" \"2024-07-20.json\"\n[21] \"2024-07-21.json\" \"2024-07-22.json\" \"2024-07-23.json\" \"2024-07-24.json\"\n[25] \"2024-07-25.json\" \"2024-07-26.json\" \"2024-07-27.json\" \"2024-07-28.json\"\n[29] \"2024-07-29.json\" \"2024-07-30.json\" \"2024-07-31.json\"\n\n\nWe can also use paste0() to combine them with the base URL to get the download links\n\njson_urls &lt;- paste0(json_base_url, filename_list)\n\nNow we can use this list to download all the json files from doublexp.net. Let’s put a time recorder on it too, because why not 😆.\n\ndir.create('json')\n\nWarning in dir.create(\"json\"): 'json' already exists\n\nlibrary(tictoc)\n\n\ntic('Download all missions json for July 2024')\nmapply(download.file, json_urls, paste0('json/', filename_list))\ntoc()\n\ntrying URL 'https://doublexp.net/static/json/bulkmissions/2024-07-01.json'\nContent type 'application/json' length 261344 bytes (255 KB)\n==================================================\ndownloaded 255 KB\n...\nhttps://doublexp.net/static/json/bulkmissions/2024-07-01.json https://doublexp.net/static/json/bulkmissions/2024-07-01.json \n...\nDownload all missions json for July 2024: 64.759 sec elapsed\n64 seconds. Not bad. Now let’s parse it into a data frame. But first, we need to understand the structure of the JSON."
  },
  {
    "objectID": "posts/drg-missions-dataset/index.html#processing-data",
    "href": "posts/drg-missions-dataset/index.html#processing-data",
    "title": "Case Study: Deep Rock Galactic Missions Data",
    "section": "Processing Data",
    "text": "Processing Data\n\nParsing one JSON\nDue to its unstructured nature, JSON can be messy and hard to parse. Fortunately, the JSON we have here is not too complex. The good thing is, all the JSON are structured the same way. So, we only need to figure out a way to parse one JSON, to parse all of them.\nLet’s take a look at the JSON file.\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\n\njson_data &lt;- read_json('./json/2024-07-01.json')\n\n\njson_data |&gt; length()\n\n[1] 50\n\n\n\njson_data[c(1:3)] |&gt; glimpse()\n\nList of 3\n $ 2024-07-01T00:00:00Z:List of 2\n  ..$ timestamp: chr \"2024-07-01T00:00:00Z\"\n  ..$ Biomes   :List of 6\n  .. ..$ Glacial Strata            :List of 3\n  .. ..$ Crystalline Caverns       :List of 4\n  .. ..$ Sandblasted Corridors     :List of 5\n  .. ..$ Radioactive Exclusion Zone:List of 4\n  .. ..$ Dense Biozone             :List of 5\n  .. ..$ Hollow Bough              :List of 3\n $ 2024-07-01T00:30:00Z:List of 2\n  ..$ timestamp: chr \"2024-07-01T00:30:00Z\"\n  ..$ Biomes   :List of 5\n  .. ..$ Crystalline Caverns       :List of 4\n  .. ..$ Azure Weald               :List of 4\n  .. ..$ Fungus Bogs               :List of 7\n  .. ..$ Radioactive Exclusion Zone:List of 4\n  .. ..$ Dense Biozone             :List of 4\n $ 2024-07-01T01:00:00Z:List of 2\n  ..$ timestamp: chr \"2024-07-01T01:00:00Z\"\n  ..$ Biomes   :List of 5\n  .. ..$ Glacial Strata            :List of 5\n  .. ..$ Crystalline Caverns       :List of 3\n  .. ..$ Salt Pits                 :List of 9\n  .. ..$ Azure Weald               :List of 4\n  .. ..$ Radioactive Exclusion Zone:List of 4\n\n\nSo, the JSON has 4 levels:\n\nTimestamps\nBiomes\nThe biomes themselves\nMission for each biome (note that the fields under Biomes are also lists)\n\nI’ve explored the JSON with View(), so I got a pretty rough idea of the general structure. I found there are 2 fields with different structure at the end of the lists.\n\njson_data[c(49:50)] |&gt; glimpse()\n\nList of 2\n $ dailyDeal:List of 6\n  ..$ ResourceAmount: int 138\n  ..$ ChangePercent : num 57\n  ..$ DealType      : chr \"Buy\"\n  ..$ Credits       : int 8908\n  ..$ Resource      : chr \"Enor Pearl\"\n  ..$ timestamp     : chr \"2024-07-01T00:00:00Z\"\n $ ver      : int 4\n\n\nThe ver field is probably just an internal variable for doublexp.net. So it’s safe to remove it. dailyDeal however, is a useful data we can use to analyze the daily deal of the game. But we need to process it differently so it doesn’t interfere with the missions parsing.\nLet’s store in a separate variable, dailyDeal\n\ndailyDeal &lt;- json_data$dailyDeal\n\nNow, we can remove them from the json_data. I’m not comfortable mutating our JSON directly, so let’s assign it to a new variable.\n\ndrg_missions &lt;- json_data\ndrg_missions$dailyDeal &lt;- NULL\ndrg_missions$ver &lt;- NULL\n\nWe’re good to go. It’s time to unravel this JSON to a beautiful data frame.\n\ndrg_missions &lt;- tibble(drg_missions)\n\nUnfortunately, this comes the hard part, unnesting the JSON. tidyr provides a powerful set of tools to unnest a data. Yet, I barely able to wrap my head around it.\nBasically, there’s two main functions we’ll use:\n\nunnest_wider(): To unpack a list into columns\nunnest_longer(): To unpack a list into rows\n\nFirst, we’ll do unnest_wider() to make the timestamps as a new column.\n\ndrg_missions |&gt;\n  unnest_wider(drg_missions)\n\n# A tibble: 48 × 2\n   timestamp            Biomes          \n   &lt;chr&gt;                &lt;list&gt;          \n 1 2024-07-01T00:00:00Z &lt;named list [6]&gt;\n 2 2024-07-01T00:30:00Z &lt;named list [5]&gt;\n 3 2024-07-01T01:00:00Z &lt;named list [5]&gt;\n 4 2024-07-01T01:30:00Z &lt;named list [5]&gt;\n 5 2024-07-01T02:00:00Z &lt;named list [6]&gt;\n 6 2024-07-01T02:30:00Z &lt;named list [5]&gt;\n 7 2024-07-01T03:00:00Z &lt;named list [5]&gt;\n 8 2024-07-01T03:30:00Z &lt;named list [5]&gt;\n 9 2024-07-01T04:00:00Z &lt;named list [6]&gt;\n10 2024-07-01T04:30:00Z &lt;named list [5]&gt;\n# ℹ 38 more rows\n\n\nThen, we’ll unnest_longer() twice to unwrap the Biomes, and the biomes themselves (level 2 and 3).\n\ndrg_missions |&gt; \n  unnest_wider(drg_missions) |&gt; \n  unnest_longer(Biomes) |&gt; \n  unnest_longer(Biomes)\n\n# A tibble: 1,212 × 3\n   timestamp            Biomes           Biomes_id            \n   &lt;chr&gt;                &lt;list&gt;           &lt;chr&gt;                \n 1 2024-07-01T00:00:00Z &lt;named list [7]&gt; Glacial Strata       \n 2 2024-07-01T00:00:00Z &lt;named list [8]&gt; Glacial Strata       \n 3 2024-07-01T00:00:00Z &lt;named list [8]&gt; Glacial Strata       \n 4 2024-07-01T00:00:00Z &lt;named list [7]&gt; Crystalline Caverns  \n 5 2024-07-01T00:00:00Z &lt;named list [7]&gt; Crystalline Caverns  \n 6 2024-07-01T00:00:00Z &lt;named list [7]&gt; Crystalline Caverns  \n 7 2024-07-01T00:00:00Z &lt;named list [9]&gt; Crystalline Caverns  \n 8 2024-07-01T00:00:00Z &lt;named list [7]&gt; Sandblasted Corridors\n 9 2024-07-01T00:00:00Z &lt;named list [7]&gt; Sandblasted Corridors\n10 2024-07-01T00:00:00Z &lt;named list [7]&gt; Sandblasted Corridors\n# ℹ 1,202 more rows\n\n\nFinally, we’ll do unnests_wider() to unpack the mission details from each biome (level 4).\n\ndrg_missions_unnested &lt;- drg_missions |&gt; \n  unnest_wider(drg_missions) |&gt; \n  unnest_longer(Biomes) |&gt; \n  unnest_longer(Biomes) |&gt; \n  unnest_wider(Biomes)\n\ndrg_missions_unnested\n\n# A tibble: 1,212 × 11\n   timestamp      PrimaryObjective SecondaryObjective Complexity Length CodeName\n   &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;   \n 1 2024-07-01T00… Point Extraction Glyphid Eggs       3          2      Forbidd…\n 2 2024-07-01T00… Mining Expediti… ApocaBlooms        1          2      Duplici…\n 3 2024-07-01T00… Deep Scan        ApocaBlooms        2          1      Mythic …\n 4 2024-07-01T00… Mining Expediti… Glyphid Eggs       1          1      Blue Da…\n 5 2024-07-01T00… Egg Hunt         ApocaBlooms        1          1      Crumbly…\n 6 2024-07-01T00… On-Site Refining Dystrum            2          2      Frozen …\n 7 2024-07-01T00… Deep Scan        Glyphid Eggs       3          2      Ancient…\n 8 2024-07-01T00… Elimination      Fester Fleas       2          2      Clean D…\n 9 2024-07-01T00… Escort Duty      ApocaBlooms        2          2      Fragile…\n10 2024-07-01T00… Deep Scan        Hollomite          2          1      Awful A…\n# ℹ 1,202 more rows\n# ℹ 5 more variables: included_in &lt;list&gt;, id &lt;int&gt;, MissionWarnings &lt;list&gt;,\n#   MissionMutator &lt;chr&gt;, Biomes_id &lt;chr&gt;\n\n\nAt this point, the data frame is almost done. It looks exactly like how we want it to be. Except, 2 columns are still lists: MissionWarnings and Included_in. We could do unnest_longer() on them, but it’ll make duplicate rows, since the only thing different is them. So, the alternative is to use paste to join them as one string, separated by comma.\n\ndrg_missions_unnested |&gt; \n  select(included_in) |&gt; \n  slice(c(1:4))\n\n# A tibble: 4 × 1\n  included_in\n  &lt;list&gt;     \n1 &lt;list [3]&gt; \n2 &lt;list [3]&gt; \n3 &lt;list [1]&gt; \n4 &lt;list [3]&gt; \n\n\n\ndrg_missions_unnested &lt;- drg_missions_unnested |&gt; \n  mutate(included_in = map_chr(included_in, ~ paste(.x, collapse = \", \")))\n\ndrg_missions_unnested\n\n# A tibble: 1,212 × 11\n   timestamp      PrimaryObjective SecondaryObjective Complexity Length CodeName\n   &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;   \n 1 2024-07-01T00… Point Extraction Glyphid Eggs       3          2      Forbidd…\n 2 2024-07-01T00… Mining Expediti… ApocaBlooms        1          2      Duplici…\n 3 2024-07-01T00… Deep Scan        ApocaBlooms        2          1      Mythic …\n 4 2024-07-01T00… Mining Expediti… Glyphid Eggs       1          1      Blue Da…\n 5 2024-07-01T00… Egg Hunt         ApocaBlooms        1          1      Crumbly…\n 6 2024-07-01T00… On-Site Refining Dystrum            2          2      Frozen …\n 7 2024-07-01T00… Deep Scan        Glyphid Eggs       3          2      Ancient…\n 8 2024-07-01T00… Elimination      Fester Fleas       2          2      Clean D…\n 9 2024-07-01T00… Escort Duty      ApocaBlooms        2          2      Fragile…\n10 2024-07-01T00… Deep Scan        Hollomite          2          1      Awful A…\n# ℹ 1,202 more rows\n# ℹ 5 more variables: included_in &lt;chr&gt;, id &lt;int&gt;, MissionWarnings &lt;list&gt;,\n#   MissionMutator &lt;chr&gt;, Biomes_id &lt;chr&gt;\n\n\nPerfect! Later on, we’ll use the included_in to filter the missions available to us based on the season.\nThe lasts thing to convert is the MissionWarnings. We could combine them with paste() as well, but it’s a valuable data we can use to analyse the mission. It’s better to treat them as variables and separate it into two columns with unnest_wider().\n\ndrg_missions_unnested |&gt; \n  unnest_wider(MissionWarnings, names_sep = '_')\n\n# A tibble: 1,212 × 12\n   timestamp      PrimaryObjective SecondaryObjective Complexity Length CodeName\n   &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;   \n 1 2024-07-01T00… Point Extraction Glyphid Eggs       3          2      Forbidd…\n 2 2024-07-01T00… Mining Expediti… ApocaBlooms        1          2      Duplici…\n 3 2024-07-01T00… Deep Scan        ApocaBlooms        2          1      Mythic …\n 4 2024-07-01T00… Mining Expediti… Glyphid Eggs       1          1      Blue Da…\n 5 2024-07-01T00… Egg Hunt         ApocaBlooms        1          1      Crumbly…\n 6 2024-07-01T00… On-Site Refining Dystrum            2          2      Frozen …\n 7 2024-07-01T00… Deep Scan        Glyphid Eggs       3          2      Ancient…\n 8 2024-07-01T00… Elimination      Fester Fleas       2          2      Clean D…\n 9 2024-07-01T00… Escort Duty      ApocaBlooms        2          2      Fragile…\n10 2024-07-01T00… Deep Scan        Hollomite          2          1      Awful A…\n# ℹ 1,202 more rows\n# ℹ 6 more variables: included_in &lt;chr&gt;, id &lt;int&gt;, MissionWarnings_1 &lt;chr&gt;,\n#   MissionWarnings_2 &lt;chr&gt;, MissionMutator &lt;chr&gt;, Biomes_id &lt;chr&gt;\n\n\nPerfect! Now, we know how to parse 1 JSON. Let’s apply it to all the JSON we have and combine them into one giant tibble.\n\n\nParsing multiple JSON\nTo automate the algorithm above, we’d need to condense it to a function in which we can map it to all the JSON files.\n\nparse_json_drg_mission &lt;- function(json_path) {\n  json_data &lt;- read_json(json_path)\n  json_data$dailyDeal &lt;- NULL\n  json_data$ver &lt;- NULL\n  \n  drg_missions &lt;- tibble(drg_missions = json_data)\n  drg_missions &lt;-  drg_missions |&gt; \n    unnest_wider(drg_missions) |&gt; \n    unnest_longer(Biomes) |&gt; \n    unnest_longer(Biomes) |&gt; \n    unnest_wider(Biomes)\n  \n  drg_missions &lt;-  drg_missions |&gt; \n    unnest_wider(MissionWarnings, names_sep = '_') |&gt; \n    mutate(included_in = map_chr(included_in, ~ paste(.x, collapse = \", \")))\n  \n  return(drg_missions)\n}\n\n\nparse_json_drg_dailydeal &lt;- function(json_path) {\n  json_data &lt;- read_json(json_path)\n  dailyDeal &lt;- json_data$dailyDeal\n  dailyDeal &lt;- tibble(list(dailyDeal))\n  dailyDeal &lt;- dailyDeal |&gt; \n    unnest_wider(everything())\n  return(dailyDeal)\n}\n\nYou probably noticed why I reassign dailyDeal multiple times instead of just chaining the pipe operator. Please don’t ask why 😅. I spent more than 30 minutes debugging why the pipe operator messes up the code. In a nutshell, reassigning it preserves the ‘dailyDeal’ name inside the column, while pipe operator do the opposite.\nPreviously, it was supposed to be one function, returning a list of 2 tibbles. But that caused a lot of unexpected headache related to one I just mentioned. So I decided to just make separate functions to parse dailyDeal and missions.\nAnyway, here’s the output of the functions\n\nparse_json_drg_mission('./json/2024-07-01.json')\n\n# A tibble: 1,212 × 12\n   timestamp      PrimaryObjective SecondaryObjective Complexity Length CodeName\n   &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;   \n 1 2024-07-01T00… Point Extraction Glyphid Eggs       3          2      Forbidd…\n 2 2024-07-01T00… Mining Expediti… ApocaBlooms        1          2      Duplici…\n 3 2024-07-01T00… Deep Scan        ApocaBlooms        2          1      Mythic …\n 4 2024-07-01T00… Mining Expediti… Glyphid Eggs       1          1      Blue Da…\n 5 2024-07-01T00… Egg Hunt         ApocaBlooms        1          1      Crumbly…\n 6 2024-07-01T00… On-Site Refining Dystrum            2          2      Frozen …\n 7 2024-07-01T00… Deep Scan        Glyphid Eggs       3          2      Ancient…\n 8 2024-07-01T00… Elimination      Fester Fleas       2          2      Clean D…\n 9 2024-07-01T00… Escort Duty      ApocaBlooms        2          2      Fragile…\n10 2024-07-01T00… Deep Scan        Hollomite          2          1      Awful A…\n# ℹ 1,202 more rows\n# ℹ 6 more variables: included_in &lt;chr&gt;, id &lt;int&gt;, MissionWarnings_1 &lt;chr&gt;,\n#   MissionWarnings_2 &lt;chr&gt;, MissionMutator &lt;chr&gt;, Biomes_id &lt;chr&gt;\n\n\n\nparse_json_drg_dailydeal('./json/2024-07-01.json')\n\n# A tibble: 1 × 6\n  ResourceAmount ChangePercent DealType Credits Resource   timestamp           \n           &lt;int&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;               \n1            138          57.0 Buy         8908 Enor Pearl 2024-07-01T00:00:00Z\n\n\nWorking perfectly, finally. Now, we just need to apply these functions to the list of JSON file paths and combine each of them with bindrows().\n\nfilepath_list &lt;- paste0('json/',filename_list)\n\n\ntic('combining dailyDeal')\ndailyDeal_df_list &lt;- map(filepath_list, parse_json_drg_dailydeal)\ntoc()\n\ntic('combining missions')\nmissions_df_list &lt;- map(filepath_list, parse_json_drg_mission)\ntoc()\n\ncombining dailyDeal: 1.266 sec elapsed\ncombining missions: 7.144 sec elapsed\nWow, that’s quite a significant processing time. Let’s cache them to csv files so I don’t need to convert them again from scratch if I need to debug.\nBut first, we need to combine them into one data frames\n\ndailyDeal_df_combined &lt;- dailyDeal_df_list |&gt;\n  bind_rows()\nmissions_df_combined &lt;- missions_df_list |&gt; \n  bind_rows()\n\n\ndailyDeal_df_combined |&gt; write_csv('csv/drg_dailydeal_july_2024.csv')\nmissions_df_combined |&gt; write_csv('csv/drg_missions_july_2024.csv')\n\nNow we can read from these csv files instead.\n\ndailyDeal_df_combined &lt;- read_csv('csv/drg_dailydeal_july_2024.csv')\n\nRows: 31 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): DealType, Resource\ndbl  (3): ResourceAmount, ChangePercent, Credits\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmissions_df_combined &lt;- read_csv('csv/drg_missions_july_2024.csv', col_types = 'c')\n\n\ndailyDeal_df_combined\n\n# A tibble: 31 × 6\n   ResourceAmount ChangePercent DealType Credits Resource   timestamp          \n            &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dttm&gt;             \n 1            138          57.0 Buy         8908 Enor Pearl 2024-07-01 00:00:00\n 2            168          68.3 Buy         7982 Enor Pearl 2024-07-02 00:00:00\n 3             68          49.7 Buy         5130 Croppa     2024-07-03 00:00:00\n 4             98         266.  Sell       13057 Croppa     2024-07-04 00:00:00\n 5             95          36.0 Buy         9120 Magnite    2024-07-05 00:00:00\n 6            130          51.9 Buy         9369 Umanite    2024-07-06 00:00:00\n 7             87         213.  Sell        9264 Umanite    2024-07-07 00:00:00\n 8             60          44.7 Buy         4978 Jadiz      2024-07-08 00:00:00\n 9             55          47.9 Buy         4294 Croppa     2024-07-09 00:00:00\n10             56         152.  Sell        4258 Enor Pearl 2024-07-10 00:00:00\n# ℹ 21 more rows\n\n\n\nmissions_df_combined\n\n# A tibble: 38,521 × 12\n   timestamp      PrimaryObjective SecondaryObjective Complexity Length CodeName\n   &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1 2024-07-01T00… Point Extraction Glyphid Eggs                3      2 Forbidd…\n 2 2024-07-01T00… Mining Expediti… ApocaBlooms                 1      2 Duplici…\n 3 2024-07-01T00… Deep Scan        ApocaBlooms                 2      1 Mythic …\n 4 2024-07-01T00… Mining Expediti… Glyphid Eggs                1      1 Blue Da…\n 5 2024-07-01T00… Egg Hunt         ApocaBlooms                 1      1 Crumbly…\n 6 2024-07-01T00… On-Site Refining Dystrum                     2      2 Frozen …\n 7 2024-07-01T00… Deep Scan        Glyphid Eggs                3      2 Ancient…\n 8 2024-07-01T00… Elimination      Fester Fleas                2      2 Clean D…\n 9 2024-07-01T00… Escort Duty      ApocaBlooms                 2      2 Fragile…\n10 2024-07-01T00… Deep Scan        Hollomite                   2      1 Awful A…\n# ℹ 38,511 more rows\n# ℹ 6 more variables: included_in &lt;chr&gt;, id &lt;dbl&gt;, MissionWarnings_1 &lt;chr&gt;,\n#   MissionWarnings_2 &lt;chr&gt;, MissionMutator &lt;chr&gt;, Biomes_id &lt;chr&gt;\n\n\nNice. Now we have tidy data frames we can work with. The last step before it’s 100% ready to be analyzed is to clean it.\n\n\nCleaning Data\n\nmissions_df_combined_cleaned &lt;- missions_df_combined\n\n\nNon-tidy Data\nIn his book R for Data Science, Hadley introduces a concept of Tidy Data, which makes data analysis much easier:\n\n\nEach column is a variable.\nEach row is an observation.\nEach cell is a single value.\n\n\nFortunately, it looks like we’ve passed 3 rules, so our data is already tidy.\n\n\nImproper Column Names\nIt’s good enough as it is, but we can make it even better.\n\nInconsistencies: most of the column names are in PascalCase, but included_in is in snake_case, and timestamp is just all lowercase.\nGrammatically Wrong: MissionWarnings_1, MissionWarnings_2, and Biomes_id only contain one value in each row, so they should be singular\nMisc: Biomes_id just sounds weird. “id” shouldn’t be there.\n\n\nmissions_df_combined_cleaned &lt;- missions_df_combined_cleaned |&gt; \n  rename(Timestamp = timestamp,\n         IncludedIn = included_in,\n         Biome = Biomes_id,\n         MissionWarning1 = MissionWarnings_1,\n         MissionWarning2 = MissionWarnings_2)\n\n\nmissions_df_combined_cleaned |&gt; \n  names()\n\n [1] \"Timestamp\"          \"PrimaryObjective\"   \"SecondaryObjective\"\n [4] \"Complexity\"         \"Length\"             \"CodeName\"          \n [7] \"IncludedIn\"         \"id\"                 \"MissionWarning1\"   \n[10] \"MissionWarning2\"    \"MissionMutator\"     \"Biome\"             \n\n\n\n\nIrrelevant Data\nPreviously, I mentioned that we’d only analyze missions data that is usually availabe to most people. Which is the unseasoned ones. In this data, it’s the s0 from included_in. So we can filter only the rows with s0.\n\nmissions_df_combined_cleaned &lt;- missions_df_combined_cleaned |&gt; \n  filter(IncludedIn |&gt; str_detect('s0'))\n\n\nmissions_df_combined_cleaned\n\n# A tibble: 28,272 × 12\n   Timestamp      PrimaryObjective SecondaryObjective Complexity Length CodeName\n   &lt;chr&gt;          &lt;chr&gt;            &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1 2024-07-01T00… Point Extraction Glyphid Eggs                3      2 Forbidd…\n 2 2024-07-01T00… Mining Expediti… ApocaBlooms                 1      2 Duplici…\n 3 2024-07-01T00… Mining Expediti… Glyphid Eggs                1      1 Blue Da…\n 4 2024-07-01T00… Egg Hunt         ApocaBlooms                 1      1 Crumbly…\n 5 2024-07-01T00… On-Site Refining Dystrum                     2      2 Frozen …\n 6 2024-07-01T00… Deep Scan        Glyphid Eggs                3      2 Ancient…\n 7 2024-07-01T00… Elimination      Fester Fleas                2      2 Clean D…\n 8 2024-07-01T00… Escort Duty      ApocaBlooms                 2      2 Fragile…\n 9 2024-07-01T00… Deep Scan        Hollomite                   2      1 Awful A…\n10 2024-07-01T00… Industrial Sabo… Hollomite                   2      2 Dreadfu…\n# ℹ 28,262 more rows\n# ℹ 6 more variables: IncludedIn &lt;chr&gt;, id &lt;dbl&gt;, MissionWarning1 &lt;chr&gt;,\n#   MissionWarning2 &lt;chr&gt;, MissionMutator &lt;chr&gt;, Biome &lt;chr&gt;\n\n\n\n\nIncorrect Data Types\nSince we manually extracted and parsed the data from the JSON files, R didn’t assume any data type and just render everything as chr. Which mean, a lot of them needs to be changed.\n\nFactors\nBiomes, PrimaryObjective, SecondaryObjective, MissionMutator, MissionWarning1, and MissionWarning2 should be factors, since they’re categorical variables with limited set of values. Complexity and Length are a bit tricky since they’re ordinal variables, which can be treated as either qualitative or quantitative. Claude Sonnet 3.5, however, argues that since we don’t know the exact interval between 1 to 2, or 2 to 3, they should be treated as categorical. Sonnet 3.5 has helped me a lot, so I’ll trust her 🤣.\n\nBased on the information provided, your ‘Complexity Level’ variable is an ordinal categorical variable. Even though it’s represented by numbers (1, 2, 3), it’s not truly numeric in nature, as the intervals between levels may not be equal.\n\nCharacters\nCodeName can stay as chr since they don’t really hold any values other than being randomly generated names. included_in should probably be factors, but the structure isn’t standardized yet, and we don’t need to deal with them anyway.\nIntegers\nid should be integers. Oh wait, it’s already is! I didn’t know that.\nDate\nThis one is obvious. The Timestamp column should be in date object format. Otherwise, we can’t do any date computation with it.\n\n\nmissions_df_combined_cleaned &lt;- missions_df_combined_cleaned |&gt; \n  mutate(across(c(Biome, PrimaryObjective, SecondaryObjective, MissionMutator, MissionWarning1, MissionWarning2, Complexity, Length),\n         as_factor)) |&gt; \n  mutate(Timestamp = ymd_hms(Timestamp))\n\nmissions_df_combined_cleaned\n\n# A tibble: 28,272 × 12\n   Timestamp           PrimaryObjective    SecondaryObjective Complexity Length\n   &lt;dttm&gt;              &lt;fct&gt;               &lt;fct&gt;              &lt;fct&gt;      &lt;fct&gt; \n 1 2024-07-01 00:00:00 Point Extraction    Glyphid Eggs       3          2     \n 2 2024-07-01 00:00:00 Mining Expedition   ApocaBlooms        1          2     \n 3 2024-07-01 00:00:00 Mining Expedition   Glyphid Eggs       1          1     \n 4 2024-07-01 00:00:00 Egg Hunt            ApocaBlooms        1          1     \n 5 2024-07-01 00:00:00 On-Site Refining    Dystrum            2          2     \n 6 2024-07-01 00:00:00 Deep Scan           Glyphid Eggs       3          2     \n 7 2024-07-01 00:00:00 Elimination         Fester Fleas       2          2     \n 8 2024-07-01 00:00:00 Escort Duty         ApocaBlooms        2          2     \n 9 2024-07-01 00:00:00 Deep Scan           Hollomite          2          1     \n10 2024-07-01 00:00:00 Industrial Sabotage Hollomite          2          2     \n# ℹ 28,262 more rows\n# ℹ 7 more variables: CodeName &lt;chr&gt;, IncludedIn &lt;chr&gt;, id &lt;dbl&gt;,\n#   MissionWarning1 &lt;fct&gt;, MissionWarning2 &lt;fct&gt;, MissionMutator &lt;fct&gt;,\n#   Biome &lt;fct&gt;\n\n\n\n\nDuplicate Data\nWe also need to scan for duplicates. Assuming the extractor script is robust, there shouldn’t be any. But Just in case, we should still check.\nFirst, we can check the frequency count of each id, since it’s supposed to be the unique indentifier.\n\nmissions_df_combined_cleaned |&gt; \n  count(id, sort = TRUE)\n\n# A tibble: 28,272 × 2\n      id     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  9722     1\n 2  9723     1\n 3  9725     1\n 4  9726     1\n 5  9727     1\n 6  9728     1\n 7  9729     1\n 8  9730     1\n 9  9731     1\n10  9732     1\n# ℹ 28,262 more rows\n\n\nThere’s no duplicate id, great. The next column we could check is the codeName\n\nmissions_df_combined_cleaned |&gt; \n  count(CodeName, sort = TRUE)\n\n# A tibble: 20,867 × 2\n   CodeName             n\n   &lt;chr&gt;            &lt;int&gt;\n 1 Corrupt Trail        8\n 2 Gangrenous End       7\n 3 Rejected Shelter     7\n 4 Burning Comeback     6\n 5 Burning Desert       6\n 6 Sharp Trench         6\n 7 True Ravine          6\n 8 Wild Anvil           6\n 9 Wild Edge            6\n10 Wild End             6\n# ℹ 20,857 more rows\n\n\nThere’s quite several repetitions. However, as long as they’re in different time, it’s safe. So that’s our third comparison: timestamp\n\nmissions_df_combined_cleaned |&gt; \n  count(CodeName, Timestamp, sort = TRUE)\n\n# A tibble: 28,272 × 3\n   CodeName          Timestamp               n\n   &lt;chr&gt;             &lt;dttm&gt;              &lt;int&gt;\n 1 Abandoned Abyss   2024-07-10 01:30:00     1\n 2 Abandoned Agony   2024-07-06 17:00:00     1\n 3 Abandoned Arm     2024-07-13 14:30:00     1\n 4 Abandoned Barrens 2024-07-04 17:30:00     1\n 5 Abandoned Base    2024-07-13 22:30:00     1\n 6 Abandoned Base    2024-07-14 07:00:00     1\n 7 Abandoned Basin   2024-07-20 22:30:00     1\n 8 Abandoned Bed     2024-07-12 20:30:00     1\n 9 Abandoned Bedrock 2024-07-10 11:30:00     1\n10 Abandoned Benefit 2024-07-31 10:00:00     1\n# ℹ 28,262 more rows\n\n\nYup, all unique. We could also pick one codename and see its time pattern:\n\nmissions_df_combined_cleaned |&gt; \n  count(CodeName, Timestamp, sort = TRUE) |&gt; \n  filter(CodeName == 'Corrupt Trail')\n\n# A tibble: 8 × 3\n  CodeName      Timestamp               n\n  &lt;chr&gt;         &lt;dttm&gt;              &lt;int&gt;\n1 Corrupt Trail 2024-07-09 12:30:00     1\n2 Corrupt Trail 2024-07-09 15:00:00     1\n3 Corrupt Trail 2024-07-11 03:00:00     1\n4 Corrupt Trail 2024-07-15 00:00:00     1\n5 Corrupt Trail 2024-07-16 04:00:00     1\n6 Corrupt Trail 2024-07-25 00:00:00     1\n7 Corrupt Trail 2024-07-27 13:00:00     1\n8 Corrupt Trail 2024-07-31 04:00:00     1\n\n\nNo particular pattern visible, but we can see they’re on different days. Only one on the same day, but with 3 hours difference.\nAlright, all set. We’re good to go."
  },
  {
    "objectID": "posts/drg-missions-dataset/index.html#analyzing-data",
    "href": "posts/drg-missions-dataset/index.html#analyzing-data",
    "title": "Case Study: Deep Rock Galactic Missions Data",
    "section": "Analyzing Data",
    "text": "Analyzing Data\n\nMission Pattern\n\nOverall Distribution\nBefore we analyze the pattern of Double XP’s mission mutator, let’s first see the overall distribution of mission mutators\n\nmissions_df_combined_cleaned |&gt; \n  ggplot(aes(MissionMutator)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nI’m not sure why NA values are also visualized, since they’re usually implicitly removed.\n\nmissions_df_combined_cleaned |&gt; \n  select(MissionMutator) |&gt; \n  filter(!is.na(MissionMutator)) |&gt; \n  ggplot(aes(MissionMutator)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNow the NA values are removed, the labels are hard to read due to how many it is. Let’s try mitigating this by wrapping the labels to newlines\n\nmissions_df_combined_cleaned |&gt; \n  select(MissionMutator) |&gt; \n  filter(!is.na(MissionMutator)) |&gt; \n  ggplot(aes(MissionMutator)) +\n  geom_bar() +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 5))\n\n\n\n\n\n\n\n\nNow it looks much better. We can reorder the bar descendingly with fct_infreq() to see what’s the highest and lowest, but honestly, they all don’t seem much differ in values, except for Blood Sugar.\n\nmissions_df_combined_cleaned |&gt; \n  select(MissionMutator) |&gt; \n  filter(!is.na(MissionMutator)) |&gt; \n  ggplot(aes(fct_infreq(MissionMutator))) +\n  geom_bar() +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 5))\n\n\n\n\n\n\n\n\nSee? Well, I guess this is to be expected as it’s generated by computers 😅.\n\n\nDouble XP Pattern\nFinally, we reach the main objective.\nSince we have the complete data with complete timestamp for the entire month, there’s various ways we can visualize the pattern.\n\nHourly\nDaily\nWeekly\n\nTo make our analysis easier, we need to extract each component of the date from the Timestamp, which are: week days (names), days (numbers), and hours\n\ndouble_xp_trend &lt;- missions_df_combined_cleaned |&gt; \n  mutate(WeekDay = wday(Timestamp, label=TRUE),\n         Day = day(Timestamp),\n         Hour = hour(Timestamp),\n         .after = Timestamp) |&gt; \n  filter(MissionMutator == 'Double XP')\n\n\ndouble_xp_trend\n\n# A tibble: 613 × 15\n   Timestamp           WeekDay   Day  Hour PrimaryObjective   SecondaryObjective\n   &lt;dttm&gt;              &lt;ord&gt;   &lt;int&gt; &lt;int&gt; &lt;fct&gt;              &lt;fct&gt;             \n 1 2024-07-01 03:00:00 Mon         1     3 Elimination        Hollomite         \n 2 2024-07-01 03:30:00 Mon         1     3 On-Site Refining   Gunk Seeds        \n 3 2024-07-01 04:00:00 Mon         1     4 Point Extraction   Dystrum           \n 4 2024-07-01 05:00:00 Mon         1     5 Mining Expedition  Bha Barnacles     \n 5 2024-07-01 06:30:00 Mon         1     6 Deep Scan          Fossils           \n 6 2024-07-01 10:30:00 Mon         1    10 Mining Expedition  Ebonuts           \n 7 2024-07-01 10:30:00 Mon         1    10 Elimination        Ebonuts           \n 8 2024-07-01 10:30:00 Mon         1    10 Industrial Sabota… Hollomite         \n 9 2024-07-01 11:00:00 Mon         1    11 Mining Expedition  Gunk Seeds        \n10 2024-07-01 11:30:00 Mon         1    11 Deep Scan          Fester Fleas      \n# ℹ 603 more rows\n# ℹ 9 more variables: Complexity &lt;fct&gt;, Length &lt;fct&gt;, CodeName &lt;chr&gt;,\n#   IncludedIn &lt;chr&gt;, id &lt;dbl&gt;, MissionWarning1 &lt;fct&gt;, MissionWarning2 &lt;fct&gt;,\n#   MissionMutator &lt;fct&gt;, Biome &lt;fct&gt;\n\n\n\nWeekly\nLet’s compute the average occurrence per day. We can obtain it by first computing the total count each day, and then get the mean from them.\n\ndouble_xp_trend_weekly &lt;- double_xp_trend |&gt; \n  count(Day, WeekDay) |&gt; \n  group_by(WeekDay) |&gt; \n  summarize(mean = mean(n))\n\ndouble_xp_trend_weekly\n\n# A tibble: 7 × 2\n  WeekDay  mean\n  &lt;ord&gt;   &lt;dbl&gt;\n1 Sun      21.2\n2 Mon      18.6\n3 Tue      20.2\n4 Wed      19  \n5 Thu      17  \n6 Fri      20.2\n7 Sat      22.5\n\n\nIt looks like there isn’t much difference between the day. We can verify it by checking the SD.\n\ndouble_xp_trend_weekly |&gt; \n  pull(mean) |&gt; \n  sd()\n\n[1] 1.810584\n\n\nYeah, that’s minuscule. But still, let’s visualize it to get a better picture.\n\ndouble_xp_trend_weekly |&gt;  \n  ggplot(aes(WeekDay, mean, group = 1)) +  \n  geom_line() +\n  geom_label(aes(label = mean)) +\n  labs(x='Week Day', y='Averge Occurence (mean)', title = 'Average Occurence (Mean) of Double XP Each Day')\n\n\n\n\n\n\n\n\nSo the peak is on Saturday and the trough is on Thursday. Still, if you think about it as a gamer, it really isnt’ much of a difference, except maybe for Thursday. 20 Double XP a day is a lot. One game can range from 30 to 60 minutes. Even if you stay up all day, you can’t get all of them anyway.\n\n\nDaily\n\ndouble_xp_trend |&gt; \n  count(Day, WeekDay) |&gt; \n  ggplot(aes(Day, n)) +\n  geom_line() +\n  scale_x_continuous(breaks = c(1:31)) +\n  labs(title='Daily Occurences of Double XP')\n\n\n\n\n\n\n\n\nWhen we zoom out and see the pattern throughout the month, we can see a repeating up and down cycle that’s going down nearing the end of the month. I wish we have data from the other months to complete the pattern.\n\n\nHourly\n\ndouble_xp_trend |&gt; \n  count(Day, Hour) |&gt; \n  group_by(Hour) |&gt; \n  summarize(mean = mean(n))\n\n# A tibble: 24 × 2\n    Hour  mean\n   &lt;int&gt; &lt;dbl&gt;\n 1     0  1.64\n 2     1  1.8 \n 3     2  1.31\n 4     3  1.43\n 5     4  1.53\n 6     5  1.45\n 7     6  1.35\n 8     7  1.44\n 9     8  1.23\n10     9  1.53\n# ℹ 14 more rows\n\n\n\ndouble_xp_trend |&gt; \n  count(Hour) |&gt; \n  ggplot(aes(Hour, n)) +\n  geom_line() +\n  scale_x_continuous(breaks = c(0:23))\n\n\n\n\n\n\n\n\nIf we zoom in on the hours and sum all the occurrences on each hour, we can see that the peak is on 10:00 UTC. But is it really that much of a difference compared to the rest? As we did previously, we can use the mean instead to see how many occurence is it on average.\n\ndouble_xp_trend_hourly &lt;- double_xp_trend |&gt; \n  count(Day, Hour) |&gt; \n  group_by(Hour) |&gt; \n  summarize(mean = mean(n))\n\ndouble_xp_trend_hourly |&gt;\n  ggplot(aes(Hour, mean)) +\n  geom_line() +\n  scale_x_continuous(breaks = 0:23)\n\n\n\n\n\n\n\n\nOhhh, that’s unexpected. So on average, the highest is not on 10:00 UTC, but on 01:00 UTC, which was quite low when we see the the total count. This might indicate a presence of outliers.\nAnyway, it doesn’t matter. The range is only about 1 difference, from 1 to 2. That’s meaningless when you consider the game duration per mission. Once you finish one, a new mission will be generated."
  },
  {
    "objectID": "posts/drg-missions-dataset/index.html#summary",
    "href": "posts/drg-missions-dataset/index.html#summary",
    "title": "Case Study: Deep Rock Galactic Missions Data",
    "section": "Summary",
    "text": "Summary\n\nInsight 1: Double XP Pattern\nStatistically, there are trends and pattern of peaks and troughs, of when Double XP most likely to happen.\nUnfortunately, from the gamer’s perspective, the difference is not significant enough. On Weekly difference, you don’t play game all day. On hourly difference, a new mission will be generated once you played.\nIn short, there is no strategy that we, as the gamers, can use to find the right time to play. Fortunately, since we have the complete timestamps, we can use it to instead snipe the Double XP missions directly.\n\n\nInsight 2: Mutators Occurrences\nThere’s another interesting insight, though. The hourly average is about 1.5. However, we know that the game mission refreshes every 30 minutes. And there’s like 20 missions at once. Which mean, on average, there can only be 1 Double XP mission in every cycle.\nBut remember, we have filtered the data to only include missions that are Double XP. If we have included all mutators, it’s highly possible they have similar pattern, considering the frequency of all mutators are highly similar."
  },
  {
    "objectID": "posts/drg-missions-dataset/index.html#appendix-other-findings",
    "href": "posts/drg-missions-dataset/index.html#appendix-other-findings",
    "title": "Case Study: Deep Rock Galactic Missions Data",
    "section": "Appendix: Other Findings",
    "text": "Appendix: Other Findings\nBelow, you can find other findings that aren’t related to Double XP, but might be interesting to know.\n\nMissions\n\nmissions_df_combined_cleaned\n\n# A tibble: 28,272 × 12\n   Timestamp           PrimaryObjective    SecondaryObjective Complexity Length\n   &lt;dttm&gt;              &lt;fct&gt;               &lt;fct&gt;              &lt;fct&gt;      &lt;fct&gt; \n 1 2024-07-01 00:00:00 Point Extraction    Glyphid Eggs       3          2     \n 2 2024-07-01 00:00:00 Mining Expedition   ApocaBlooms        1          2     \n 3 2024-07-01 00:00:00 Mining Expedition   Glyphid Eggs       1          1     \n 4 2024-07-01 00:00:00 Egg Hunt            ApocaBlooms        1          1     \n 5 2024-07-01 00:00:00 On-Site Refining    Dystrum            2          2     \n 6 2024-07-01 00:00:00 Deep Scan           Glyphid Eggs       3          2     \n 7 2024-07-01 00:00:00 Elimination         Fester Fleas       2          2     \n 8 2024-07-01 00:00:00 Escort Duty         ApocaBlooms        2          2     \n 9 2024-07-01 00:00:00 Deep Scan           Hollomite          2          1     \n10 2024-07-01 00:00:00 Industrial Sabotage Hollomite          2          2     \n# ℹ 28,262 more rows\n# ℹ 7 more variables: CodeName &lt;chr&gt;, IncludedIn &lt;chr&gt;, id &lt;dbl&gt;,\n#   MissionWarning1 &lt;fct&gt;, MissionWarning2 &lt;fct&gt;, MissionMutator &lt;fct&gt;,\n#   Biome &lt;fct&gt;\n\n\n\nmissions_df_other_findings &lt;- missions_df_combined_cleaned |&gt; \n  mutate(WeekDay = wday(Timestamp, label=TRUE),\n         Day = day(Timestamp),\n         Hour = hour(Timestamp),\n         .after = Timestamp)\n\n\nBiome\n\n\nCode\nmissions_df_other_findings |&gt;\n  count(Day, Biome, name = 'Total_Occurences') |&gt; \n  group_by(Biome) |&gt; \n  summarize(Daily_Occurence = mean(Total_Occurences)) |&gt; \n  arrange(desc(Daily_Occurence)) |&gt; \n  ggplot(aes(Daily_Occurence, fct_rev(fct_infreq(Biome, Daily_Occurence)))) +\n  geom_col() +\n  labs(title = 'Most Common Biomes', subtitle = 'Based on its Mean of Daily Occurences', x = 'Daily Occurence', y = 'Biomes')\n\n\n\n\n\n\n\n\n\n\n\nPrimary Objective\n\n\nCode\nmissions_df_other_findings |&gt; \n  count(Day, PrimaryObjective, name = 'Total_Occurences') |&gt; \n  group_by(PrimaryObjective) |&gt; \n  summarize(Daily_Occurence = mean(Total_Occurences)) |&gt; \n  ggplot(aes(fct_infreq(PrimaryObjective, Daily_Occurence), Daily_Occurence)) +\n  geom_col() +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n  labs(title = 'Most Common Primary Objectives', subtitle = 'Based on its Mean of Daily Occurences', x = 'Primary Objective', y = 'Daily Occurence')\n\n\n\n\n\n\n\n\n\n\n\nSecondary Objective\n\n\nCode\nmissions_df_other_findings |&gt;\n  mutate(SecondaryObjective = str_replace(SecondaryObjective, 'ApocaBlooms', 'Apoca Bloom')) |&gt; \n  count(Day, SecondaryObjective, name = 'Total_Occurences') |&gt; \n  group_by(SecondaryObjective) |&gt; \n  summarize(Daily_Occurence = mean(Total_Occurences)) |&gt; \n  ggplot(aes(fct_rev(fct_infreq(SecondaryObjective, Daily_Occurence)), Daily_Occurence)) +\n  geom_col() +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n  labs(title = 'Most Common Secondary Objectives', subtitle = 'Based on its Mean of Daily Occurences', x = 'Secondary Objective', y = 'Daily Occurence')\n\n\n\n\n\n\n\n\n\n\n\nMission Warnings\n\nPrimary Warning\n\n\nCode\nmissions_df_other_findings |&gt;\n  filter(!is.na(MissionWarning1)) |&gt; \n  count(Day, MissionWarning1, name = 'Total_Occurences') |&gt; \n  group_by(MissionWarning1) |&gt; \n  summarize(Daily_Occurence = mean(Total_Occurences)) |&gt; \n  arrange(desc(Daily_Occurence)) |&gt; \n  head(5) |&gt; \n  ggplot(aes(Daily_Occurence, fct_rev(fct_infreq(MissionWarning1, Daily_Occurence)))) +\n  geom_col() +\n  labs(title = 'Top Five Most Common Frimary Mission Warnings', subtitle = 'Based on its Mean of Daily Occurences', x = 'Daily Occurence', y = 'Primary Mission Warnings')\n\n\n\n\n\n\n\n\n\n\n\nCode\nmissions_df_other_findings |&gt;\n  filter(!is.na(MissionWarning1)) |&gt; \n  count(Day, MissionWarning1, name = 'Total_Occurences') |&gt; \n  group_by(MissionWarning1) |&gt; \n  summarize(Daily_Occurence = mean(Total_Occurences)) |&gt; \n  arrange(Daily_Occurence) |&gt; \n  head(5) |&gt; \n  ggplot(aes(Daily_Occurence, fct_infreq(MissionWarning1, Daily_Occurence))) +\n  geom_col() +\n  labs(title = 'Top Five Rarest Primary Mission Warnings', subtitle = 'Based on its Mean of Daily Occurences', x = 'Daily Occurence', y = 'Primary Mission Warnings')\n\n\n\n\n\n\n\n\n\n\n\n\nMission Warnings Combinations\n\nTop 10\n\n\nCode\nmissions_df_other_findings |&gt; \n  filter(!is.na(MissionWarning1), !is.na(MissionWarning2)) |&gt; \n  count(MissionWarning1, MissionWarning2, name = 'Total_Occurence') |&gt; \n  arrange(desc(Total_Occurence))\n\n\n# A tibble: 91 × 3\n   MissionWarning1      MissionWarning2      Total_Occurence\n   &lt;fct&gt;                &lt;fct&gt;                          &lt;int&gt;\n 1 Elite Threat         Regenerative Bugs                 28\n 2 Duck and Cover       Regenerative Bugs                 27\n 3 Exploder Infestation Lethal Enemies                    27\n 4 Duck and Cover       Exploder Infestation              26\n 5 Elite Threat         Parasites                         25\n 6 Parasites            Swarmageddon                      25\n 7 Shield Disruption    Swarmageddon                      24\n 8 Regenerative Bugs    Swarmageddon                      24\n 9 Elite Threat         Mactera Plague                    24\n10 Mactera Plague       Parasites                         24\n# ℹ 81 more rows\n\n\n\n\nLowest 10\n\n\nCode\nmissions_df_other_findings |&gt; \n  filter(!is.na(MissionWarning1), !is.na(MissionWarning2)) |&gt; \n  count(MissionWarning1, MissionWarning2, name = 'Total_Occurence') |&gt; \n  arrange(Total_Occurence)\n\n\n# A tibble: 91 × 3\n   MissionWarning1    MissionWarning2      Total_Occurence\n   &lt;fct&gt;              &lt;fct&gt;                          &lt;int&gt;\n 1 Cave Leech Cluster Haunted Cave                       5\n 2 Haunted Cave       Rival Presence                     6\n 3 Haunted Cave       Shield Disruption                  7\n 4 Haunted Cave       Low Oxygen                         8\n 5 Ebonite Outbreak   Haunted Cave                       8\n 6 Duck and Cover     Ebonite Outbreak                   9\n 7 Elite Threat       Rival Presence                     9\n 8 Ebonite Outbreak   Rival Presence                     9\n 9 Ebonite Outbreak   Exploder Infestation               9\n10 Low Oxygen         Mactera Plague                     9\n# ℹ 81 more rows\n\n\n\n\n\n\nDaily Deal\n\ndailyDeal_df_combined\n\n# A tibble: 31 × 6\n   ResourceAmount ChangePercent DealType Credits Resource   timestamp          \n            &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dttm&gt;             \n 1            138          57.0 Buy         8908 Enor Pearl 2024-07-01 00:00:00\n 2            168          68.3 Buy         7982 Enor Pearl 2024-07-02 00:00:00\n 3             68          49.7 Buy         5130 Croppa     2024-07-03 00:00:00\n 4             98         266.  Sell       13057 Croppa     2024-07-04 00:00:00\n 5             95          36.0 Buy         9120 Magnite    2024-07-05 00:00:00\n 6            130          51.9 Buy         9369 Umanite    2024-07-06 00:00:00\n 7             87         213.  Sell        9264 Umanite    2024-07-07 00:00:00\n 8             60          44.7 Buy         4978 Jadiz      2024-07-08 00:00:00\n 9             55          47.9 Buy         4294 Croppa     2024-07-09 00:00:00\n10             56         152.  Sell        4258 Enor Pearl 2024-07-10 00:00:00\n# ℹ 21 more rows\n\n\n\nOverall Trend\n\n\nCode\ndailyDeal_df_combined |&gt; \n  ggplot(aes(timestamp,Credits, colour = DealType)) +\n  geom_line() +\n  labs(title = 'Trend of Daily Deal over July 2024', color='Deal Type', x='Date', y='Credits Offered')\n\n\n\n\n\n\n\n\n\n\n\nCommon Resource on Sale\n\n\nCode\ndailyDeal_df_combined |&gt; \n  count(Resource) |&gt; \n  ggplot(aes(fct_infreq(Resource, n), n)) +\n  geom_col() +\n  labs(title = 'Most Common Resource on Sale', subtitle = 'Based on total occurences over July 2024', x = 'Resource', y = 'Total Occurence')"
  },
  {
    "objectID": "posts/amazon-prime-titles-eda/index.html",
    "href": "posts/amazon-prime-titles-eda/index.html",
    "title": "EDA on Amazon Prime Movies and TV Shows",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(naniar)\nlibrary(visdat)\nlibrary(DataExplorer)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(validate)\n\n\nAttaching package: 'validate'\n\nThe following object is masked from 'package:naniar':\n\n    all_complete\n\nThe following object is masked from 'package:dplyr':\n\n    expr\n\nThe following object is masked from 'package:ggplot2':\n\n    expr\n\n\n\n\n\n\ndf &lt;- read_csv('./amazon_prime_titles.csv')\n\nRows: 9668 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): show_id, type, title, director, cast, country, date_added, rating,...\ndbl  (1): release_year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/amazon-prime-titles-eda/index.html#setup",
    "href": "posts/amazon-prime-titles-eda/index.html#setup",
    "title": "EDA on Amazon Prime Movies and TV Shows",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(naniar)\nlibrary(visdat)\nlibrary(DataExplorer)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(validate)\n\n\nAttaching package: 'validate'\n\nThe following object is masked from 'package:naniar':\n\n    all_complete\n\nThe following object is masked from 'package:dplyr':\n\n    expr\n\nThe following object is masked from 'package:ggplot2':\n\n    expr\n\n\n\n\n\n\ndf &lt;- read_csv('./amazon_prime_titles.csv')\n\nRows: 9668 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): show_id, type, title, director, cast, country, date_added, rating,...\ndbl  (1): release_year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/amazon-prime-titles-eda/index.html#inspect-dataset",
    "href": "posts/amazon-prime-titles-eda/index.html#inspect-dataset",
    "title": "EDA on Amazon Prime Movies and TV Shows",
    "section": "Inspect Dataset",
    "text": "Inspect Dataset\n\nUnderstand the Dataset\n\ndf |&gt; \n  head()\n\n# A tibble: 6 × 12\n  show_id type  title      director cast  country date_added release_year rating\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n1 s1      Movie The Grand… Don McK… Bren… Canada  March 30,…         2014 &lt;NA&gt;  \n2 s2      Movie Take Care… Girish … Mahe… India   March 30,…         2018 13+   \n3 s3      Movie Secrets o… Josh We… Tom … United… March 30,…         2017 &lt;NA&gt;  \n4 s4      Movie Pink: Sta… Sonia A… Inte… United… March 30,…         2014 &lt;NA&gt;  \n5 s5      Movie Monster M… Giles F… Harr… United… March 30,…         1989 &lt;NA&gt;  \n6 s6      Movie Living Wi… Paul We… Greg… United… March 30,…         1989 &lt;NA&gt;  \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\ndf |&gt; tail()\n\n# A tibble: 6 × 12\n  show_id type    title    director cast  country date_added release_year rating\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n1 s9663   Movie   River    Emily S… Mary… &lt;NA&gt;    &lt;NA&gt;               2021 16+   \n2 s9664   Movie   Pride O… Joseph … Leo … &lt;NA&gt;    &lt;NA&gt;               1940 7+    \n3 s9665   TV Show Planet … &lt;NA&gt;     DICK… &lt;NA&gt;    &lt;NA&gt;               2018 13+   \n4 s9666   Movie   Outpost  Steve B… Ray … &lt;NA&gt;    &lt;NA&gt;               2008 R     \n5 s9667   TV Show Maradon… &lt;NA&gt;     Este… &lt;NA&gt;    &lt;NA&gt;               2021 TV-MA \n6 s9668   Movie   Harry B… Daniel … Mich… &lt;NA&gt;    &lt;NA&gt;               2010 R     \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\ndf |&gt; \n  glimpse()\n\nRows: 9,668\nColumns: 12\n$ show_id      &lt;chr&gt; \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\", \"s9\", \"s1…\n$ type         &lt;chr&gt; \"Movie\", \"Movie\", \"Movie\", \"Movie\", \"Movie\", \"Movie\", \"Mo…\n$ title        &lt;chr&gt; \"The Grand Seduction\", \"Take Care Good Night\", \"Secrets o…\n$ director     &lt;chr&gt; \"Don McKellar\", \"Girish Joshi\", \"Josh Webber\", \"Sonia And…\n$ cast         &lt;chr&gt; \"Brendan Gleeson, Taylor Kitsch, Gordon Pinsent\", \"Mahesh…\n$ country      &lt;chr&gt; \"Canada\", \"India\", \"United States\", \"United States\", \"Uni…\n$ date_added   &lt;chr&gt; \"March 30, 2021\", \"March 30, 2021\", \"March 30, 2021\", \"Ma…\n$ release_year &lt;dbl&gt; 2014, 2018, 2017, 2014, 1989, 1989, 2017, 2016, 2017, 199…\n$ rating       &lt;chr&gt; NA, \"13+\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ duration     &lt;chr&gt; \"113 min\", \"110 min\", \"74 min\", \"69 min\", \"45 min\", \"52 m…\n$ listed_in    &lt;chr&gt; \"Comedy, Drama\", \"Drama, International\", \"Action, Drama, …\n$ description  &lt;chr&gt; \"A small fishing village must procure a local doctor to s…\n\n\n\nWhat’s a show_id?\n\ndf |&gt; \n  mutate(show_id = parse_number(show_id)) |&gt;\n  pull(show_id) |&gt; \n  is_linear_sequence()\n\n[1] TRUE\n\n\n\n\n\nCheck Data Quality\n\nMissing Values\n\ndf |&gt; \n  vis_miss()\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  gg_miss_upset()\n\n\n\n\n\n\n\n\n\n\nDuplicate Rows\n\ndf |&gt; \n  get_dupes()\n\nNo variable names specified - using all columns.\n\n\nNo duplicate combinations found of: show_id, type, title, director, cast, country, date_added, release_year, rating, ... and 3 other variables\n\n\n# A tibble: 0 × 13\n# ℹ 13 variables: show_id &lt;chr&gt;, type &lt;chr&gt;, title &lt;chr&gt;, director &lt;chr&gt;,\n#   cast &lt;chr&gt;, country &lt;chr&gt;, date_added &lt;chr&gt;, release_year &lt;dbl&gt;,\n#   rating &lt;chr&gt;, duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;,\n#   dupe_count &lt;int&gt;\n\n\n\ndf |&gt; \n  get_dupes(show_id)\n\nNo duplicate combinations found of: show_id\n\n\n# A tibble: 0 × 13\n# ℹ 13 variables: show_id &lt;chr&gt;, dupe_count &lt;int&gt;, type &lt;chr&gt;, title &lt;chr&gt;,\n#   director &lt;chr&gt;, cast &lt;chr&gt;, country &lt;chr&gt;, date_added &lt;chr&gt;,\n#   release_year &lt;dbl&gt;, rating &lt;chr&gt;, duration &lt;chr&gt;, listed_in &lt;chr&gt;,\n#   description &lt;chr&gt;\n\n\n\ndf |&gt; \n  get_dupes(description)\n\n# A tibble: 394 × 13\n   description dupe_count show_id type   title director cast  country date_added\n   &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     \n 1 1                   15 s7128   Movie  Act … 1        1     &lt;NA&gt;    &lt;NA&gt;      \n 2 1                   15 s7129   Movie  Act … 1        1     &lt;NA&gt;    &lt;NA&gt;      \n 3 1                   15 s7218   TV Sh… Seri… &lt;NA&gt;     1     &lt;NA&gt;    &lt;NA&gt;      \n 4 1                   15 s8709   TV Sh… Clip… &lt;NA&gt;     1     &lt;NA&gt;    &lt;NA&gt;      \n 5 1                   15 s8728   TV Sh… Act … &lt;NA&gt;     1     &lt;NA&gt;    September…\n 6 1                   15 s8729   TV Sh… ACT … &lt;NA&gt;     1     &lt;NA&gt;    &lt;NA&gt;      \n 7 1                   15 s8753   TV Sh… ACT … &lt;NA&gt;     1     &lt;NA&gt;    &lt;NA&gt;      \n 8 1                   15 s8754   TV Sh… ACT … &lt;NA&gt;     1     &lt;NA&gt;    &lt;NA&gt;      \n 9 1                   15 s8756   Movie  ACT … 1        1     &lt;NA&gt;    &lt;NA&gt;      \n10 1                   15 s8758   Movie  ACT … 1        1     &lt;NA&gt;    &lt;NA&gt;      \n# ℹ 384 more rows\n# ℹ 4 more variables: release_year &lt;dbl&gt;, rating &lt;chr&gt;, duration &lt;chr&gt;,\n#   listed_in &lt;chr&gt;\n\n\n\ndf |&gt; \n  get_dupes(cast) |&gt; \n  filter(!is.na(cast)) |&gt; \n  distinct(cast)\n\n# A tibble: 210 × 1\n   cast                            \n   &lt;chr&gt;                           \n 1 Maggie Binkley                  \n 2 1                               \n 3 Anne-Marie Newland              \n 4 Cassandra Peterson              \n 5 Grace Tamayo, Erin Webbs        \n 6 Gene Autry, Champion, Gail Davis\n 7 Stevin John                     \n 8 Eddie Izzard                    \n 9 Gallagher                       \n10 LB, Aaron Michael               \n# ℹ 200 more rows\n\n\n\n\nData Constraints\n\nCheck Numerical Data\n\ndf |&gt; \n  ggplot(aes(release_year)) +\n  geom_histogram() +\n  labs(title = 'Release Year')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  mutate(duration = parse_number(duration)) |&gt; \n  ggplot(aes(duration)) +\n  geom_histogram() +\n  labs(title = 'Duration (in Minute)')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  mutate(duration = parse_number(duration)) |&gt; \n  ggplot(aes(duration)) +\n  geom_histogram(binwidth = 1) +\n  coord_cartesian(xlim = c(200,700), ylim = c(0,9))\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  mutate(duration = parse_number(duration)) |&gt; \n  arrange(desc(duration)) |&gt; \n  relocate(title, duration, type, description)\n\n# A tibble: 9,668 × 12\n   title    duration type  description show_id director cast  country date_added\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     \n 1 Soothin…      601 Movie Black scre… s934    Mark Kn… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 2 Himalay…      550 Movie This ambie… s3851   Mark Kn… Niv … &lt;NA&gt;    &lt;NA&gt;      \n 3 Midnigh…      541 Movie Our most p… s1674   Mark Kn… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 4 Gentle …      541 Movie One of our… s2368   Mark Kn… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 5 Gentle …      541 Movie Beautiful … s2369   Mark Kn… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 6 9 Hour …      541 Movie Black scre… s3446   Mark Kn… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 7 Thunder…      541 Movie Black scre… s4088   Mark Kn… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 8 Pacific…      541 Movie For a full… s4129   Mark Kn… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 9 New Yor…      541 Movie Are you a … s4278   Mark Kn… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n10 Gentle …      541 Movie Drift off … s4584   Mark Kn… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n# ℹ 9,658 more rows\n# ℹ 3 more variables: release_year &lt;dbl&gt;, rating &lt;chr&gt;, listed_in &lt;chr&gt;\n\n\n\n\nCheck Categorical Data\n\ndf |&gt; \n  count(type)\n\n# A tibble: 2 × 2\n  type        n\n  &lt;chr&gt;   &lt;int&gt;\n1 Movie    7814\n2 TV Show  1854\n\n\n\ndf |&gt; \n  count(listed_in)\n\n# A tibble: 518 × 2\n   listed_in                                     n\n   &lt;chr&gt;                                     &lt;int&gt;\n 1 Action                                      238\n 2 Action, Adventure                            50\n 3 Action, Adventure, Animation                  2\n 4 Action, Adventure, Anime                      4\n 5 Action, Adventure, Comedy                    26\n 6 Action, Adventure, Documentary                3\n 7 Action, Adventure, Drama                     37\n 8 Action, Adventure, Faith and Spirituality     1\n 9 Action, Adventure, Fantasy                    7\n10 Action, Adventure, Horror                    10\n# ℹ 508 more rows\n\n\n\n\n\nPossible Null Value\n\ndf |&gt; \n  vis_expect(function(x) nchar(x) &lt; 10)\n\n\n\n\n\n\n\n\n\nnchar_limit &lt;- 10\ndf_possible_null &lt;- df |&gt;\n  select(title, cast, description, director)\n\nv_nchar &lt;- validator(\n title = nchar(title) &gt; nchar_limit,\n cast =  nchar(cast) &gt; nchar_limit,\n director = nchar(director) &gt; nchar_limit,\n description = nchar(description) &gt; nchar_limit\n)\n\ncf_nchar &lt;- confront(df_possible_null, v_nchar)\nsummary(cf_nchar)\n\n         name items passes fails  nNA error warning\n1       title  9668   7447  2221    0 FALSE   FALSE\n2        cast  9668   8206   229 1233 FALSE   FALSE\n3    director  9668   6445  1141 2082 FALSE   FALSE\n4 description  9668   9638    30    0 FALSE   FALSE\n                        expression\n1       nchar(title) &gt; nchar_limit\n2        nchar(cast) &gt; nchar_limit\n3    nchar(director) &gt; nchar_limit\n4 nchar(description) &gt; nchar_limit\n\n\n\nTitle\n\ndf |&gt; \n  select(release_year, type:director, description) |&gt; \n  filter(nchar(title) &lt; 3)\n\n# A tibble: 9 × 5\n  release_year type    title director          description                      \n         &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;                            \n1         2014 Movie   41    Glenn Triggs      \"A young man discovers a hole in…\n2         2021 Movie   X     Scott J. Ramsey   \"The chair of a mysterious found…\n3         2007 Movie   YO    Rafa Cortes       \"A handyman named Hans moves to …\n4         1994 Movie   IQ    Fred Schepisi     \"All the world knows about Einst…\n5         2019 Movie   M     Yolande Zauberman \"\\\"M\\\" as Menahem, child prodigy…\n6         2021 Movie   We    Mani Nasry        \"In the pursuit of love and happ…\n7         2008 Movie   21    Robert Luketic    \"Academy Award Winner Kevin Spac…\n8         2021 TV Show GD    &lt;NA&gt;              \"In the 1930s,America was just f…\n9         1991 Movie   Us    Michael Landon    \"In his final role, Michael Land…\n\n\n\n\nCast\n\ndf |&gt; \n names()\n\n [1] \"show_id\"      \"type\"         \"title\"        \"director\"     \"cast\"        \n [6] \"country\"      \"date_added\"   \"release_year\" \"rating\"       \"duration\"    \n[11] \"listed_in\"    \"description\" \n\n\n\ndf |&gt; \n  select(cast, description) |&gt; \n  filter(nchar(cast) &lt; 4)\n\n# A tibble: 38 × 2\n   cast  description                                                            \n   &lt;chr&gt; &lt;chr&gt;                                                                  \n 1 TJ    In his debut special taped in Brooklyn, Haitian born stand-up comedian…\n 2 Nas   When Nas was just 19 years old, he composed what would become one of h…\n 3 1     2                                                                      \n 4 1     Series before 1C onboarding                                            \n 5 1     Title Post onboarding 8                                                \n 6 1     1                                                                      \n 7 1     1                                                                      \n 8 1     clip1                                                                  \n 9 1     1                                                                      \n10 1     ACT 4 - Title after removing 1C - 5                                    \n# ℹ 28 more rows\n\n\n\n\nDirector\n\ndf |&gt; \n  relocate(director, title) |&gt; \n  filter(nchar(director) &lt; 3)\n\n# A tibble: 17 × 12\n   director title     show_id type  cast  country date_added release_year rating\n   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; \n 1 TC       The Miss… s6724   Movie TC, … &lt;NA&gt;    &lt;NA&gt;               2021 18+   \n 2 1        Title Po… s6931   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 18+   \n 3 1        Act 4 - … s7128   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n 4 1        Act 4 - … s7129   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n 5 1        Clip: 1   s7140   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 18+   \n 6 1        Clip: AC… s7239   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n 7 1        Clip: AC… s7240   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n 8 1        ACT 4 - … s7246   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n 9 1        Clip: AC… s7259   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 7+    \n10 1        Act 6 - … s8502   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n11 1        Act 5 - … s8503   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n12 1        ACT 2 - … s8756   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n13 1        ACT 2 - … s8757   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n14 1        ACT 2 - … s8758   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n15 1        ACT 2 - … s8759   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n16 1        Clip: Ac… s9463   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n17 1        Clip: Ac… s9464   Movie 1     &lt;NA&gt;    &lt;NA&gt;               2021 ALL   \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\n\nDescription\n\ndf |&gt; \n  relocate(description, title) |&gt; \n  filter(nchar(description) &lt; 15)\n\n# A tibble: 32 × 12\n   description title             show_id type  director cast  country date_added\n   &lt;chr&gt;       &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     \n 1 a           Simple Gifts: Th… s1001   Movie Habib A… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 2 Test title  Movie10y          s1589   Movie Test ti… Prod… &lt;NA&gt;    &lt;NA&gt;      \n 3 Test title  Movie10x          s1590   Movie Test ti… &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 4 test        IN and Classic -… s2123   Movie &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 5 Elfen Lied  Elfen Lied        s2564   TV S… &lt;NA&gt;     Kira… &lt;NA&gt;    &lt;NA&gt;      \n 6 test        Dmec and other    s5378   Movie &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;      \n 7 2           Series before 1C… s6419   TV S… &lt;NA&gt;     1     &lt;NA&gt;    July 12, …\n 8 1           Act 4 - Title be… s7128   Movie 1        1     &lt;NA&gt;    &lt;NA&gt;      \n 9 1           Act 4 - Title be… s7129   Movie 1        1     &lt;NA&gt;    &lt;NA&gt;      \n10 clip1       Clip: 1           s7140   Movie 1        1     &lt;NA&gt;    &lt;NA&gt;      \n# ℹ 22 more rows\n# ℹ 4 more variables: release_year &lt;dbl&gt;, rating &lt;chr&gt;, duration &lt;chr&gt;,\n#   listed_in &lt;chr&gt;\n\n\n\n\n\nCross Check\n\nCategorical Data\n\n\n\nSource: https://www.amazon.com/Series-before-1C-onboarding-2/dp/B09942SNJQ\n\n\nNumerical Data\n Source: https://www.amazon.com/norte-sleep-hours-black-screen/dp/B0799FS2RY"
  },
  {
    "objectID": "posts/amazon-prime-titles-eda/index.html#summary-of-preliminary-data-inspection",
    "href": "posts/amazon-prime-titles-eda/index.html#summary-of-preliminary-data-inspection",
    "title": "EDA on Amazon Prime Movies and TV Shows",
    "section": "Summary of Preliminary Data Inspection",
    "text": "Summary of Preliminary Data Inspection\n\nNo duplicate rows\n19% of missing values. 98% of them came from date_added. Which normally isn’t published anyway.\nFound nearly 100 suspicious values such as ‘1’, ‘2’, ‘test’, and ‘test title’ across cast, director, and description. They were cross-checked and were found on Amazon Prime’s website, which indicate a ‘fake’ title posted for the purpose of testing.\nOutliers such as unusually long movie length are confirmed to exist on Amazon Prime.\nduration data type depends on the title type. TV Show uses season and Movie uses minute"
  },
  {
    "objectID": "posts/amazon-prime-titles-eda/index.html#clean-data",
    "href": "posts/amazon-prime-titles-eda/index.html#clean-data",
    "title": "EDA on Amazon Prime Movies and TV Shows",
    "section": "Clean Data",
    "text": "Clean Data\n\nRemove columns with too many missing values\n\ndf |&gt; \n  names()\n\n [1] \"show_id\"      \"type\"         \"title\"        \"director\"     \"cast\"        \n [6] \"country\"      \"date_added\"   \"release_year\" \"rating\"       \"duration\"    \n[11] \"listed_in\"    \"description\" \n\n\n\ndf_removed_columns &lt;- df |&gt; \n  select(-country, -date_added)\n\ndf_removed_columns |&gt; \n  names()\n\n [1] \"show_id\"      \"type\"         \"title\"        \"director\"     \"cast\"        \n [6] \"release_year\" \"rating\"       \"duration\"     \"listed_in\"    \"description\" \n\n\n\n\nRename columns\n\ndf_renamed_column &lt;- df_removed_columns |&gt; \n  rename(genre = listed_in,\n         id = show_id)\n\n\n\nCorrect data types\n\ndf |&gt; \n  glimpse()\n\nRows: 9,668\nColumns: 12\n$ show_id      &lt;chr&gt; \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\", \"s9\", \"s1…\n$ type         &lt;chr&gt; \"Movie\", \"Movie\", \"Movie\", \"Movie\", \"Movie\", \"Movie\", \"Mo…\n$ title        &lt;chr&gt; \"The Grand Seduction\", \"Take Care Good Night\", \"Secrets o…\n$ director     &lt;chr&gt; \"Don McKellar\", \"Girish Joshi\", \"Josh Webber\", \"Sonia And…\n$ cast         &lt;chr&gt; \"Brendan Gleeson, Taylor Kitsch, Gordon Pinsent\", \"Mahesh…\n$ country      &lt;chr&gt; \"Canada\", \"India\", \"United States\", \"United States\", \"Uni…\n$ date_added   &lt;chr&gt; \"March 30, 2021\", \"March 30, 2021\", \"March 30, 2021\", \"Ma…\n$ release_year &lt;dbl&gt; 2014, 2018, 2017, 2014, 1989, 1989, 2017, 2016, 2017, 199…\n$ rating       &lt;chr&gt; NA, \"13+\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ duration     &lt;chr&gt; \"113 min\", \"110 min\", \"74 min\", \"69 min\", \"45 min\", \"52 m…\n$ listed_in    &lt;chr&gt; \"Comedy, Drama\", \"Drama, International\", \"Action, Drama, …\n$ description  &lt;chr&gt; \"A small fishing village must procure a local doctor to s…\n\n\n\ndf_correct_types &lt;- df_renamed_column |&gt; mutate(\n  id = as.integer(parse_number(id)),\n  type = as_factor(type),\n  release_year = as.integer(release_year),\n  duration = parse_number(duration)\n)"
  },
  {
    "objectID": "posts/amazon-prime-titles-eda/index.html#transform-data",
    "href": "posts/amazon-prime-titles-eda/index.html#transform-data",
    "title": "EDA on Amazon Prime Movies and TV Shows",
    "section": "Transform Data",
    "text": "Transform Data\n\nPivot dataset to long format by Genre\n\ndf_pivot_long_by_genre &lt;- df_correct_types |&gt; \n  mutate(genre = str_split(genre, ', ')) |&gt; \n  unnest_longer(genre) |&gt; \n  write_csv('amazon_prime_titles_pivot_long_by_genre.csv')\n\ndf_pivot_long_by_genre |&gt;\n  pull(genre) |&gt; \n  head(10)\n\n [1] \"Comedy\"        \"Drama\"         \"Drama\"         \"International\"\n [5] \"Action\"        \"Drama\"         \"Suspense\"      \"Documentary\"  \n [9] \"Drama\"         \"Fantasy\"      \n\n\n\ndf_pivot_long_by_genre |&gt; \n  write_csv('amazon_prime_titles_pivot_long_by_genre.csv')\n\n\n\nPivot dataset to long format by Cast\n\ndf_pivot_long_by_cast &lt;-  df_correct_types |&gt; \n  mutate(cast = str_split(cast, ', ')) |&gt; \n  unnest_longer(cast) |&gt; \n  write_csv('amazon_prime_titles_pivot_long_by_cast.csv')\n\ndf_pivot_long_by_cast |&gt; \n  pull(cast) |&gt; \n  head(10)\n\n [1] \"Brendan Gleeson\"  \"Taylor Kitsch\"    \"Gordon Pinsent\"   \"Mahesh Manjrekar\"\n [5] \"Abhay Mahajan\"    \"Sachin Khedekar\"  \"Tom Sizemore\"     \"Lorenzo Lamas\"   \n [9] \"Robert LaSardo\"   \"Richard Jones\"   \n\n\n\ndf_pivot_long_by_cast |&gt; \n  write_csv('amazon_prime_titles_pivot_long_by_cast.csv')\n\n\n\nPivot dataset to wide by Genre\n\ndf_pivot_wide_by_genre &lt;- df_correct_types |&gt; \n  select(id, title, director, release_year, genre) |&gt; \n  mutate(genre = strsplit(genre, ', ')) |&gt; \n  unnest_longer(genre) |&gt; \n  pivot_wider(names_from = genre,\n              values_from = genre,\n              values_fn = ~ !is.na(.),\n              values_fill = FALSE,\n              names_prefix = 'is_')\n\ndf_pivot_wide_by_genre |&gt; \n  names()\n\n [1] \"id\"                           \"title\"                       \n [3] \"director\"                     \"release_year\"                \n [5] \"is_Comedy\"                    \"is_Drama\"                    \n [7] \"is_International\"             \"is_Action\"                   \n [9] \"is_Suspense\"                  \"is_Documentary\"              \n[11] \"is_Fantasy\"                   \"is_Kids\"                     \n[13] \"is_Special Interest\"          \"is_Science Fiction\"          \n[15] \"is_Adventure\"                 \"is_Horror\"                   \n[17] \"is_Sports\"                    \"is_Talk Show and Variety\"    \n[19] \"is_Anime\"                     \"is_Arts\"                     \n[21] \"is_Entertainment\"             \"is_and Culture\"              \n[23] \"is_TV Shows\"                  \"is_Animation\"                \n[25] \"is_Music Videos and Concerts\" \"is_Fitness\"                  \n[27] \"is_Faith and Spirituality\"    \"is_Military and War\"         \n[29] \"is_Western\"                   \"is_LGBTQ\"                    \n[31] \"is_Romance\"                   \"is_Unscripted\"               \n[33] \"is_Young Adult Audience\"      \"is_Arthouse\"                 \n[35] \"is_Historical\"               \n\n\n\ndf_pivot_wide_by_genre |&gt; \n  write_csv('amazon_prime_titles_pivot_wide_by_genre.csv')"
  },
  {
    "objectID": "posts/amazon-prime-titles-eda/index.html#analyze-data",
    "href": "posts/amazon-prime-titles-eda/index.html#analyze-data",
    "title": "EDA on Amazon Prime Movies and TV Shows",
    "section": "Analyze Data",
    "text": "Analyze Data\n\nTableau Links\n\nGeneral Analysis (using cleaned csv):\n\nGenre Analysis (using pivot by genre csv):"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Hello, welcome to my blog. This blog will showcase my learning projects as well as my journey as a data analyst. Feel free to look around.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Indonesia’s Busiest Train Station: Manggarai\n\n\n\n\n\n\nProject\n\n\nPython\n\n\nR\n\n\n\nFinding the most optimal time for passengers to commute\n\n\n\n\n\nOct 1, 2024\n\n\ninvictus\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Top 10.000 Most Popular Movies on IMDb\n\n\n\n\n\n\nProject\n\n\nSQL\n\n\nR\n\n\n\nFinding the common characteristics among Top 10.000 movies of all time\n\n\n\n\n\nSep 15, 2024\n\n\ninvictus\n\n\n\n\n\n\n\n\n\n\n\n\nEDA on Amazon Prime Movies and TV Shows\n\n\n\n\n\n\nCase Study\n\n\nR\n\n\nTableau\n\n\n\nExploratory Data Analysis on Amazon Prime Titles\n\n\n\n\n\nAug 26, 2024\n\n\ninvictus\n\n\n\n\n\n\n\n\n\n\n\n\nEDA on islam religion growth worldwide\n\n\n\n\n\n\nEDA\n\n\nR\n\n\n\nExploring World Religion dataset to uncover the growth of Islam in various countries using R\n\n\n\n\n\nAug 14, 2024\n\n\ninvictus\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study: Deep Rock Galactic Missions Data\n\n\n\n\n\n\nCase Study\n\n\nR\n\n\n\nAnalyzing Deep Rock Galactic Daily Missions Data Over July 2024\n\n\n\n\n\nJul 31, 2024\n\n\ninvictus\n\n\n\n\n\n\n\n\n\n\n\n\nCase Study: RevoU DAMC Certification (July 22)\n\n\n\n\n\n\nFictional Project\n\n\nR\n\n\n\nSee how I tackle RevoU DAMC’s Certification Case Study using R\n\n\n\n\n\nJul 24, 2024\n\n\ninvictus\n\n\n\n\n\n\n\n\n\n\n\n\nWorking through G4DS: Getting Started - Part 2\n\n\n\n\n\n\nBook Walkthrough\n\n\nR\n\n\n\nFollow along while I go through this book for the first time\n\n\n\n\n\nJul 17, 2024\n\n\ninvictus\n\n\n\n\n\n\n\n\n\n\n\n\nSetting Up Web Analytics\n\n\n\n\n\nSetting up Google Analytics and Statscounter on this site\n\n\n\n\n\nJul 15, 2024\n\n\ninvictus\n\n\n\n\n\n\n\n\n\n\n\n\nWorking through G4DS: Getting Started - Part 1\n\n\n\n\n\n\nBook Walkthrough\n\n\nR\n\n\n\nThis is the first post on this series. I will go over the book and give genuine commentary as I follow the tutorial.\n\n\n\n\n\nJul 9, 2024\n\n\ninvictus\n\n\n\n\n\n\n\n\n\n\n\n\nSmartPath - Customer Data\n\n\n\n\n\n\nFictional Project\n\n\nR\n\n\n\nExploring the fictional dataset Customer Data, provided during SmartPath Data Analyst Bootcamp\n\n\n\n\n\nJul 4, 2024\n\n\ninvictus\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/smartpath-customer-data/index.html",
    "href": "posts/smartpath-customer-data/index.html",
    "title": "SmartPath - Customer Data",
    "section": "",
    "text": "We’ll be using the tidyverse library, as well as some useful packages for a quick data inspection such as summarytools and DataExplorer\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DataExplorer)\nlibrary(summarytools)\n\nWarning: no DISPLAY variable so Tk is not available\n\n\nsystem might not have X11 capabilities; in case of errors when using dfSummary(), set st_options(use.x11 = FALSE)\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\n\n\n\nI’m not good with scientific notation. It’s hard to read. So let’s turn it off by setting the threshold to a high value. While we’re at it, we can also set the amuont of significance digits.\n\noptions(scipen = 999, digits = 2)\n\n\n\n\n\ndf &lt;- read.csv('customer_data.csv') |&gt; \n  as_tibble()\n\n\n\n\nAll set, let’s check out the dataset (no pun intended).\n\ndf\n\n# A tibble: 2,240 × 9\n      ID Year_Birth Education  Income Dt_Customer Recency NumWebPurchases\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;         &lt;int&gt;           &lt;int&gt;\n 1  5524       1957 Graduation  58138 9/4/2012         58               8\n 2  2174       1954 Graduation  46344 3/8/2014         38               1\n 3  4141       1965 Graduation  71613 8/21/2013        26               8\n 4  6182       1984 Graduation  26646 2/10/2014        26               2\n 5  5324       1981 PhD         58293 1/19/2014        94               5\n 6  7446       1967 Master      62513 9/9/2013         16               6\n 7   965       1971 Graduation  55635 11/13/2012       34               7\n 8  6177       1985 PhD         33454 5/8/2013         32               4\n 9  4855       1974 PhD         30351 6/6/2013         19               3\n10  5899       1950 PhD          5648 3/13/2014        68               1\n# ℹ 2,230 more rows\n# ℹ 2 more variables: NumStorePurchases &lt;int&gt;, NumWebVisitsMonth &lt;int&gt;\n\n\n\nglimpse(df)\n\nRows: 2,240\nColumns: 9\n$ ID                &lt;int&gt; 5524, 2174, 4141, 6182, 5324, 7446, 965, 6177, 4855,…\n$ Year_Birth        &lt;int&gt; 1957, 1954, 1965, 1984, 1981, 1967, 1971, 1985, 1974…\n$ Education         &lt;chr&gt; \"Graduation\", \"Graduation\", \"Graduation\", \"Graduatio…\n$ Income            &lt;int&gt; 58138, 46344, 71613, 26646, 58293, 62513, 55635, 334…\n$ Dt_Customer       &lt;chr&gt; \"9/4/2012\", \"3/8/2014\", \"8/21/2013\", \"2/10/2014\", \"1…\n$ Recency           &lt;int&gt; 58, 38, 26, 26, 94, 16, 34, 32, 19, 68, 11, 59, 82, …\n$ NumWebPurchases   &lt;int&gt; 8, 1, 8, 2, 5, 6, 7, 4, 3, 1, 1, 2, 3, 6, 1, 7, 3, 4…\n$ NumStorePurchases &lt;int&gt; 4, 2, 10, 4, 6, 10, 7, 4, 2, 0, 2, 3, 8, 5, 3, 12, 3…\n$ NumWebVisitsMonth &lt;int&gt; 7, 5, 4, 6, 5, 6, 6, 8, 9, 20, 7, 8, 2, 6, 8, 3, 8, …\n\n\nAt a glance, we can spot several problems with this dataset:\n\nDt_Customer should’ve been a date format. from the first few rows, the date seems to be complete, so let’s try parsing it.\n\n\ndf$Dt_Customer |&gt; \n  mdy() |&gt; \n  head()\n\n[1] \"2012-09-04\" \"2014-03-08\" \"2013-08-21\" \"2014-02-10\" \"2014-01-19\"\n[6] \"2013-09-09\"\n\n\nThere is no error, fortunately, so we can apply it right away.\n\ndf$Dt_Customer &lt;- df$Dt_Customer |&gt; \n  mdy()\n\n\nYear_Birth is a bit tricky. It’s actually an incomplete date, but we can’t convert it to date format either because we need the month and days too. We can set everything as January the 1st, but that would be misleading. Integers would also be inaccurate because it’s not really a number, we aren’t supposed to perform arithmetic operations on it. However, I believe it’s the most ideal type. So we can leave it as it is.\n\nNow let’s further explore the variables. We’ll start with the categorical since there is only one.\n\n\n\n\nfreq(df)\n\nVariable(s) ignored: ID, Year_Birth, Income, Dt_Customer, Recency\n\n\nFrequencies  \ndf$Education  \nType: Character  \n\n                   Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n---------------- ------ --------- -------------- --------- --------------\n        2n Cycle    203      9.06           9.06      9.06           9.06\n           Basic     54      2.41          11.47      2.41          11.47\n      Graduation   1127     50.31          61.79     50.31          61.79\n          Master    370     16.52          78.30     16.52          78.30\n             PhD    486     21.70         100.00     21.70         100.00\n            &lt;NA&gt;      0                               0.00         100.00\n           Total   2240    100.00         100.00    100.00         100.00\n\ndf$NumWebPurchases  \nType: Integer  \n\n              Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n----------- ------ --------- -------------- --------- --------------\n          0     49     2.188          2.188     2.188          2.188\n          1    354    15.804         17.991    15.804         17.991\n          2    373    16.652         34.643    16.652         34.643\n          3    336    15.000         49.643    15.000         49.643\n          4    280    12.500         62.143    12.500         62.143\n          5    220     9.821         71.964     9.821         71.964\n          6    205     9.152         81.116     9.152         81.116\n          7    155     6.920         88.036     6.920         88.036\n          8    102     4.554         92.589     4.554         92.589\n          9     75     3.348         95.938     3.348         95.938\n         10     43     1.920         97.857     1.920         97.857\n         11     44     1.964         99.821     1.964         99.821\n         23      1     0.045         99.866     0.045         99.866\n         25      1     0.045         99.911     0.045         99.911\n         27      2     0.089        100.000     0.089        100.000\n       &lt;NA&gt;      0                              0.000        100.000\n      Total   2240   100.000        100.000   100.000        100.000\n\ndf$NumStorePurchases  \nType: Integer  \n\n              Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n----------- ------ --------- -------------- --------- --------------\n          0     15      0.67           0.67      0.67           0.67\n          1      7      0.31           0.98      0.31           0.98\n          2    223      9.96          10.94      9.96          10.94\n          3    490     21.88          32.81     21.88          32.81\n          4    323     14.42          47.23     14.42          47.23\n          5    212      9.46          56.70      9.46          56.70\n          6    178      7.95          64.64      7.95          64.64\n          7    143      6.38          71.03      6.38          71.03\n          8    149      6.65          77.68      6.65          77.68\n          9    106      4.73          82.41      4.73          82.41\n         10    125      5.58          87.99      5.58          87.99\n         11     81      3.62          91.61      3.62          91.61\n         12    105      4.69          96.29      4.69          96.29\n         13     83      3.71         100.00      3.71         100.00\n       &lt;NA&gt;      0                               0.00         100.00\n      Total   2240    100.00         100.00    100.00         100.00\n\ndf$NumWebVisitsMonth  \nType: Integer  \n\n              Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n----------- ------ --------- -------------- --------- --------------\n          0     11     0.491          0.491     0.491          0.491\n          1    153     6.830          7.321     6.830          7.321\n          2    202     9.018         16.339     9.018         16.339\n          3    205     9.152         25.491     9.152         25.491\n          4    218     9.732         35.223     9.732         35.223\n          5    281    12.545         47.768    12.545         47.768\n          6    340    15.179         62.946    15.179         62.946\n          7    393    17.545         80.491    17.545         80.491\n          8    342    15.268         95.759    15.268         95.759\n          9     83     3.705         99.464     3.705         99.464\n         10      3     0.134         99.598     0.134         99.598\n         13      1     0.045         99.643     0.045         99.643\n         14      2     0.089         99.732     0.089         99.732\n         17      1     0.045         99.777     0.045         99.777\n         19      2     0.089         99.866     0.089         99.866\n         20      3     0.134        100.000     0.134        100.000\n       &lt;NA&gt;      0                              0.000        100.000\n      Total   2240   100.000        100.000   100.000        100.000\n\n\nI honestly didn’t expect the 3 numerical variables to show up. After I think about it, it kinda makes sense, because they’re integers, so they aren’t continuous, they discrete. It may be useful to inspect their frequency to spot outliers, but for now we’ll focus on Education.\n\ndf$Education |&gt; freq()\n\nFrequencies  \ndf$Education  \nType: Character  \n\n                   Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n---------------- ------ --------- -------------- --------- --------------\n        2n Cycle    203      9.06           9.06      9.06           9.06\n           Basic     54      2.41          11.47      2.41          11.47\n      Graduation   1127     50.31          61.79     50.31          61.79\n          Master    370     16.52          78.30     16.52          78.30\n             PhD    486     21.70         100.00     21.70         100.00\n            &lt;NA&gt;      0                               0.00         100.00\n           Total   2240    100.00         100.00    100.00         100.00\n\n\n\ndf$Education |&gt; plot_bar()\n\n\n\n\n\n\n\n\nNo serious problems spotted. No missing values, no weird values. The distribution also seems ‘normal’. The frequency increases as the education goes up, except for the PhD. We could guess that the customers are mostly from well-educated backgrounds.\nNow let’s explore the numerical variables.\n\n\n\n\ndescr(df, stats=c('n.valid', 'pct.valid', 'mean', 'min', 'q1', 'q3', 'max'))\n\nNon-numerical variable(s) ignored: Education, Dt_Customer\n\n\nDescriptive Statistics  \ndf  \nN: 2240  \n\n                        ID      Income   NumStorePurchases   NumWebPurchases   NumWebVisitsMonth\n--------------- ---------- ----------- ------------------- ----------------- -------------------\n        N.Valid    2240.00     2216.00             2240.00           2240.00             2240.00\n      Pct.Valid     100.00       98.93              100.00            100.00              100.00\n           Mean    5592.16    52247.25                5.79              4.08                5.32\n            Min       0.00     1730.00                0.00              0.00                0.00\n             Q1    2827.50    35284.00                3.00              2.00                3.00\n             Q3    8428.50    68557.00                8.00              6.00                7.00\n            Max   11191.00   666666.00               13.00             27.00               20.00\n\nTable: Table continues below\n\n \n\n                  Recency   Year_Birth\n--------------- --------- ------------\n        N.Valid   2240.00      2240.00\n      Pct.Valid    100.00       100.00\n           Mean     49.11      1968.81\n            Min      0.00      1893.00\n             Q1     24.00      1959.00\n             Q3     74.00      1977.00\n            Max     99.00      1996.00\n\n\n\ndf |&gt; plot_histogram()\n\n\n\n\n\n\n\n\nNearly no missing values, perfect. The only missing values are within Income, which are only about 2%. We could drop the rows, but we don’t need to for now. The valid data in the other columns can still provide aid to other analysis.\nI’m interested to why they are missing, though. We have 2 data that we could analyze it with: Year_Birth and Education. Maybe older people tend to be sensitive about their income, so they leave it empty. Or maybe it’s the younger people that do! (who knows?).\nUnfortunately, I think 2% is too small of a sample to get reliable result. So let’s just leave it as it is.\n\n\nWith all these numeric variables, there’s various relationships we could investigate. As starters, let’s draw up a correlation matrix to see a broad overview. We’ll remove ID since it’s irrelevant.\n\ndf_numeric &lt;- df |&gt; \n  select(!ID)\nplot_correlation(df_numeric, type = 'continuous', maxcat = 0, cor_args =  list(\"use\" = \"pairwise.complete.obs\"))"
  },
  {
    "objectID": "posts/smartpath-customer-data/index.html#setup",
    "href": "posts/smartpath-customer-data/index.html#setup",
    "title": "SmartPath - Customer Data",
    "section": "",
    "text": "We’ll be using the tidyverse library, as well as some useful packages for a quick data inspection such as summarytools and DataExplorer\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DataExplorer)\nlibrary(summarytools)\n\nWarning: no DISPLAY variable so Tk is not available\n\n\nsystem might not have X11 capabilities; in case of errors when using dfSummary(), set st_options(use.x11 = FALSE)\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\n\n\n\n\nI’m not good with scientific notation. It’s hard to read. So let’s turn it off by setting the threshold to a high value. While we’re at it, we can also set the amuont of significance digits.\n\noptions(scipen = 999, digits = 2)\n\n\n\n\n\ndf &lt;- read.csv('customer_data.csv') |&gt; \n  as_tibble()\n\n\n\n\nAll set, let’s check out the dataset (no pun intended).\n\ndf\n\n# A tibble: 2,240 × 9\n      ID Year_Birth Education  Income Dt_Customer Recency NumWebPurchases\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;         &lt;int&gt;           &lt;int&gt;\n 1  5524       1957 Graduation  58138 9/4/2012         58               8\n 2  2174       1954 Graduation  46344 3/8/2014         38               1\n 3  4141       1965 Graduation  71613 8/21/2013        26               8\n 4  6182       1984 Graduation  26646 2/10/2014        26               2\n 5  5324       1981 PhD         58293 1/19/2014        94               5\n 6  7446       1967 Master      62513 9/9/2013         16               6\n 7   965       1971 Graduation  55635 11/13/2012       34               7\n 8  6177       1985 PhD         33454 5/8/2013         32               4\n 9  4855       1974 PhD         30351 6/6/2013         19               3\n10  5899       1950 PhD          5648 3/13/2014        68               1\n# ℹ 2,230 more rows\n# ℹ 2 more variables: NumStorePurchases &lt;int&gt;, NumWebVisitsMonth &lt;int&gt;\n\n\n\nglimpse(df)\n\nRows: 2,240\nColumns: 9\n$ ID                &lt;int&gt; 5524, 2174, 4141, 6182, 5324, 7446, 965, 6177, 4855,…\n$ Year_Birth        &lt;int&gt; 1957, 1954, 1965, 1984, 1981, 1967, 1971, 1985, 1974…\n$ Education         &lt;chr&gt; \"Graduation\", \"Graduation\", \"Graduation\", \"Graduatio…\n$ Income            &lt;int&gt; 58138, 46344, 71613, 26646, 58293, 62513, 55635, 334…\n$ Dt_Customer       &lt;chr&gt; \"9/4/2012\", \"3/8/2014\", \"8/21/2013\", \"2/10/2014\", \"1…\n$ Recency           &lt;int&gt; 58, 38, 26, 26, 94, 16, 34, 32, 19, 68, 11, 59, 82, …\n$ NumWebPurchases   &lt;int&gt; 8, 1, 8, 2, 5, 6, 7, 4, 3, 1, 1, 2, 3, 6, 1, 7, 3, 4…\n$ NumStorePurchases &lt;int&gt; 4, 2, 10, 4, 6, 10, 7, 4, 2, 0, 2, 3, 8, 5, 3, 12, 3…\n$ NumWebVisitsMonth &lt;int&gt; 7, 5, 4, 6, 5, 6, 6, 8, 9, 20, 7, 8, 2, 6, 8, 3, 8, …\n\n\nAt a glance, we can spot several problems with this dataset:\n\nDt_Customer should’ve been a date format. from the first few rows, the date seems to be complete, so let’s try parsing it.\n\n\ndf$Dt_Customer |&gt; \n  mdy() |&gt; \n  head()\n\n[1] \"2012-09-04\" \"2014-03-08\" \"2013-08-21\" \"2014-02-10\" \"2014-01-19\"\n[6] \"2013-09-09\"\n\n\nThere is no error, fortunately, so we can apply it right away.\n\ndf$Dt_Customer &lt;- df$Dt_Customer |&gt; \n  mdy()\n\n\nYear_Birth is a bit tricky. It’s actually an incomplete date, but we can’t convert it to date format either because we need the month and days too. We can set everything as January the 1st, but that would be misleading. Integers would also be inaccurate because it’s not really a number, we aren’t supposed to perform arithmetic operations on it. However, I believe it’s the most ideal type. So we can leave it as it is.\n\nNow let’s further explore the variables. We’ll start with the categorical since there is only one.\n\n\n\n\nfreq(df)\n\nVariable(s) ignored: ID, Year_Birth, Income, Dt_Customer, Recency\n\n\nFrequencies  \ndf$Education  \nType: Character  \n\n                   Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n---------------- ------ --------- -------------- --------- --------------\n        2n Cycle    203      9.06           9.06      9.06           9.06\n           Basic     54      2.41          11.47      2.41          11.47\n      Graduation   1127     50.31          61.79     50.31          61.79\n          Master    370     16.52          78.30     16.52          78.30\n             PhD    486     21.70         100.00     21.70         100.00\n            &lt;NA&gt;      0                               0.00         100.00\n           Total   2240    100.00         100.00    100.00         100.00\n\ndf$NumWebPurchases  \nType: Integer  \n\n              Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n----------- ------ --------- -------------- --------- --------------\n          0     49     2.188          2.188     2.188          2.188\n          1    354    15.804         17.991    15.804         17.991\n          2    373    16.652         34.643    16.652         34.643\n          3    336    15.000         49.643    15.000         49.643\n          4    280    12.500         62.143    12.500         62.143\n          5    220     9.821         71.964     9.821         71.964\n          6    205     9.152         81.116     9.152         81.116\n          7    155     6.920         88.036     6.920         88.036\n          8    102     4.554         92.589     4.554         92.589\n          9     75     3.348         95.938     3.348         95.938\n         10     43     1.920         97.857     1.920         97.857\n         11     44     1.964         99.821     1.964         99.821\n         23      1     0.045         99.866     0.045         99.866\n         25      1     0.045         99.911     0.045         99.911\n         27      2     0.089        100.000     0.089        100.000\n       &lt;NA&gt;      0                              0.000        100.000\n      Total   2240   100.000        100.000   100.000        100.000\n\ndf$NumStorePurchases  \nType: Integer  \n\n              Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n----------- ------ --------- -------------- --------- --------------\n          0     15      0.67           0.67      0.67           0.67\n          1      7      0.31           0.98      0.31           0.98\n          2    223      9.96          10.94      9.96          10.94\n          3    490     21.88          32.81     21.88          32.81\n          4    323     14.42          47.23     14.42          47.23\n          5    212      9.46          56.70      9.46          56.70\n          6    178      7.95          64.64      7.95          64.64\n          7    143      6.38          71.03      6.38          71.03\n          8    149      6.65          77.68      6.65          77.68\n          9    106      4.73          82.41      4.73          82.41\n         10    125      5.58          87.99      5.58          87.99\n         11     81      3.62          91.61      3.62          91.61\n         12    105      4.69          96.29      4.69          96.29\n         13     83      3.71         100.00      3.71         100.00\n       &lt;NA&gt;      0                               0.00         100.00\n      Total   2240    100.00         100.00    100.00         100.00\n\ndf$NumWebVisitsMonth  \nType: Integer  \n\n              Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n----------- ------ --------- -------------- --------- --------------\n          0     11     0.491          0.491     0.491          0.491\n          1    153     6.830          7.321     6.830          7.321\n          2    202     9.018         16.339     9.018         16.339\n          3    205     9.152         25.491     9.152         25.491\n          4    218     9.732         35.223     9.732         35.223\n          5    281    12.545         47.768    12.545         47.768\n          6    340    15.179         62.946    15.179         62.946\n          7    393    17.545         80.491    17.545         80.491\n          8    342    15.268         95.759    15.268         95.759\n          9     83     3.705         99.464     3.705         99.464\n         10      3     0.134         99.598     0.134         99.598\n         13      1     0.045         99.643     0.045         99.643\n         14      2     0.089         99.732     0.089         99.732\n         17      1     0.045         99.777     0.045         99.777\n         19      2     0.089         99.866     0.089         99.866\n         20      3     0.134        100.000     0.134        100.000\n       &lt;NA&gt;      0                              0.000        100.000\n      Total   2240   100.000        100.000   100.000        100.000\n\n\nI honestly didn’t expect the 3 numerical variables to show up. After I think about it, it kinda makes sense, because they’re integers, so they aren’t continuous, they discrete. It may be useful to inspect their frequency to spot outliers, but for now we’ll focus on Education.\n\ndf$Education |&gt; freq()\n\nFrequencies  \ndf$Education  \nType: Character  \n\n                   Freq   % Valid   % Valid Cum.   % Total   % Total Cum.\n---------------- ------ --------- -------------- --------- --------------\n        2n Cycle    203      9.06           9.06      9.06           9.06\n           Basic     54      2.41          11.47      2.41          11.47\n      Graduation   1127     50.31          61.79     50.31          61.79\n          Master    370     16.52          78.30     16.52          78.30\n             PhD    486     21.70         100.00     21.70         100.00\n            &lt;NA&gt;      0                               0.00         100.00\n           Total   2240    100.00         100.00    100.00         100.00\n\n\n\ndf$Education |&gt; plot_bar()\n\n\n\n\n\n\n\n\nNo serious problems spotted. No missing values, no weird values. The distribution also seems ‘normal’. The frequency increases as the education goes up, except for the PhD. We could guess that the customers are mostly from well-educated backgrounds.\nNow let’s explore the numerical variables.\n\n\n\n\ndescr(df, stats=c('n.valid', 'pct.valid', 'mean', 'min', 'q1', 'q3', 'max'))\n\nNon-numerical variable(s) ignored: Education, Dt_Customer\n\n\nDescriptive Statistics  \ndf  \nN: 2240  \n\n                        ID      Income   NumStorePurchases   NumWebPurchases   NumWebVisitsMonth\n--------------- ---------- ----------- ------------------- ----------------- -------------------\n        N.Valid    2240.00     2216.00             2240.00           2240.00             2240.00\n      Pct.Valid     100.00       98.93              100.00            100.00              100.00\n           Mean    5592.16    52247.25                5.79              4.08                5.32\n            Min       0.00     1730.00                0.00              0.00                0.00\n             Q1    2827.50    35284.00                3.00              2.00                3.00\n             Q3    8428.50    68557.00                8.00              6.00                7.00\n            Max   11191.00   666666.00               13.00             27.00               20.00\n\nTable: Table continues below\n\n \n\n                  Recency   Year_Birth\n--------------- --------- ------------\n        N.Valid   2240.00      2240.00\n      Pct.Valid    100.00       100.00\n           Mean     49.11      1968.81\n            Min      0.00      1893.00\n             Q1     24.00      1959.00\n             Q3     74.00      1977.00\n            Max     99.00      1996.00\n\n\n\ndf |&gt; plot_histogram()\n\n\n\n\n\n\n\n\nNearly no missing values, perfect. The only missing values are within Income, which are only about 2%. We could drop the rows, but we don’t need to for now. The valid data in the other columns can still provide aid to other analysis.\nI’m interested to why they are missing, though. We have 2 data that we could analyze it with: Year_Birth and Education. Maybe older people tend to be sensitive about their income, so they leave it empty. Or maybe it’s the younger people that do! (who knows?).\nUnfortunately, I think 2% is too small of a sample to get reliable result. So let’s just leave it as it is.\n\n\nWith all these numeric variables, there’s various relationships we could investigate. As starters, let’s draw up a correlation matrix to see a broad overview. We’ll remove ID since it’s irrelevant.\n\ndf_numeric &lt;- df |&gt; \n  select(!ID)\nplot_correlation(df_numeric, type = 'continuous', maxcat = 0, cor_args =  list(\"use\" = \"pairwise.complete.obs\"))"
  },
  {
    "objectID": "posts/islam-growth-worldwide-eda/index.html",
    "href": "posts/islam-growth-worldwide-eda/index.html",
    "title": "EDA on islam religion growth worldwide",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(visdat)\nlibrary(DataExplorer)\nlibrary(naniar)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(validate)\n\n\nAttaching package: 'validate'\n\nThe following object is masked from 'package:naniar':\n\n    all_complete\n\nThe following object is masked from 'package:dplyr':\n\n    expr\n\nThe following object is masked from 'package:ggplot2':\n\n    expr\n\nlibrary(countrycode)\n\n\ndf &lt;- read_csv('./ThrowbackDataThursday 201912 - Religion.csv')\n\nRows: 2308 Columns: 76\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): StateNme\ndbl (75): index, Year, Christianity - Protestants (Population), Christianity...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/islam-growth-worldwide-eda/index.html#setup",
    "href": "posts/islam-growth-worldwide-eda/index.html#setup",
    "title": "EDA on islam religion growth worldwide",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(visdat)\nlibrary(DataExplorer)\nlibrary(naniar)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(validate)\n\n\nAttaching package: 'validate'\n\nThe following object is masked from 'package:naniar':\n\n    all_complete\n\nThe following object is masked from 'package:dplyr':\n\n    expr\n\nThe following object is masked from 'package:ggplot2':\n\n    expr\n\nlibrary(countrycode)\n\n\ndf &lt;- read_csv('./ThrowbackDataThursday 201912 - Religion.csv')\n\nRows: 2308 Columns: 76\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): StateNme\ndbl (75): index, Year, Christianity - Protestants (Population), Christianity...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/islam-growth-worldwide-eda/index.html#inspect-dataset",
    "href": "posts/islam-growth-worldwide-eda/index.html#inspect-dataset",
    "title": "EDA on islam religion growth worldwide",
    "section": "Inspect Dataset",
    "text": "Inspect Dataset\n\nUnderstand the data\n\ndf\n\n# A tibble: 2,308 × 76\n   index  Year `Christianity - Protestants (Population)` Christianity - Roman …¹\n   &lt;dbl&gt; &lt;dbl&gt;                                     &lt;dbl&gt;                   &lt;dbl&gt;\n 1     0  1945                                  66069671                38716742\n 2     1  1950                                  73090083                42635882\n 3     2  1955                                  79294628                46402368\n 4     3  1960                                  90692928                50587880\n 5     4  1965                                  94165803                64761783\n 6     5  1970                                  93918347                69119143\n 7     6  1975                                  93578909                77181399\n 8     7  1980                                 101216516                75149580\n 9     8  1985                                 107309700                82463100\n10     9  1990                                 132040181                65530789\n# ℹ 2,298 more rows\n# ℹ abbreviated name: ¹​`Christianity - Roman Catholics (Population)`\n# ℹ 72 more variables: `Christianity - Eastern Orthodox (Population)` &lt;dbl&gt;,\n#   `Christianity - Anglican (Population)` &lt;dbl&gt;,\n#   `Christianity - Others (Population)` &lt;dbl&gt;,\n#   `Christianity - Total (Population)` &lt;dbl&gt;,\n#   `Judaism - Orthodox (Population)` &lt;dbl&gt;, …\n\n\n\ndf |&gt; names()\n\n [1] \"index\"                                       \n [2] \"Year\"                                        \n [3] \"Christianity - Protestants (Population)\"     \n [4] \"Christianity - Roman Catholics (Population)\" \n [5] \"Christianity - Eastern Orthodox (Population)\"\n [6] \"Christianity - Anglican (Population)\"        \n [7] \"Christianity - Others (Population)\"          \n [8] \"Christianity - Total (Population)\"           \n [9] \"Judaism - Orthodox (Population)\"             \n[10] \"Judaism - Conservatives (Population)\"        \n[11] \"Judaism - Reform (Population)\"               \n[12] \"Judaism - Others (Population)\"               \n[13] \"Judaism - Total (Population)\"                \n[14] \"Islam - Sunni (Population)\"                  \n[15] \"Islam - Shi'a (Population)\"                  \n[16] \"Islam - Ibadhi (Population)\"                 \n[17] \"Islam - Nation of Islam (Population)\"        \n[18] \"Islam - Alawite (Population)\"                \n[19] \"Islam - Ahmadiyya (Population)\"              \n[20] \"Islam - Others (Population)\"                 \n[21] \"Islam - Total (Population)\"                  \n[22] \"Buddhism - Mahayana (Population)\"            \n[23] \"Buddhism - Theravada (Population)\"           \n[24] \"Buddhism - Others (Population)\"              \n[25] \"Buddhism - Total (Population)\"               \n[26] \"Zoroastrian - Total (Population)\"            \n[27] \"Hindu - Total (Population)\"                  \n[28] \"Sike - Total (Population)\"                   \n[29] \"Shinto - Total (Population)\"                 \n[30] \"Baha'i - Total (Population)\"                 \n[31] \"Taoism - Total (Population)\"                 \n[32] \"Confucianism - Total (Population)\"           \n[33] \"Jain - Total (Population)\"                   \n[34] \"Syncretic Religions - Total (Population)\"    \n[35] \"Animist - Total (Population)\"                \n[36] \"Non Religious - Total (Population)\"          \n[37] \"Other Religions - Total (Population)\"        \n[38] \"Total Religious - Total (Population)\"        \n[39] \"Population - Total (Population)\"             \n[40] \"Christianity - Protestants (Percent)\"        \n[41] \"Christianity - Roman Catholics (Percent)\"    \n[42] \"Christianity - Eastern Orthodox (Percent)\"   \n[43] \"Christianity - Anglican (Percent)\"           \n[44] \"Christianity - Others (Percent)\"             \n[45] \"Christianity - Total (Percent)\"              \n[46] \"Judaism - Orthodox (Percent)\"                \n[47] \"Judaism - Conservatives (Percent)\"           \n[48] \"Judaism - Reform (Percent)\"                  \n[49] \"Judaism - Others (Percent)\"                  \n[50] \"Judaism - Total (Percent)\"                   \n[51] \"Islam - Sunni (Percent)\"                     \n[52] \"Islam - Shi'a (Percent)\"                     \n[53] \"Islam - Ibadhi (Percent)\"                    \n[54] \"Islam - Nation of Islam (Percent)\"           \n[55] \"Islam - Alawite (Percent)\"                   \n[56] \"Islam - Ahmadiyya (Percent)\"                 \n[57] \"Islam - Others (Percent)\"                    \n[58] \"Islam - Total (Percent)\"                     \n[59] \"Buddhism - Mahayana (Percent)\"               \n[60] \"Buddhism - Theravada (Percent)\"              \n[61] \"Buddhism - Others (Percent)\"                 \n[62] \"Buddhism - Total (Percent)\"                  \n[63] \"Zoroastrian - Total (Percent)\"               \n[64] \"Hindu - Total (Percent)\"                     \n[65] \"Sike - Total (Percent)\"                      \n[66] \"Shinto - Total (Percent)\"                    \n[67] \"Baha'i - Total (Percent)\"                    \n[68] \"Taoism - Total (Percent)\"                    \n[69] \"Confucianism - Total (Percent)\"              \n[70] \"Jain - Total (Percent)\"                      \n[71] \"Syncretic Religions - Total (Percent)\"       \n[72] \"Animist - Total (Percent)\"                   \n[73] \"Non Religious - Total (Percent)\"             \n[74] \"Other Religions - Total (Percent)\"           \n[75] \"Total Religious - Total (Percent)\"           \n[76] \"StateNme\"                                    \n\n\n\n\nCheck Data Quality\n\nMissing Values\n\ndf |&gt; \n  vis_miss()\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  vis_expect(function(x) x != 0)\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  replace_with_na_all(~ . == 0) |&gt; \n  vis_miss()\n\n\n\n\n\n\n\n\n\nTotal Population Columns\n\ndf |&gt; \n  select(contains('Total') & contains('Population')) |&gt; \n  rename_with(function(x) str_remove(x, ' - Total \\\\(Population\\\\)')) |&gt; \n  replace_with_na_all(~ . == 0) |&gt; \n  vis_miss(sort_miss = TRUE)\n\n\n\n\n\n\n\n\n\n\nTotal Percent Columns\n\ndf |&gt; \n  select(contains('Total') & contains('Percent')) |&gt; \n  rename_with(function(x) str_remove(x, ' - Total \\\\(Percent\\\\)')) |&gt; \n  replace_with_na_all(~ . == 0) |&gt; \n  vis_miss(sort_miss = TRUE)\n\n\n\n\n\n\n\n\n\n\nPopulation but Non Total columns\n\ndf |&gt; \n  select(-contains('Total') & contains('Population')) |&gt;\n  rename_with(function(x) str_remove(x, ' \\\\(Population\\\\)')) |&gt; \n  replace_with_na_all(~ . == 0) |&gt; \n  vis_miss(sort_miss = TRUE)"
  },
  {
    "objectID": "posts/islam-growth-worldwide-eda/index.html#clean-data",
    "href": "posts/islam-growth-worldwide-eda/index.html#clean-data",
    "title": "EDA on islam religion growth worldwide",
    "section": "Clean Data",
    "text": "Clean Data\n\nPick relevant columns\n\ndf_islam &lt;- df |&gt; \n  select(Country = StateNme, Year, starts_with('Islam'))\n\ndf_islam\n\n# A tibble: 2,308 × 18\n   Country                   Year Islam - Sunni (Popula…¹ Islam - Shi'a (Popul…²\n   &lt;chr&gt;                    &lt;dbl&gt;                   &lt;dbl&gt;                  &lt;dbl&gt;\n 1 United States of America  1945                       0                      0\n 2 United States of America  1950                       0                      0\n 3 United States of America  1955                       0                      0\n 4 United States of America  1960                       0                      0\n 5 United States of America  1965                       0                      0\n 6 United States of America  1970                       0                      0\n 7 United States of America  1975                       0                      0\n 8 United States of America  1980                 2407464                 254121\n 9 United States of America  1985                 2186675                 298183\n10 United States of America  1990                 2019386                 275371\n# ℹ 2,298 more rows\n# ℹ abbreviated names: ¹​`Islam - Sunni (Population)`,\n#   ²​`Islam - Shi'a (Population)`\n# ℹ 14 more variables: `Islam - Ibadhi (Population)` &lt;dbl&gt;,\n#   `Islam - Nation of Islam (Population)` &lt;dbl&gt;,\n#   `Islam - Alawite (Population)` &lt;dbl&gt;,\n#   `Islam - Ahmadiyya (Population)` &lt;dbl&gt;, …\n\n\n\n\nCheck Data type\n\ndf_islam |&gt; \n  str()\n\ntibble [2,308 × 18] (S3: tbl_df/tbl/data.frame)\n $ Country                             : chr [1:2308] \"United States of America\" \"United States of America\" \"United States of America\" \"United States of America\" ...\n $ Year                                : num [1:2308] 1945 1950 1955 1960 1965 ...\n $ Islam - Sunni (Population)          : num [1:2308] 0 0 0 0 0 ...\n $ Islam - Shi'a (Population)          : num [1:2308] 0 0 0 0 0 ...\n $ Islam - Ibadhi (Population)         : num [1:2308] 0 0 0 0 0 0 0 0 0 0 ...\n $ Islam - Nation of Islam (Population): num [1:2308] 0 0 0 0 0 0 0 0 0 0 ...\n $ Islam - Alawite (Population)        : num [1:2308] 0 0 0 0 0 0 0 0 0 0 ...\n $ Islam - Ahmadiyya (Population)      : num [1:2308] 0 0 0 0 0 ...\n $ Islam - Others (Population)         : num [1:2308] 0 0 0 0 0 0 0 0 0 0 ...\n $ Islam - Total (Population)          : num [1:2308] 0 0 0 0 0 ...\n $ Islam - Sunni (Percent)             : num [1:2308] 0 0 0 0 0 0 0 0.0106 0.0092 0.0081 ...\n $ Islam - Shi'a (Percent)             : num [1:2308] 0 0 0 0 0 0 0 0.0011 0.0013 0.0011 ...\n $ Islam - Ibadhi (Percent)            : num [1:2308] 0 0 0 0 0 0 0 0 0 0 ...\n $ Islam - Nation of Islam (Percent)   : num [1:2308] 0 0 0 0 0 0 0 0 0 0 ...\n $ Islam - Alawite (Percent)           : num [1:2308] 0 0 0 0 0 0 0 0 0 0 ...\n $ Islam - Ahmadiyya (Percent)         : num [1:2308] 0 0 0 0 0 0 0 0 0 0 ...\n $ Islam - Others (Percent)            : num [1:2308] 0 0 0 0 0 0 0 0 0 0 ...\n $ Islam - Total (Percent)             : num [1:2308] 0 0 0 0 0 0 0 0.0117 0.0104 0.0092 ...\n\n\n\nFix Data Types\n\ndf_islam_correct_type &lt;- df_islam |&gt; \n  mutate(Country = as_factor(Country))\n\n\n\n\nCheck Duplicate\n\ndf_islam_correct_type |&gt; names()\n\n [1] \"Country\"                             \n [2] \"Year\"                                \n [3] \"Islam - Sunni (Population)\"          \n [4] \"Islam - Shi'a (Population)\"          \n [5] \"Islam - Ibadhi (Population)\"         \n [6] \"Islam - Nation of Islam (Population)\"\n [7] \"Islam - Alawite (Population)\"        \n [8] \"Islam - Ahmadiyya (Population)\"      \n [9] \"Islam - Others (Population)\"         \n[10] \"Islam - Total (Population)\"          \n[11] \"Islam - Sunni (Percent)\"             \n[12] \"Islam - Shi'a (Percent)\"             \n[13] \"Islam - Ibadhi (Percent)\"            \n[14] \"Islam - Nation of Islam (Percent)\"   \n[15] \"Islam - Alawite (Percent)\"           \n[16] \"Islam - Ahmadiyya (Percent)\"         \n[17] \"Islam - Others (Percent)\"            \n[18] \"Islam - Total (Percent)\"             \n\n\n\nCheck duplicate Year\n\ndf_islam_correct_type |&gt; \n  get_dupes() |&gt; \n  relocate(dupe_count)\n\nNo variable names specified - using all columns.\n\n\n# A tibble: 626 × 19\n   dupe_count Country  Year `Islam - Sunni (Population)` Islam - Shi'a (Popula…¹\n        &lt;int&gt; &lt;fct&gt;   &lt;dbl&gt;                        &lt;dbl&gt;                   &lt;dbl&gt;\n 1          2 Cuba     1945                            0                       0\n 2          2 Cuba     1945                            0                       0\n 3          2 Cuba     1950                            0                       0\n 4          2 Cuba     1950                            0                       0\n 5          2 Cuba     1955                            0                       0\n 6          2 Cuba     1955                            0                       0\n 7          2 Cuba     1960                            0                       0\n 8          2 Cuba     1960                            0                       0\n 9          2 Cuba     1965                            0                       0\n10          2 Cuba     1965                            0                       0\n# ℹ 616 more rows\n# ℹ abbreviated name: ¹​`Islam - Shi'a (Population)`\n# ℹ 14 more variables: `Islam - Ibadhi (Population)` &lt;dbl&gt;,\n#   `Islam - Nation of Islam (Population)` &lt;dbl&gt;,\n#   `Islam - Alawite (Population)` &lt;dbl&gt;,\n#   `Islam - Ahmadiyya (Population)` &lt;dbl&gt;,\n#   `Islam - Others (Population)` &lt;dbl&gt;, `Islam - Total (Population)` &lt;dbl&gt;, …\n\n\n\ndf_islam_correct_type |&gt;\n  count(Year, Country) |&gt; \n  arrange(desc(n)) |&gt; \n  count(n)\n\nStoring counts in `nn`, as `n` already present in input\nℹ Use `name = \"new_name\"` to pick a new name.\n\n\n# A tibble: 2 × 2\n      n    nn\n  &lt;int&gt; &lt;int&gt;\n1     1  1682\n2     2   313\n\n\n\n\nRemove duplicate using distinct()\n\ndf_islam |&gt; \n  nrow()\n\n[1] 2308\n\n\n\ndf_islam_correct_type |&gt; \n  distinct() |&gt; \n  nrow()\n\n[1] 1995\n\n\n\ndf_islam_removedduplicate &lt;- df_islam_correct_type |&gt; \n  distinct()\n\n\n\n\nNormalize Dataframe\n\nRemove redundant ‘Islam’\n\ndf_islam_columns_renamed &lt;- df_islam_removedduplicate |&gt; \n  rename_with(function(x) str_remove(x, 'Islam - '),\n              starts_with('Islam'))\n\n\n\n\nCheck Missing Value\n\ndf_islam_columns_renamed |&gt; \n  vis_miss()\n\n\n\n\n\n\n\n\n\ndf_islam_replacedna &lt;- df_islam_columns_renamed |&gt; \n  mutate(across(everything(), function(x) replace_na(x, 0)))\n\ndf_islam_replacedna |&gt; \n  vis_miss()\n\n\n\n\n\n\n\n\n\nCheck Zero Distribution (Possible Null Value)\n\ndf_islam_replacedna |&gt;\n  select(!Country) |&gt; \n  vis_expect(function(x) x != 0)\n\n\n\n\n\n\n\n\n\n\nReplace zero with NA\n\ndf_islam_replacedzero &lt;- df_islam_replacedna |&gt; \n  mutate(across(everything(), function(x) if_else(x == 0, NA, x)))\n\n\n\n\nCheck Missing Value (Second Stage)\n\ndf_islam_replacedzero |&gt; \n  miss_var_summary()\n\n# A tibble: 18 × 3\n   variable                     n_miss pct_miss\n   &lt;chr&gt;                         &lt;int&gt;    &lt;num&gt;\n 1 Nation of Islam (Population)   1995    100  \n 2 Ibadhi (Percent)               1995    100  \n 3 Nation of Islam (Percent)      1995    100  \n 4 Ibadhi (Population)            1985     99.5\n 5 Alawite (Population)           1974     98.9\n 6 Alawite (Percent)              1974     98.9\n 7 Ahmadiyya (Percent)            1960     98.2\n 8 Ahmadiyya (Population)         1953     97.9\n 9 Shi'a (Population)             1639     82.2\n10 Shi'a (Percent)                1639     82.2\n11 Sunni (Percent)                1422     71.3\n12 Sunni (Population)             1421     71.2\n13 Others (Percent)                879     44.1\n14 Others (Population)             856     42.9\n15 Total (Percent)                 354     17.7\n16 Total (Population)              329     16.5\n17 Country                           0      0  \n18 Year                              0      0  \n\n\n\ndf_islam_replacedzero |&gt; \n  gg_miss_var()\n\n\n\n\n\n\n\n\n\ndf_islam_replacedzero\n\n# A tibble: 1,995 × 18\n   Country  Year `Sunni (Population)` `Shi'a (Population)` `Ibadhi (Population)`\n   &lt;fct&gt;   &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;                 &lt;dbl&gt;\n 1 United…  1945                   NA                   NA                    NA\n 2 United…  1950                   NA                   NA                    NA\n 3 United…  1955                   NA                   NA                    NA\n 4 United…  1960                   NA                   NA                    NA\n 5 United…  1965                   NA                   NA                    NA\n 6 United…  1970                   NA                   NA                    NA\n 7 United…  1975                   NA                   NA                    NA\n 8 United…  1980              2407464               254121                    NA\n 9 United…  1985              2186675               298183                    NA\n10 United…  1990              2019386               275371                    NA\n# ℹ 1,985 more rows\n# ℹ 13 more variables: `Nation of Islam (Population)` &lt;dbl&gt;,\n#   `Alawite (Population)` &lt;dbl&gt;, `Ahmadiyya (Population)` &lt;dbl&gt;,\n#   `Others (Population)` &lt;dbl&gt;, `Total (Population)` &lt;dbl&gt;,\n#   `Sunni (Percent)` &lt;dbl&gt;, `Shi'a (Percent)` &lt;dbl&gt;, `Ibadhi (Percent)` &lt;dbl&gt;,\n#   `Nation of Islam (Percent)` &lt;dbl&gt;, `Alawite (Percent)` &lt;dbl&gt;,\n#   `Ahmadiyya (Percent)` &lt;dbl&gt;, `Others (Percent)` &lt;dbl&gt;, …\n\n\n\ndf_islam_replacedzero |&gt; \n  gg_miss_upset()\n\n\n\n\n\n\n\n\n\n\nCheck Data Constrains\n\nCheck Country Names\n\nExact Check\n\nvalid_countries &lt;- codelist$country.name.en\ndf_country_only &lt;- df_islam |&gt; \n  distinct(Country)\n\nvalidation_rules_country &lt;- validator(\n  Country %in% valid_countries\n)\n\ncf_country &lt;- confront(df_country_only, validation_rules_country)\n\n\nsummary(cf_country)\n\n  name items passes fails nNA error warning                    expression\n1   V1   200    184    16   0 FALSE   FALSE Country %vin% valid_countries\n\n\n\ninvalid_country &lt;- violating(df_country_only, cf_country) |&gt; \n  arrange(Country) |&gt; \n  pull(Country)\n\ninvalid_country\n\n [1] \"Bosnia and Herzegovina\"           \"Congo\"                           \n [3] \"Czech Republic\"                   \"Democratic Republic of the Congo\"\n [5] \"East Timor\"                       \"Federated States of Micronesia\"  \n [7] \"German Federal Republic\"          \"Ivory Coast\"                     \n [9] \"Macedonia\"                        \"Myanmar\"                         \n[11] \"Sao Tome and Principe\"            \"St. Kitts and Nevis\"             \n[13] \"St. Vincent and the Grenadines\"   \"Swaziland\"                       \n[15] \"Trinidad and Tobago\"              \"United States of America\"        \n\n\n\n\nAutomated Check by countrycode Package\n\ntibble(invalid_country_name = invalid_country,\n       converted_country_name = countryname(invalid_country))\n\n# A tibble: 16 × 2\n   invalid_country_name             converted_country_name          \n   &lt;chr&gt;                            &lt;chr&gt;                           \n 1 Bosnia and Herzegovina           Bosnia & Herzegovina            \n 2 Congo                            Congo - Brazzaville             \n 3 Czech Republic                   Czechia                         \n 4 Democratic Republic of the Congo Congo - Kinshasa                \n 5 East Timor                       Timor-Leste                     \n 6 Federated States of Micronesia   Micronesia (Federated States of)\n 7 German Federal Republic          Germany                         \n 8 Ivory Coast                      Côte d’Ivoire                   \n 9 Macedonia                        North Macedonia                 \n10 Myanmar                          Myanmar (Burma)                 \n11 Sao Tome and Principe            São Tomé & Príncipe             \n12 St. Kitts and Nevis              St. Kitts & Nevis               \n13 St. Vincent and the Grenadines   St. Vincent & Grenadines        \n14 Swaziland                        Eswatini                        \n15 Trinidad and Tobago              Trinidad & Tobago               \n16 United States of America         United States                   \n\n\n\n\nCorrect Country Names\n\ndf_islam_correct_country &lt;- df_islam_replacedzero |&gt; \n  mutate(Country = countryname(Country))\n\n\n\n\nCheck Integers\n\nvalidation_rules_integer &lt;- validator(\n  `Sunni (Population)` %% 1 == 0,\n  `Shi'a (Population)` %% 1 == 0,\n  `Ibadhi (Population)` %% 1 == 0,\n  `Nation of Islam (Population)` %% 1 == 0,\n  `Alawite (Population)` %% 1 == 0,\n  `Ahmadiyya (Population)` %% 1 == 0,\n  `Others (Population)` %% 1 == 0,\n  `Total (Population)` %% 1 == 0\n)\ncf_integer &lt;- confront(df_islam_correct_country, validation_rules_integer)\nsummary(cf_integer)\n\n  name items passes fails  nNA error warning\n1   V1  1995    574     0 1421 FALSE   FALSE\n2   V2  1995    356     0 1639 FALSE   FALSE\n3   V3  1995     10     0 1985 FALSE   FALSE\n4   V4  1995      0     0 1995 FALSE   FALSE\n5   V5  1995     21     0 1974 FALSE   FALSE\n6   V6  1995     42     0 1953 FALSE   FALSE\n7   V7  1995   1139     0  856 FALSE   FALSE\n8   V8  1995   1666     0  329 FALSE   FALSE\n                              expression\n1           `Sunni (Population)`%%1 == 0\n2           `Shi'a (Population)`%%1 == 0\n3          `Ibadhi (Population)`%%1 == 0\n4 `Nation of Islam (Population)`%%1 == 0\n5         `Alawite (Population)`%%1 == 0\n6       `Ahmadiyya (Population)`%%1 == 0\n7          `Others (Population)`%%1 == 0\n8           `Total (Population)`%%1 == 0\n\n\n\n\nCheck Decimals\n\nvalidation_rules_decimal &lt;- validator(\n  `Sunni (Percent)` %% 1 &gt; 0,\n  `Shi'a (Percent)` %% 1 &gt; 0,\n  `Ibadhi (Percent)` %% 1 &gt; 0,\n  `Nation of Islam (Percent)` %% 1 &gt; 0,\n  `Alawite (Percent)` %% 1 &gt; 0,\n  `Ahmadiyya (Percent)` %% 1 &gt; 0,\n  `Others (Percent)` %% 1 &gt; 0,\n  `Total (Percent)` %% 1 &gt; 0\n)\n\ncf_decimal &lt;- confront(df_islam_correct_country,validation_rules_decimal)\nsummary(cf_decimal)\n\n  name items passes fails  nNA error warning                         expression\n1   V1  1995    571     2 1422 FALSE   FALSE           `Sunni (Percent)`%%1 &gt; 0\n2   V2  1995    356     0 1639 FALSE   FALSE           `Shi'a (Percent)`%%1 &gt; 0\n3   V3  1995      0     0 1995 FALSE   FALSE          `Ibadhi (Percent)`%%1 &gt; 0\n4   V4  1995      0     0 1995 FALSE   FALSE `Nation of Islam (Percent)`%%1 &gt; 0\n5   V5  1995     21     0 1974 FALSE   FALSE         `Alawite (Percent)`%%1 &gt; 0\n6   V6  1995     35     0 1960 FALSE   FALSE       `Ahmadiyya (Percent)`%%1 &gt; 0\n7   V7  1995   1116     0  879 FALSE   FALSE          `Others (Percent)`%%1 &gt; 0\n8   V8  1995   1635     6  354 FALSE   FALSE           `Total (Percent)`%%1 &gt; 0\n\n\n\nviolating(df_islam_correct_country |&gt; \n            select(Country, Year, contains('Percent')),\n          cf_decimal)\n\n# A tibble: 6 × 10\n  Country            Year `Sunni (Percent)` `Shi'a (Percent)` `Ibadhi (Percent)`\n  &lt;chr&gt;             &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;\n1 Yemen              2000             0.69               0.31                 NA\n2 Yemen People's R…  1970             1                 NA                    NA\n3 Yemen People's R…  1975             1                 NA                    NA\n4 Qatar              1970             0.9                0.1                  NA\n5 Oman               1970             0.498             NA                    NA\n6 Oman               1975             0.5               NA                    NA\n# ℹ 5 more variables: `Nation of Islam (Percent)` &lt;dbl&gt;,\n#   `Alawite (Percent)` &lt;dbl&gt;, `Ahmadiyya (Percent)` &lt;dbl&gt;,\n#   `Others (Percent)` &lt;dbl&gt;, `Total (Percent)` &lt;dbl&gt;\n\n\n\n\n\nCheck Record Completeness\n\nBy Year\n\ndf_islam_replacedzero |&gt; \n  ggplot(aes(Year)) +\n  geom_line(stat = 'count') + \n  geom_point(stat='count') +\n  scale_x_continuous(breaks = seq(1945, 2010, by=5))\n\n\n\n\n\n\n\n\n\n\nBy Country\n\ndf_islam_replacedzero |&gt; \n  distinct(Country) |&gt; \n  nrow()\n\n[1] 200\n\n\n\n\nCountry Per Year\n\ndf_islam_year_data &lt;- df_islam_replacedzero |&gt; \n  count(Country) |&gt; \n  count(n) |&gt; \n  arrange(desc(nn)) |&gt; \n  rename('Year Data Availibility' = n, 'Total Country' = nn)\n\nStoring counts in `nn`, as `n` already present in input\nℹ Use `name = \"new_name\"` to pick a new name.\n\ndf_islam_year_data |&gt; \n  ggplot(aes(`Year Data Availibility`, `Total Country`)) +\n  geom_col() +\n  scale_x_continuous(breaks = seq(1:14))\n\n\n\n\n\n\n\n\n\nvalidation_rules_year &lt;- validator(\n  contains_at_least( \n      keys = data.frame(Year = seq(1945, 2010, by=5)),\n      by=list(Country) )\n)\n\ncf_year &lt;- confront(df_islam_replacedzero, validation_rules_year)\nsummary(cf_year)\n\n  name items passes fails nNA error warning\n1   V1  1995    868  1127   0 FALSE   FALSE\n                                                                                expression\n1 contains_at_least(keys = data.frame(Year = seq(1945, 2010, by = 5)), by = list(Country))\n\n\n\nincomplete_country &lt;- violating(df_islam_replacedzero, cf_year) |&gt; \n  select(Country, Year)\n\n\nCheck Year Gap\n\nincomplete_country |&gt;\n  group_by(Country) |&gt; \n  mutate(lagged_Year = lag(Year, default = first(Year))) |&gt; \n  mutate(Year_Gap = Year -lagged_Year) |&gt; \n  ggplot(aes(Year_Gap)) +\n  geom_bar() +\n  scale_x_continuous(breaks = seq(0,100, by=5))\n\n\n\n\n\n\n\n\n\n\nAverage Starting Year\n\ndf_islam_replacedzero |&gt; \n  select(Country, Year) |&gt; \n  group_by(Country) |&gt; \n  slice(1) |&gt; \n  ggplot(aes(Year)) +\n  geom_bar() +\n  scale_x_continuous(breaks = seq(1945, 2010, by=5))\n\n\n\n\n\n\n\n\n\n\nAverage Final Year\n\ndf_islam_final_year_count &lt;- df_islam_replacedzero |&gt; \n  select(Country, Year) |&gt; \n  group_by(Country) |&gt; \n  slice_tail() |&gt; \n  ungroup() |&gt; \n  count(Year)\n\ndf_islam_final_year_count\n\n# A tibble: 4 × 2\n   Year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  1970     1\n2  1985     1\n3  1990     4\n4  2010   194\n\n\n\ndf_islam_final_year_count |&gt; \n  ggplot(aes(Year, n)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\nNormalize Dataframe\n\nSeparate Population from Percent columns\n\ndf_islam_population &lt;- df_islam_correct_country |&gt; \n  select(Country, Year, contains('Population')) |&gt; \n  rename_with(function(x) str_remove(x, ' \\\\(Population\\\\)'),\n              contains('Population'))\n\ndf_islam_population\n\n# A tibble: 1,995 × 10\n   Country       Year   Sunni `Shi'a` Ibadhi `Nation of Islam` Alawite Ahmadiyya\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 United Stat…  1945      NA      NA     NA                NA      NA        NA\n 2 United Stat…  1950      NA      NA     NA                NA      NA        NA\n 3 United Stat…  1955      NA      NA     NA                NA      NA        NA\n 4 United Stat…  1960      NA      NA     NA                NA      NA        NA\n 5 United Stat…  1965      NA      NA     NA                NA      NA        NA\n 6 United Stat…  1970      NA      NA     NA                NA      NA        NA\n 7 United Stat…  1975      NA      NA     NA                NA      NA        NA\n 8 United Stat…  1980 2407464  254121     NA                NA      NA      4975\n 9 United Stat…  1985 2186675  298183     NA                NA      NA        NA\n10 United Stat…  1990 2019386  275371     NA                NA      NA        NA\n# ℹ 1,985 more rows\n# ℹ 2 more variables: Others &lt;dbl&gt;, Total &lt;dbl&gt;\n\n\n\ndf_islam_percent &lt;- df_islam_correct_country |&gt; \n  select(Country, Year, contains('Percent')) |&gt; \n  rename_with(function(x) str_remove(x, ' \\\\(Percent\\\\)'),\n              contains('Percent'))\n\ndf_islam_percent\n\n# A tibble: 1,995 × 10\n   Country       Year   Sunni `Shi'a` Ibadhi `Nation of Islam` Alawite Ahmadiyya\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 United Stat…  1945 NA      NA          NA                NA      NA        NA\n 2 United Stat…  1950 NA      NA          NA                NA      NA        NA\n 3 United Stat…  1955 NA      NA          NA                NA      NA        NA\n 4 United Stat…  1960 NA      NA          NA                NA      NA        NA\n 5 United Stat…  1965 NA      NA          NA                NA      NA        NA\n 6 United Stat…  1970 NA      NA          NA                NA      NA        NA\n 7 United Stat…  1975 NA      NA          NA                NA      NA        NA\n 8 United Stat…  1980  0.0106  0.0011     NA                NA      NA        NA\n 9 United Stat…  1985  0.0092  0.0013     NA                NA      NA        NA\n10 United Stat…  1990  0.0081  0.0011     NA                NA      NA        NA\n# ℹ 1,985 more rows\n# ℹ 2 more variables: Others &lt;dbl&gt;, Total &lt;dbl&gt;\n\n\n\n\n\nCheck Sanity\n\nCheck whether Total value equals to all the other columns combined\n\nPopulation\n\ndf_islam_population_total_check &lt;- df_islam_population |&gt; \n  mutate(Total_Recheck = rowSums(across(-c(Country, Year, Total)), na.rm = TRUE)) |&gt; \n  mutate(Total_Recheck = if_else(Total_Recheck == 0, NA, Total_Recheck)) |&gt; \n  relocate(Total, Total_Recheck, .after = Year)\n\nvalidation_rules_total_population &lt;- validator(Total == Total_Recheck)\ncf_total_population &lt;- confront(df_islam_population_total_check, validation_rules_total_population)\nsummary(cf_total_population)\n\n  name items passes fails nNA error warning                          expression\n1   V1  1995   1663     3 329 FALSE   FALSE abs(Total - Total_Recheck) &lt;= 1e-08\n\n\n\nviolating(df_islam_population_total_check, cf_total_population)\n\n# A tibble: 3 × 11\n  Country      Year  Total Total_Recheck  Sunni `Shi'a` Ibadhi `Nation of Islam`\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;\n1 Burkina Fa…  2010 9.02e6       9023118 8.88e6  140256     NA                NA\n2 Kenya        2010 4.20e6       4198089 3.99e6  209904     NA                NA\n3 Tanzania     2010 1.55e7      15505202 1.37e7 1768358     NA                NA\n# ℹ 3 more variables: Alawite &lt;dbl&gt;, Ahmadiyya &lt;dbl&gt;, Others &lt;dbl&gt;\n\n\n\n\nPercent\n\ndf_islam_percent_total_check &lt;- df_islam_percent |&gt; \n  mutate(Total_Recheck = rowSums(across(-c(Country, Year, Total)), na.rm = TRUE)) |&gt; \n  mutate(Total_Recheck = if_else(Total_Recheck == 0, NA, Total_Recheck)) |&gt; \n  relocate(Total, Total_Recheck, .after = Year)\n\nvalidation_rules_total_percent &lt;- validator(Total == Total_Recheck)\ncf_total_percent &lt;- confront(df_islam_percent_total_check, validation_rules_total_percent)\nsummary(cf_total_percent)\n\n  name items passes fails nNA error warning                          expression\n1   V1  1995   1566    75 354 FALSE   FALSE abs(Total - Total_Recheck) &lt;= 1e-08\n\n\n\nviolating(df_islam_percent_total_check, cf_total_percent)\n\n# A tibble: 75 × 11\n   Country    Year  Total Total_Recheck   Sunni `Shi'a` Ibadhi `Nation of Islam`\n   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;\n 1 United S…  1985 0.0104        0.0105  0.0092  0.0013     NA                NA\n 2 Canada     1980 0.0072        0.0071  0.0064  0.0007     NA                NA\n 3 Canada     1995 0.0043        0.0042  0.0038  0.0004     NA                NA\n 4 Suriname   1975 0.196         0.196  NA      NA          NA                NA\n 5 United K…  1980 0.0147        0.0148 NA      NA          NA                NA\n 6 Luxembou…  2010 0.02          0.0201  0.0181  0.002      NA                NA\n 7 France     2000 0.0753        0.0754  0.0669  0.0085     NA                NA\n 8 Germany    2010 0.049         0.0489  0.0355  0.0034     NA                NA\n 9 Yugoslav…  2010 0.031         0.0311  0.0264  0.0047     NA                NA\n10 Sweden     1980 0.001         0.0009 NA      NA          NA                NA\n# ℹ 65 more rows\n# ℹ 3 more variables: Alawite &lt;dbl&gt;, Ahmadiyya &lt;dbl&gt;, Others &lt;dbl&gt;\n\n\n\n\n\n\nCheck Distribution\n\nPopulation\n\n df_islam_population |&gt; \n  plot_histogram(ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPercent\n\ndf_islam_percent |&gt; \n  plot_histogram(ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning Summary\n\nVerified Values\n\nAll Population columns are integers\nAll Percentage columns are decimals\nNo missing year records in-between. For example: 1945, 1950, 1960 (missing 1955)\nTotal Population column is equal to other Population columns combined\nTotal Percent column is equal to other Percent columns combined\n\n\n\nChanges Applied\n\nRemoved 58 irrelevant columns\nCorrected 1 column’s data type\nRemoved 303 duplicate rows\nReplaced all 0 values (71% of all data) with NA\nCorrected 16 countries’ names\nSeparate Population and Percent columns into two dataframes.\nRemove ‘Islam -’ prefix and ‘(Population)’/‘(Percent)’ suffix\n\n\n\nOther findings\n\nExtremely high prevalence of missing values.\nThe majority of countries (60) have complete records from 1945 to 2010. But the rest vary. Ranging from only having 1 to 14 records.\nDespite point 1, nearly all countries have record of 2010, 2005, 2000, and so on. So, the missing records started from the bottom.\n\n\n\n\nNext Steps\nThe plan was to analyze the growth of islam worldwide. However, the prevalenece of missing values are concerning. Most non-null values come from Sunni and Others category. So, it’s better to just use the Total column.\nIf you look at the data quality check at the beginning, Islam is actually the least variable with missing values. Other religions are even more dominated by missing values. This reduces the credibility of the data itself. Although, it wasn’t clearly mentioned what’s the data source anyway.\nI can continue analyzing with just focusing on the overall Islam growth, but I doubt the finding would be trustworthy. For now, let’s convert the latest data frame into csv for later analysis. I’m thinking of uploading it to Tableau for easier exploration.\n\nExport to CSV for further analysis\n\ndf_islam_population |&gt; \n  write_csv('dataset_islam_population.csv')\n\ndf_islam_percent |&gt; \n  write_csv('dataset_islam_percent.csv')"
  },
  {
    "objectID": "posts/g4ds-getting-started-part-2/index.html",
    "href": "posts/g4ds-getting-started-part-2/index.html",
    "title": "Working through G4DS: Getting Started - Part 2",
    "section": "",
    "text": "Continuing from part one, we’ll tackle the exercises from the book ggplot2: Elegant Graphics for Data Analysis (3e), or as I call it, G4DS. A name inspired from the book R4DS\nPreviously we read and comment on the lessons written on the book. But it turned out it made the post so much longer. This time, we’ll focus on the Exercises, and only briefly mention interesting points from the lessons."
  },
  {
    "objectID": "posts/g4ds-getting-started-part-2/index.html#introduction",
    "href": "posts/g4ds-getting-started-part-2/index.html#introduction",
    "title": "Working through G4DS: Getting Started - Part 2",
    "section": "",
    "text": "Continuing from part one, we’ll tackle the exercises from the book ggplot2: Elegant Graphics for Data Analysis (3e), or as I call it, G4DS. A name inspired from the book R4DS\nPreviously we read and comment on the lessons written on the book. But it turned out it made the post so much longer. This time, we’ll focus on the Exercises, and only briefly mention interesting points from the lessons."
  },
  {
    "objectID": "posts/g4ds-getting-started-part-2/index.html#g4ds",
    "href": "posts/g4ds-getting-started-part-2/index.html#g4ds",
    "title": "Working through G4DS: Getting Started - Part 2",
    "section": "G4DS",
    "text": "G4DS\nLet’s load the 2 most useful libraries we might have to use for the exercises.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nIf then we need other libraries, we’ll add that later.\n\nKey Components\nThis part covers the most crucial part of ggplot2 syntax:\n\nEvery ggplot2 plot has three key components:\n\ndata,\nA set of aesthetic mappings between variables in the data and visual properties, and\nAt least one layer which describes how to render each observation. Layers are usually created with a geom function.\n\n\nPretty useful to keep that in mind.\n\nExercises\n\n1. Relationships between cty and hwy\n\nHow would you describe the relationship between cty and hwy? Do you have any concerns about drawing conclusions from that plot?\n\nFirst, let’s inspect the dataset again\n\nmpg \n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\n\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n\nAlright, looks great. Let’s move on to the task. We’ll begin with a scatter plot. I find that very useful for visualizing relationship.\n\nmpg |&gt; \n  ggplot(aes(cty, hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\nQuite a strong relationships. If we were to compute the Pearson’s r, it’ll be at least 0.8.\n\nmpg |&gt; \n  select(cty, hwy) |&gt; \n  cor()\n\n          cty       hwy\ncty 1.0000000 0.9559159\nhwy 0.9559159 1.0000000\n\n\nOhh, nevermind. It’s even close to 1. It makes sense to be so strong since the two variables are very similar, measuring the same metrics (miles per gallon). The difference is the place (city vs highway). We can also see that the cty variable is generally lower than hwy, because there are much more obstacles in a city than in a highway, reducing fuel efficiency.\n\nDo you have any concerns about drawing conclusions from that plot?\n\nI don’t have any. But since it’s been brought up, I guess there must be something. Let me know in the comments if you know what it is.\n\n\n2. Review a scatter plot of model vs manufacturer\n\n\nWhat does ggplot(mpg, aes(model, manufacturer)) + geom_point() show? Is it useful? How could you modify the data to make it more informative?\n\n\nBoth variables are categorical, which aren’t usually visualized with a scatter plot. Since they’re discrete and have limited range of values, the plot will look like lines.\nLet’s see\n\nmpg |&gt; \n  ggplot(aes(model, manufacturer)) + \n  geom_point()\n\n\n\n\n\n\n\n\nOh, my bad. It’s lines. I guess lines only appear when the other variable is numerical. Anyway, it’s true that scatter plot is bad for measuring relationships between two categorical variables. We can’t even see the labels on the x-axis.\nThe first improvement would be to flip the x and y-axis since the x-axis is much wider than the y.\n\nmpg |&gt; \n  ggplot(aes(manufacturer, model)) + \n  geom_point()\n\n\n\n\n\n\n\n\nGreat, at least we can make sense some of the letters now, haha. Okay, i’m stuck here. Another way we could simplify this is to just take the top 5 manufacturers who produces the most amount of models, instead of visualizing all of them at once.\n\nmpg |&gt; \n  distinct(manufacturer, model) |&gt; \n  group_by(manufacturer) |&gt; \n  count() |&gt; \n  arrange(desc(n))\n\n# A tibble: 15 × 2\n# Groups:   manufacturer [15]\n   manufacturer     n\n   &lt;chr&gt;        &lt;int&gt;\n 1 toyota           6\n 2 chevrolet        4\n 3 dodge            4\n 4 ford             4\n 5 volkswagen       4\n 6 audi             3\n 7 nissan           3\n 8 hyundai          2\n 9 subaru           2\n10 honda            1\n11 jeep             1\n12 land rover       1\n13 lincoln          1\n14 mercury          1\n15 pontiac          1\n\n\nSo, the top five are toyota, chevrolet, dodge, ford, and volkswagen. Let’s put that into the filter:\n\nmpg |&gt; \n  filter(manufacturer %in% c('toyota', 'chevrolet', 'dodge', 'ford', 'volkswagen')) |&gt; \n  ggplot(aes(manufacturer, model)) +\n  geom_point()\n\n\n\n\n\n\n\n\nNow it’s much more readable. Since there are overlapping points, geom_count() would be better.\n\nmpg |&gt; \n  filter(manufacturer %in% c('toyota', 'chevrolet', 'dodge', 'ford', 'volkswagen')) |&gt; \n  ggplot(aes(manufacturer, model)) +\n  geom_count()\n\n\n\n\n\n\n\n\nHowever, something feels off. It feels like the visual barely gives you useful information compared to the table we’ve made with dplyr.\nLet’s move on the last question.\n\n\n3. Guess and Describe ggplot plots\n\n\nDescribe the data, aesthetic mappings and layers used for each of the following plots. You’ll need to guess a little because you haven’t seen all the datasets and functions yet, but use your common sense! See if you can predict what the plot will look like before running the code.\n\nggplot(mpg, aes(cty, hwy)) + geom_point()\nggplot(diamonds, aes(carat, price)) + geom_point()\nggplot(economics, aes(date, unemploy)) + geom_line()\nggplot(mpg, aes(cty)) + geom_histogram()\n\n\n\nThis one is relatively easy compared to what we just did.\n\ngeom_point() will produce a scatter plot. We’ve created the exact plot on exercise #1.\nSame plot, with different data.\ngeom_line() is basically like scatter plots, except the dots are connected\ngeom_histogram() will produce a histogram of a single variable, which is why the aes() function only takes one variable."
  },
  {
    "objectID": "posts/g4ds-getting-started-part-2/index.html#closing",
    "href": "posts/g4ds-getting-started-part-2/index.html#closing",
    "title": "Working through G4DS: Getting Started - Part 2",
    "section": "Closing",
    "text": "Closing\nAlright, that concludes part 2, key component. There’s still few key concepts left, we’ll continue that later sometimes in the future. I’d like to do some projects for now.\nLeave a comment down below if you have any feedback. Thanks!\n\nDisclaimer\nAfter writing this part, I looked up scatter plot and discovered that they’re not always bad for categorical variables. There are even dedicated scatter plot for it, such as strip plot, jitter plot, etc. Read more on Categorical Scatter Plots"
  },
  {
    "objectID": "posts/revou-damc-sales/index.html",
    "href": "posts/revou-damc-sales/index.html",
    "title": "Case Study: RevoU DAMC Certification (July 22)",
    "section": "",
    "text": "Recently, I enrolled in a Mini-Course Data Analytics course from RevoU. It’s a basic overview and introduction to the world of Data Analytics. At the end of it, you’ll get a case study you can work on. That case study is the one I’ll be working on today.\nActually, I’ve done a pretty rough analysis and visualization with it, but it was all over the place. So, I want to redo it from scratch, hopefully in a more structured format.\n\n\n\n\n\nThe case study contains a dataset about sales data of a fictional company. The columns are described as follows:\n\nORDERNUMBER: A unique identifier for each sales order.\nQUANTITYORDERED: The number of units ordered for a particular product.\nPRICEEACH: The price of a single unit of the product.\nORDERDATE: The date when the order was placed.\nSTATUS: The current status of the order (e.g., Shipped, Cancelled, On Hold).\nPRODUCTLINE: The category or line to which the product belongs.\nPRODUCTCODE: A unique code identifying the product.\nCUSTOMERNAME: The name of the customer who placed the order.\nCITY: The city where the customer is located.\nDEALSIZE: The size category of the deal (e.g., Small, Medium, Large)\n\n\n\n\nThe task is to find answers to the following questions:\n\n\nWhich product lines have the highest and lowest sales? Create a chart that is representable.\nShow sales performance over time, is there any pattern?\nHow does deal size (small, medium, large) correlate with total sales? What is the percentage of contribution for each type of deal?"
  },
  {
    "objectID": "posts/revou-damc-sales/index.html#introduction",
    "href": "posts/revou-damc-sales/index.html#introduction",
    "title": "Case Study: RevoU DAMC Certification (July 22)",
    "section": "",
    "text": "Recently, I enrolled in a Mini-Course Data Analytics course from RevoU. It’s a basic overview and introduction to the world of Data Analytics. At the end of it, you’ll get a case study you can work on. That case study is the one I’ll be working on today.\nActually, I’ve done a pretty rough analysis and visualization with it, but it was all over the place. So, I want to redo it from scratch, hopefully in a more structured format.\n\n\n\n\n\nThe case study contains a dataset about sales data of a fictional company. The columns are described as follows:\n\nORDERNUMBER: A unique identifier for each sales order.\nQUANTITYORDERED: The number of units ordered for a particular product.\nPRICEEACH: The price of a single unit of the product.\nORDERDATE: The date when the order was placed.\nSTATUS: The current status of the order (e.g., Shipped, Cancelled, On Hold).\nPRODUCTLINE: The category or line to which the product belongs.\nPRODUCTCODE: A unique code identifying the product.\nCUSTOMERNAME: The name of the customer who placed the order.\nCITY: The city where the customer is located.\nDEALSIZE: The size category of the deal (e.g., Small, Medium, Large)\n\n\n\n\nThe task is to find answers to the following questions:\n\n\nWhich product lines have the highest and lowest sales? Create a chart that is representable.\nShow sales performance over time, is there any pattern?\nHow does deal size (small, medium, large) correlate with total sales? What is the percentage of contribution for each type of deal?"
  },
  {
    "objectID": "posts/revou-damc-sales/index.html#get-started",
    "href": "posts/revou-damc-sales/index.html#get-started",
    "title": "Case Study: RevoU DAMC Certification (July 22)",
    "section": "Get Started",
    "text": "Get Started\n\nInspect and Clean Data\nWithout further ado, let’s load the tidyverse!\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nJust to be safe, let’s take look at the first few lines of the csv.\n\nread_lines('./revou_damc_case_study.csv', n_max = 5)\n\n[1] \"ORDERNUMBER,QUANTITYORDERED,PRICEEACH,ORDERDATE,STATUS,PRODUCTLINE,PRODUCTCODE,CUSTOMERNAME,CITY,DEALSIZE\"\n[2] \"10100,30,100,1/6/2003 0:00,Shipped,Vintage Cars,S18_1749,Online Diecast Creations Co.,Nashua,Medium\"      \n[3] \"10100,50,67.8,1/6/2003 0:00,Shipped,Vintage Cars,S18_2248,Online Diecast Creations Co.,Nashua,Medium\"     \n[4] \"10100,22,86.51,1/6/2003 0:00,Shipped,Vintage Cars,S18_4409,Online Diecast Creations Co.,Nashua,Small\"     \n[5] \"10100,49,34.47,1/6/2003 0:00,Shipped,Vintage Cars,S24_3969,Online Diecast Creations Co.,Nashua,Small\"     \n\n\nLooks good enough. Now let’s load the file and take a closer look\n\ndf &lt;- read_csv('./revou_damc_case_study.csv')\n\nRows: 2824 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): ORDERDATE, STATUS, PRODUCTLINE, PRODUCTCODE, CUSTOMERNAME, CITY, DE...\ndbl (3): ORDERNUMBER, QUANTITYORDERED, PRICEEACH\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndf |&gt; str()\n\nspc_tbl_ [2,824 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ORDERNUMBER    : num [1:2824] 10100 10100 10100 10100 10101 ...\n $ QUANTITYORDERED: num [1:2824] 30 50 22 49 25 26 45 46 39 41 ...\n $ PRICEEACH      : num [1:2824] 100 67.8 86.5 34.5 100 ...\n $ ORDERDATE      : chr [1:2824] \"1/6/2003 0:00\" \"1/6/2003 0:00\" \"1/6/2003 0:00\" \"1/6/2003 0:00\" ...\n $ STATUS         : chr [1:2824] \"Shipped\" \"Shipped\" \"Shipped\" \"Shipped\" ...\n $ PRODUCTLINE    : chr [1:2824] \"Vintage Cars\" \"Vintage Cars\" \"Vintage Cars\" \"Vintage Cars\" ...\n $ PRODUCTCODE    : chr [1:2824] \"S18_1749\" \"S18_2248\" \"S18_4409\" \"S24_3969\" ...\n $ CUSTOMERNAME   : chr [1:2824] \"Online Diecast Creations Co.\" \"Online Diecast Creations Co.\" \"Online Diecast Creations Co.\" \"Online Diecast Creations Co.\" ...\n $ CITY           : chr [1:2824] \"Nashua\" \"Nashua\" \"Nashua\" \"Nashua\" ...\n $ DEALSIZE       : chr [1:2824] \"Medium\" \"Medium\" \"Small\" \"Small\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ORDERNUMBER = col_double(),\n  ..   QUANTITYORDERED = col_double(),\n  ..   PRICEEACH = col_double(),\n  ..   ORDERDATE = col_character(),\n  ..   STATUS = col_character(),\n  ..   PRODUCTLINE = col_character(),\n  ..   PRODUCTCODE = col_character(),\n  ..   CUSTOMERNAME = col_character(),\n  ..   CITY = col_character(),\n  ..   DEALSIZE = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\ndf\n\n# A tibble: 2,824 × 10\n   ORDERNUMBER QUANTITYORDERED PRICEEACH ORDERDATE      STATUS  PRODUCTLINE \n         &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;       \n 1       10100              30     100   1/6/2003 0:00  Shipped Vintage Cars\n 2       10100              50      67.8 1/6/2003 0:00  Shipped Vintage Cars\n 3       10100              22      86.5 1/6/2003 0:00  Shipped Vintage Cars\n 4       10100              49      34.5 1/6/2003 0:00  Shipped Vintage Cars\n 5       10101              25     100   1/9/2003 0:00  Shipped Vintage Cars\n 6       10101              26     100   1/9/2003 0:00  Shipped Vintage Cars\n 7       10101              45      31.2 1/9/2003 0:00  Shipped Vintage Cars\n 8       10101              46      53.8 1/9/2003 0:00  Shipped Vintage Cars\n 9       10102              39     100   1/10/2003 0:00 Shipped Vintage Cars\n10       10102              41      50.1 1/10/2003 0:00 Shipped Vintage Cars\n# ℹ 2,814 more rows\n# ℹ 4 more variables: PRODUCTCODE &lt;chr&gt;, CUSTOMERNAME &lt;chr&gt;, CITY &lt;chr&gt;,\n#   DEALSIZE &lt;chr&gt;\n\n\n\nClean Date\nThe first glaring issue is the date. It’s still in &lt;chr&gt;. However, the first few rows have all the same hms (hours, minutes, seconds). So, there’s a possibility it’s the same all the way down. Let’s see…\n\ndf |&gt; \n  separate_wider_delim(ORDERDATE, delim = ' ', names = c('ORDERDATE', 'ORDERHOUR')) |&gt; \n  count(ORDERHOUR)\n\n# A tibble: 1 × 2\n  ORDERHOUR     n\n  &lt;chr&gt;     &lt;int&gt;\n1 0:00       2824\n\n\nI knew it. It’s very unlikely for all orders to be exactly on the same hours. So, the most probable thing is they don’t have data on the hms, so they just insert 0:00 as a placeholder. Since the placeholder is not useful for us, let’s drop it.\n\ndf &lt;- df |&gt; \n  separate_wider_delim(ORDERDATE, delim = ' ', names = c('ORDERDATE', 'ORDERHOUR')) |&gt; \n  select(!ORDERHOUR)\n\nLet’s also parse the date while we’re at it.\n\ndf &lt;- df |&gt; \n  mutate(ORDERDATE = mdy(ORDERDATE))\n\nORDERDATE is likely be one of the key variables in our analysis. So, we’ll frequently work with that\n\n\nClean Price\nSince we’re dealing with sales data, the next important thing is the numbers: price, quantity, revenue. Let’s take a look again at the data.\n\ndf\n\n# A tibble: 2,824 × 10\n   ORDERNUMBER QUANTITYORDERED PRICEEACH ORDERDATE  STATUS  PRODUCTLINE \n         &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;       \n 1       10100              30     100   2003-01-06 Shipped Vintage Cars\n 2       10100              50      67.8 2003-01-06 Shipped Vintage Cars\n 3       10100              22      86.5 2003-01-06 Shipped Vintage Cars\n 4       10100              49      34.5 2003-01-06 Shipped Vintage Cars\n 5       10101              25     100   2003-01-09 Shipped Vintage Cars\n 6       10101              26     100   2003-01-09 Shipped Vintage Cars\n 7       10101              45      31.2 2003-01-09 Shipped Vintage Cars\n 8       10101              46      53.8 2003-01-09 Shipped Vintage Cars\n 9       10102              39     100   2003-01-10 Shipped Vintage Cars\n10       10102              41      50.1 2003-01-10 Shipped Vintage Cars\n# ℹ 2,814 more rows\n# ℹ 4 more variables: PRODUCTCODE &lt;chr&gt;, CUSTOMERNAME &lt;chr&gt;, CITY &lt;chr&gt;,\n#   DEALSIZE &lt;chr&gt;\n\n\nUnfortunately, the case study doesn’t provide any information on the currency. So, let’s assume it’s in USD. Now let’s see the distribution.\n\ndf |&gt; \n  ggplot(aes(PRICEEACH)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe price is indeed unusual. From the first few rows, it even seems that it’s not just about ranging around 100, but it’s exactly 100. Let’s set the binwidth to 1 to prove it.\n\ndf |&gt; \n  ggplot(aes(PRICEEACH)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nYup, exactly. Let’s see it from the table for clearer numbers using count()\n\ndf |&gt; \n  count(PRICEEACH, sort = TRUE)\n\n# A tibble: 1,016 × 2\n   PRICEEACH     n\n       &lt;dbl&gt; &lt;int&gt;\n 1     100    1304\n 2      59.9     6\n 3      96.3     6\n 4      51.9     5\n 5      57.7     5\n 6      62.0     5\n 7      67.1     5\n 8      80.6     5\n 9      89.4     5\n10      90.2     5\n# ℹ 1,006 more rows\n\n\nYeah… I’ll be honest, I have no idea why this could’ve happened, unless the data is fake. (Oh wait, it is!). But in a real world, we can’t do that assumption. We have to be prepared for any unexpected data. So, let’s treat this dataset as a real one even though we know it’s not.\nOkay, so the data is sales data, precisely orders data. So, what we got previously is the distribution of the sales, not the price of the product themselves. So, one explanation of it happened is because the products that are most sought of are the one that has the price of $100.\nNevertheless, how many products are there that are priced at $100? 1? 2? if there’s many products priced at $100, then we can assume there’s nothing special with the products. They’re just the most common.\nLet’s see…\n\ndf |&gt; \n  distinct(PRODUCTCODE, PRICEEACH) |&gt; \n  ggplot(aes(PRICEEACH)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nNow the data is getting weirder. Why would a store prices most of the products at $100?\nAgain, I have no idea. So, if you know something, let me know in the comments. s\n\n\nClean QuantityOrdered\nThe next important variable is QUANTITYORDERED. Let’s see if it also has similar anomaly\n\ndf |&gt; \n  ggplot(aes(QUANTITYORDERED)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nFinally, a normal data! just kidding 😂. Anyway, the distribution looks acceptable. Few outliers there and there, most values are around the median, etc. I’m not familiar with business, but I’m quite certain it doesn’t have the same anomaly as the PRICEEACH.\n\n\n\nAnalyze and Visualize\nThe important variables has been checked. Now it’s time to solve the questions.\n\n1. Product Lines Sales\n\n\nWhich product lines have the highest and lowest sales? Create a chart that is representable.\n\n\nOh, right. Forgot to check the categorical variables. Welp, let’s just deal with it as we go. Anyway, I assume ‘sales’ here mean the quantity of units sold. We can compute that from aggregating the PRODUCTLINE with their QUANITITYORDERED.\n\ndf |&gt; count(PRODUCTLINE, wt = QUANTITYORDERED, sort = TRUE)\n\n# A tibble: 7 × 2\n  PRODUCTLINE          n\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Classic Cars     33992\n2 Vintage Cars     21097\n3 Motorcycles      11663\n4 Trucks and Buses 10777\n5 Planes           10727\n6 Ships             8127\n7 Trains            2712\n\n\nOhh, this is why I love count(). Anyway, the task is to create a chart. So let’s fire up ggplot.\n\ndf |&gt; \n  count(PRODUCTLINE, wt = QUANTITYORDERED) |&gt; \n  ggplot(aes(fct_reorder(PRODUCTLINE, n), n)) +\n  geom_col() +\n  labs(x='Product Lines', y='Total Sales')\n\n\n\n\n\n\n\n\nFrom this, we can see that the lowest sales are from Trains, and the highest is from Classic Cars.\n\n\n2. Sales Performance Pattern\n\n\nShow sales performance over time, is there any pattern?\n\n\nAgain, we need to understand what does Sales Perfomance mean? It can be measured from various metrics, depending on the goals, so we need to guess. Since it’s just a simple case study, let’s go with revenue, which can be obtained from multiplying QUANTITYORDERED and PRICEEACH\nFirst, let’s create a new revenue column\n\ndf &lt;- df |&gt; \n  mutate(revenue = QUANTITYORDERED * PRICEEACH, .after = PRICEEACH)\n\ndf\n\n# A tibble: 2,824 × 11\n   ORDERNUMBER QUANTITYORDERED PRICEEACH revenue ORDERDATE  STATUS  PRODUCTLINE \n         &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;       \n 1       10100              30     100     3000  2003-01-06 Shipped Vintage Cars\n 2       10100              50      67.8   3390  2003-01-06 Shipped Vintage Cars\n 3       10100              22      86.5   1903. 2003-01-06 Shipped Vintage Cars\n 4       10100              49      34.5   1689. 2003-01-06 Shipped Vintage Cars\n 5       10101              25     100     2500  2003-01-09 Shipped Vintage Cars\n 6       10101              26     100     2600  2003-01-09 Shipped Vintage Cars\n 7       10101              45      31.2   1404  2003-01-09 Shipped Vintage Cars\n 8       10101              46      53.8   2473. 2003-01-09 Shipped Vintage Cars\n 9       10102              39     100     3900  2003-01-10 Shipped Vintage Cars\n10       10102              41      50.1   2056. 2003-01-10 Shipped Vintage Cars\n# ℹ 2,814 more rows\n# ℹ 4 more variables: PRODUCTCODE &lt;chr&gt;, CUSTOMERNAME &lt;chr&gt;, CITY &lt;chr&gt;,\n#   DEALSIZE &lt;chr&gt;\n\n\n\nBy Product Lines\nNow, we can compute the total revenue of each PRODUCTLINE per day using count()\n\ndaily_revenue &lt;- df |&gt; \n  count(ORDERDATE, PRODUCTLINE, wt = revenue, name = 'REVENUE')\n\ndaily_revenue\n\n# A tibble: 600 × 3\n   ORDERDATE  PRODUCTLINE      REVENUE\n   &lt;date&gt;     &lt;chr&gt;              &lt;dbl&gt;\n 1 2003-01-06 Vintage Cars       9982.\n 2 2003-01-09 Vintage Cars       8977.\n 3 2003-01-10 Vintage Cars       5956.\n 4 2003-01-29 Classic Cars      11000 \n 5 2003-01-29 Trucks and Buses  21093.\n 6 2003-01-29 Vintage Cars      15793.\n 7 2003-01-31 Classic Cars      18185.\n 8 2003-01-31 Trains             4934.\n 9 2003-01-31 Trucks and Buses  11967.\n10 2003-02-11 Classic Cars      12000 \n# ℹ 590 more rows\n\n\n\ndaily_revenue |&gt; \n  ggplot(aes(ORDERDATE, REVENUE, color=PRODUCTLINE)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWow, there’s too much lines on the same place. Let’s divide them up with facet.\n\ndaily_revenue |&gt; \n  ggplot(aes(ORDERDATE, REVENUE, color=PRODUCTLINE)) +\n  geom_line() +\n  facet_wrap(. ~ PRODUCTLINE, ncol = 1)\n\n\n\n\n\n\n\n\nIt looks better now, still feel too much. The Y axis is barely readeable. Maybe we need to display fewer PRODUCTLINE at a time.\n\n\nAll Sales\nActually, I just realized that it wasn’t asked to group by PRODUCTLINE. So, we could’ve just plot the entire sales. For this, we need to re-aggregate the daily_revenue to get a summarized revenue across all PRODUCTLINE.\n\ndaily_revenue_total &lt;- daily_revenue |&gt; \n  count(ORDERDATE, wt = REVENUE, name = 'total_revenue')\n\n\ndaily_revenue_total |&gt; \n  ggplot(aes(ORDERDATE, total_revenue)) +\n  geom_line() \n\n\n\n\n\n\n\n\nWe can roughly see the peaks in the later parts of the year. We can add a geom_smooth() to help visualize the pattern.\n\ndaily_revenue_total |&gt; \n  ggplot(aes(ORDERDATE, total_revenue)) +\n  geom_line() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nMaybe we can see the pattern better if we divide the plot by year\n\ndaily_revenue_total &lt;- daily_revenue_total |&gt; \n  mutate(year = year(ORDERDATE) |&gt; as_factor())\n\n\ndaily_revenue_total |&gt; \n  ggplot(aes(ORDERDATE, total_revenue)) +\n  geom_line() +\n  facet_wrap(. ~ year, scales = 'free_x', ncol = 1) +\n  scale_x_date(date_labels = '%b',date_breaks = '1 month')\n\n\n\n\n\n\n\n\nNow we got a clearer broad overview of the trend by year. However, the small size of the plot make it a little bit hard to identify the pattern. Let’s zoom in by drawing each plot for each year.\n\ndaily_revenue_total |&gt; \n  filter(year(ORDERDATE) == 2003) |&gt; \n  ggplot(aes(ORDERDATE, total_revenue)) +\n  geom_line() + \n  geom_smooth() +\n  scale_x_date(date_labels = '%b',date_breaks = '1 month') +\n  labs(title = 'Total Sales Revenue per Month in 2003')\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNow we can clearly see the peak on around November - December. Besides, the sales seem to be already going upward starting from September\n\ndaily_revenue_total |&gt; \n  filter(year(ORDERDATE) == 2004) |&gt; \n  ggplot(aes(ORDERDATE, total_revenue)) +\n  geom_line() + \n  geom_smooth() +\n  scale_x_date(date_labels = '%b',date_breaks = '1 month') +\n  labs(title = 'Total Sales Revenue per Month in 2004')\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn 2005, the pattern a bit fluctuates. Yet, there’s still a generate upward trend toward the last months of the year. Moreover, the peaks are the same, which are around November - December\n\ndaily_revenue_total |&gt; \n  filter(year(ORDERDATE) == 2005) |&gt; \n  ggplot(aes(ORDERDATE, total_revenue)) +\n  geom_line() + \n  geom_smooth() +\n  scale_x_date(date_labels = '%b',date_breaks = '1 month') +\n  labs(title = 'Total Sales Revenue per Month in 2005')\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nUnfortunately, in 2005, we’re lacking data toward the later months. So we can’t see the peaks on the same months.\nHowever, based on the pattern of the previous 2 years, we can predict that sales will also increase upward toward the last part of the years, and peak around November - December"
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/EDA_R.html",
    "href": "posts/analyzing-manggarai-train-station/EDA_R.html",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "in Indonesia, especially in Jakarta, train stations are popular methods for transportation. Out of over 50 train stations, the busiest among them is Manggarai, which also act as the main hub for trains from all different routes. If you’re an Indonesian, you know how busy it is during the morning.\nHere is the map of the train route from all stations.  You can see that Manggarai lies at the center of the map, being the hub of all major routes.\nWith a full day schedule from 00:00 to 23:00, let’s uncover what interesting insight we could find.\n\n\n\nThere is an official national train company website, KAI Commuter, where you can query the schedule based on specified departure and destination. The schedule will then be displayed on the page.\n\nWe can then scrape the page to collect the data. However, I found something interesting when I opened up the dev tools and go to the network tab\n\nYou can see it got the data from an api. If you open the request url, you’ll see the full JSON file:\n\nUnfortunately, the trick no longer works. That was a week ago. Now, if you try the same trick, you’ll get a 401 unauthorized access instead.\n\n\n\n\nFirst, we need to transform them to a data frame format, where each key will be its own column. As usual, let’s load up the weapon, tidyverse!\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\n\njsondata &lt;- read_json('./manggarai-schedule.json')\njsondata &lt;- jsondata$data\ndf &lt;- tibble(schedule = jsondata)\ndf &lt;- df |&gt; unnest_wider(schedule)\n\n\n\n\n\n\nThere isn’t any particular objective at hand. I just want to explore the data and find any interesting insights. So, any column might be needed later. However, there are 2 columns that are less likely to be used: train_id and color.\ncolor is obvious. It was used to differentiate the routes visually on KAI Commuterline. However, I’m thinking for using them here for the same purpose as well when drawing plots. So, let’s keep that for now.\ntrain_id is just the identifier of each train. It’s not a useful information for regular customers because any train is virtually the same. Nevertheless, I’ll keep that too, as I’m curious if any particular train stand out.\n\n\n\nIt’s good enough it is, with snake_case and quite descriptive names. But we can make it better by changing a few:\n\n‘ka’ in ka_name stands for ‘Kereta Api’, which means ‘trains’. It looks odd because the rest of the columns use English. So, we can rename it to train_name. Honestly, they don’t feel like names at all, they’re more like lines, so let’s rename them as train_lines\nsince there is dest_time, we can change dest to dest_city.\nwe can rename time_est to est_time for better consistency with dest_time. In fact, the word est itself is not quite clear, so we can rename it further as arrival_time\n‘destination’ would be much clearer than just ‘dest’, so let’s rename all ‘dest’ to ‘destination’\n\n\ndf |&gt; names()\n\n[1] \"train_id\"   \"ka_name\"    \"route_name\" \"dest\"       \"time_est\"  \n[6] \"color\"      \"dest_time\" \n\n\n\ndf_renamed &lt;- df |&gt; \n  rename(train_line = ka_name,\n         destination_city = dest,\n         arrival_time = time_est,\n         destination_time = dest_time)\n\n\ndf_renamed |&gt;  names()\n\n[1] \"train_id\"         \"train_line\"       \"route_name\"       \"destination_city\"\n[5] \"arrival_time\"     \"color\"            \"destination_time\"\n\n\n\n\n\nI noticed earlier that the train lines are a bit too long and redundant\n\ndf_renamed |&gt; \n  count(train_line, name = 'total_train')\n\n# A tibble: 5 × 2\n  train_line                                   total_train\n  &lt;chr&gt;                                              &lt;int&gt;\n1 COMMUTER LINE BOGOR                                  302\n2 COMMUTER LINE BST                                     28\n3 COMMUTER LINE CIKARANG                               211\n4 COMMUTER LINE RANGKASBITUNG                            1\n5 DINAS RANGKAIAN KRL (TIDAK ANGKUT PENUMPANG)           3\n\n\nLet’s cut the Commuter Line.\n\ndf_renamed &lt;- df_renamed |&gt; \n  mutate(train_line = str_remove(train_line, 'COMMUTER LINE '))\n\nDINAS Rangkaian is a special train that most people can’t get in anyway. So let’s remove that.\n\ndf_renamed &lt;- df_renamed |&gt; \n  filter(train_line != \"DINAS RANGKAIAN KRL (TIDAK ANGKUT PENUMPANG)\")\n\n\n\n\nAll the columns are currently in chr data type. So, we need to fix each one of them.\n\ndf_renamed |&gt; str()\n\ntibble [542 × 7] (S3: tbl_df/tbl/data.frame)\n $ train_id        : chr [1:542] \"1448B\" \"5182\" \"1452B\" \"5002\" ...\n $ train_line      : chr [1:542] \"BOGOR\" \"CIKARANG\" \"BOGOR\" \"CIKARANG\" ...\n $ route_name      : chr [1:542] \"JAKARTAKOTA-DEPOK\" \"ANGKE-CIKARANG\" \"JAKARTAKOTA-BOGOR\" \"MANGGARAI-CIKARANG\" ...\n $ destination_city: chr [1:542] \"DEPOK\" \"CIKARANG\" \"BOGOR\" \"CIKARANG\" ...\n $ arrival_time    : chr [1:542] \"00:01:00\" \"00:15:00\" \"00:17:00\" \"04:00:00\" ...\n $ color           : chr [1:542] \"#E30A16\" \"#0084D8\" \"#E30A16\" \"#0084D8\" ...\n $ destination_time: chr [1:542] \"00:34:00\" \"01:05:00\" \"01:17:00\" \"04:50:00\" ...\n\n\n\ntrain_id: It contains both numbers and letters, so let’s leave it as chr\nka_name: this column contains a limited set of labels, so factor would be more appropriate\nroute_name, dest: same reason as ka_name\ntime_est, dest_time: date objects. More specifically, a period object from lubridate\ncolor: kinda tricky. We know there are millions of possible color combinations. But, we also know the train station only uses around 5 of them. Well, let’s count it first. if there’s truly only few, we’ll convert them to factor.\n\n\ndf_renamed |&gt; \n  count(color, name = 'total')\n\n# A tibble: 3 × 2\n  color   total\n  &lt;chr&gt;   &lt;int&gt;\n1 #0084D8   212\n2 #E30A16   302\n3 #F76114    28\n\n\nAhh, that’s even less. Alright, factor it is.\n\ndf_correct_types &lt;- df_renamed |&gt; \n  mutate(train_line = as_factor(train_line),\n         route_name = as_factor(route_name),\n         destination_city = as_factor(destination_city),\n         arrival_time = hms(arrival_time),\n         color = as_factor(color),\n         destination_time = hms(destination_time)\n         )\n\n\ndf_correct_types\n\n# A tibble: 542 × 7\n   train_id train_line route_name            destination_city arrival_time color\n   &lt;chr&gt;    &lt;fct&gt;      &lt;fct&gt;                 &lt;fct&gt;            &lt;Period&gt;     &lt;fct&gt;\n 1 1448B    BOGOR      JAKARTAKOTA-DEPOK     DEPOK            1M 0S        #E30…\n 2 5182     CIKARANG   ANGKE-CIKARANG        CIKARANG         15M 0S       #008…\n 3 1452B    BOGOR      JAKARTAKOTA-BOGOR     BOGOR            17M 0S       #E30…\n 4 5002     CIKARANG   MANGGARAI-CIKARANG    CIKARANG         4H 0M 0S     #008…\n 5 5004     CIKARANG   MANGGARAI-BEKASI      BEKASI           4H 10M 0S    #008…\n 6 5017     CIKARANG   MANGGARAI-KAMPUNGBAN… KAMPUNGBANDAN    4H 10M 0S    #008…\n 7 5008     CIKARANG   MANGGARAI-CIKARANG    CIKARANG         4H 25M 0S    #008…\n 8 1000B    BOGOR      MANGGARAI-BOGOR       BOGOR            4H 33M 0S    #E30…\n 9 5010     CIKARANG   MANGGARAI-BEKASI      BEKASI           4H 36M 0S    #008…\n10 5023     CIKARANG   MANGGARAI-KAMPUNGBAN… KAMPUNGBANDAN    4H 41M 0S    #008…\n# ℹ 532 more rows\n# ℹ 1 more variable: destination_time &lt;Period&gt;\n\n\nAlright, everything looks great. Let’s move on to analysis.\n\n\n\n\nSince there’s no specific objectives, there’s many insights we could look for.\n\n\nThere’s one library that will be helpful to aid our analysis, ggpmisc. it’ll extend our ggplot quite nicely.\n\nlibrary(ggpmisc)\n\nLoading required package: ggpp\n\n\nRegistered S3 methods overwritten by 'ggpp':\n  method                  from   \n  heightDetails.titleGrob ggplot2\n  widthDetails.titleGrob  ggplot2\n\n\n\nAttaching package: 'ggpp'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n\n\n\n\nThe first thing that comes to my mind is finding the peak hours. Knowing this, we can anticipate when is the best time to go to the station.\nAlthough, the ideal way to deduce this is to have the data of the passengers’ traffic: how many that comes every day and every hour. Nevertheless, we could still infer it based on the amount of trains there are at the moment. It’s logical for the company to increases it as the traffic goes up.\nTo get the peak hours, we can count the number of trains per hour and aggregate the sum.\n\ndf_peak_hour &lt;- df_correct_types |&gt; \n  mutate(hour = hour(arrival_time),\n         .after = arrival_time)\n\n\ndf_peak_hour |&gt; \n  count(hour, name = 'total_trains')\n\n# A tibble: 20 × 2\n    hour total_trains\n   &lt;dbl&gt;        &lt;int&gt;\n 1     0            3\n 2     4            9\n 3     5           25\n 4     6           34\n 5     7           34\n 6     8           37\n 7     9           33\n 8    10           31\n 9    11           26\n10    12           28\n11    13           25\n12    14           25\n13    15           31\n14    16           36\n15    17           34\n16    18           32\n17    19           32\n18    20           28\n19    21           24\n20    22           15\n\n\nSo, the peak is on hour 7 and 16, with a total of 37 trains.\n\ndf_peak_hour |&gt; \n  count(hour, name = 'total_trains') |&gt; \n  ggplot(aes(hour, total_trains)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWe can also group it by the lines.\n\ndf_peak_hour |&gt; \n  count(hour, train_line, name = 'total_trains') |&gt; \n  ggplot(aes(hour, total_trains, color = train_line)) +\n  geom_line() + \n  scale_color_manual(values = c(\n    \"BOGOR\" = \"red\",\n    \"BST\" = \"blue\",\n    \"RANGKASBITUNG\" = \"green\",\n    \"CIKARANG\" = \"cyan\"\n    # Add more mappings as needed\n  ))\n\n\n\n\n\n\n\n\nLet’s zoom in on each line\n\n\n\n\nCode\ndf_peak_hour |&gt; \n  count(hour, train_line, name = 'total_train') |&gt; \n  filter(train_line == 'BOGOR') |&gt; \n  ggplot(aes(hour, total_train)) +\n  geom_line(color='red') +\n  stat_peaks() +\n  stat_peaks(\n    geom = 'text',\n    x.label.fmt = 'hour:', position = position_nudge(x=-1.2, y = 1.2)\n  ) +\n  stat_peaks(geom = 'label', position = position_nudge(y=1.2)) +\n  labs(title = 'Bogor Line')\n\n\nWarning in sprintf(fmt, x): one argument not used by format 'hour:'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_peak_hour |&gt; \n  count(hour, train_line, name = 'total_train') |&gt; \n  filter(train_line == 'CIKARANG') |&gt; \n  ggplot(aes(hour, total_train)) +\n  geom_line(color='cyan') +\n  stat_peaks() +\n  stat_peaks(\n    geom = 'text',\n    x.label.fmt = 'hour:', position = position_nudge(x=-1.2, y = 1.2)\n  ) +\n  stat_peaks(geom = 'label', position = position_nudge(y=1.2)) +\n  labs(title = 'Cikarang Line')\n\n\nWarning in sprintf(fmt, x): one argument not used by format 'hour:'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_peak_hour |&gt; \n  count(hour, train_line, name = 'total_train') |&gt; \n  filter(train_line == 'BST') |&gt; \n  ggplot(aes(hour, total_train)) +\n  geom_line(color='blue') +\n  geom_point() +\n  labs(title = 'Soekarno-Hatta Line') +\n  scale_y_continuous(breaks = c(0,1,2)) +\n  scale_x_continuous(breaks = c(0:23))\n\n\n\n\n\n\n\n\n\nLooks like there is no pattern on this line. It’s basically just 1-2 train every hour\n\n\n\n\ndf_peak_hour |&gt; \n  filter(train_line == 'RANGKASBITUNG') |&gt; \n  select(train_line, arrival_time)\n\n# A tibble: 1 × 2\n  train_line    arrival_time\n  &lt;fct&gt;         &lt;Period&gt;    \n1 RANGKASBITUNG 14H 40M 0S  \n\n\nThis one is a bit special. There is only 1 time out of the whole day when this train arrives. Which is at 14:40. It will go straight to Rangkasbitung after arriving in Tanahabang, where you’d usually transit at Tanahabang to switch train.\nHere is the route again if you forget.\n\n\n\n\nThe first peaks happen around 06:00 = 07:00, whcih makes sense because that’s when most people are going to work. The socond peaks happen around 16:00-17:00, which is when most people are going home.\n\n\n\n\nThe next useful insight we could find is the interval between the trains. We could obtain this by lagging the arrival_time by 1, and subtract it from the original arrival_time for all rows.\n\ndf_train_intervals &lt;- df_peak_hour |&gt;\n  mutate(arrival_time_lagged = lag(arrival_time, default = first(arrival_time)), \n         .after = arrival_time)\n\ndf_train_intervals &lt;- df_train_intervals |&gt; \n  mutate(arrival_interval = arrival_time - arrival_time_lagged,\n         .after = arrival_time_lagged)\n\n\ndf_train_intervals |&gt; \n  select(arrival_time, arrival_time_lagged, arrival_interval)\n\n# A tibble: 542 × 3\n   arrival_time arrival_time_lagged arrival_interval\n   &lt;Period&gt;     &lt;Period&gt;            &lt;Period&gt;        \n 1 1M 0S        1M 0S               0S              \n 2 15M 0S       1M 0S               14M 0S          \n 3 17M 0S       15M 0S              2M 0S           \n 4 4H 0M 0S     17M 0S              4H -17M 0S      \n 5 4H 10M 0S    4H 0M 0S            10M 0S          \n 6 4H 10M 0S    4H 10M 0S           0S              \n 7 4H 25M 0S    4H 10M 0S           15M 0S          \n 8 4H 33M 0S    4H 25M 0S           8M 0S           \n 9 4H 36M 0S    4H 33M 0S           3M 0S           \n10 4H 41M 0S    4H 36M 0S           5M 0S           \n# ℹ 532 more rows\n\n\nPerfect. To make use of this column, we need to convert it to a numerical value, like seconds or minutes.\n\ndf_train_intervals &lt;-  df_train_intervals |&gt; \n  mutate(arrival_interval = as.numeric(arrival_interval, 'minutes'))\n\ndf_train_intervals |&gt; \n  select(arrival_time, arrival_time_lagged, arrival_interval)\n\n# A tibble: 542 × 3\n   arrival_time arrival_time_lagged arrival_interval\n   &lt;Period&gt;     &lt;Period&gt;                       &lt;dbl&gt;\n 1 1M 0S        1M 0S                              0\n 2 15M 0S       1M 0S                             14\n 3 17M 0S       15M 0S                             2\n 4 4H 0M 0S     17M 0S                           223\n 5 4H 10M 0S    4H 0M 0S                          10\n 6 4H 10M 0S    4H 10M 0S                          0\n 7 4H 25M 0S    4H 10M 0S                         15\n 8 4H 33M 0S    4H 25M 0S                          8\n 9 4H 36M 0S    4H 33M 0S                          3\n10 4H 41M 0S    4H 36M 0S                          5\n# ℹ 532 more rows\n\n\nNow, we can get to the fun part: analyzing it.\n\ndf_train_intervals |&gt; \n  pull(arrival_interval) |&gt; \n  mean()\n\n[1] 2.52214\n\n\nOn average, there’s a new train every 2.5 minutes. Wow.\nNext, we’ll use group by because combining all routes won’t make much sense unless we want to see how busy the station is, which we already did.\n\ndf_train_intervals &lt;-  df_train_intervals |&gt; \n  group_by(train_line) |&gt; \n  mutate(arrival_time_lagged = lag(arrival_time, default = first(arrival_time))) |&gt;\n  mutate(arrival_interval = as.numeric(arrival_time - arrival_time_lagged, \"minutes\")) |&gt; \n  ungroup()\n\n\ndf_train_intervals |&gt; \n  select(train_line, arrival_time, arrival_time_lagged, arrival_interval)\n\n# A tibble: 542 × 4\n   train_line arrival_time arrival_time_lagged arrival_interval\n   &lt;fct&gt;      &lt;Period&gt;     &lt;Period&gt;                       &lt;dbl&gt;\n 1 BOGOR      1M 0S        1M 0S                              0\n 2 CIKARANG   15M 0S       15M 0S                             0\n 3 BOGOR      17M 0S       1M 0S                             16\n 4 CIKARANG   4H 0M 0S     15M 0S                           225\n 5 CIKARANG   4H 10M 0S    4H 0M 0S                          10\n 6 CIKARANG   4H 10M 0S    4H 10M 0S                          0\n 7 CIKARANG   4H 25M 0S    4H 10M 0S                         15\n 8 BOGOR      4H 33M 0S    17M 0S                           256\n 9 CIKARANG   4H 36M 0S    4H 25M 0S                         11\n10 CIKARANG   4H 41M 0S    4H 36M 0S                          5\n# ℹ 532 more rows\n\n\nNow, let’s find some insight.\n\ndf_train_intervals |&gt; \n  group_by(train_line) |&gt; \n  summarize(arrival_mean = mean(arrival_interval),\n            arrival_mad = mad(arrival_interval))\n\n# A tibble: 4 × 3\n  train_line    arrival_mean arrival_mad\n  &lt;fct&gt;                &lt;dbl&gt;       &lt;dbl&gt;\n1 BOGOR                 4.52        2.97\n2 CIKARANG              6.41        4.45\n3 BST                  35.4         0   \n4 RANGKASBITUNG         0           0   \n\n\nExcept for BST and Rangkasbitung, the interval is very short. This means passengers don’t need to worry about getting late, because the next train will soon arrive in less than 10 minutes.\nUnfortunately, we combined all routes from the same line as if they’re the same routes. In reality, one line can be used for many different routes. So it would be more accurate to compute the arrival_mean and arrival_mad for each route, provided that each passenger would only focus on of these routes.\n\n\n\n\nFirst, let’s sum up what we’ve found so far.\n\n\nHighest spikes of train arrivals occur between on 06:00-07:00 and 16:00-17:00, where people are going to work and going home from work, respectively.\n\n\n\nOn average, new train arrives roughly every 5-10 min.\n\n\n\nRight now, our findings of Train Intervals isn’t helpful, because we combined all the routes as one. So, Peak Hours is our weapon for now. With this, we can say that if you want to avoid peak hours that are extremely crowded, avoid the peak hours. Conversely, if you don’t want to wait for a long time for the next train, come during peak hours.\n\n\n\nMy current analysis is still inadequate and lack depth. To be honest, I find it hard to write an article while doing EDA at the same time in the same place.\nNext time, I’ll first focus entirely on EDA, exploring all the interesting variables that come to mind without writing any explanations. After that, I’ll do a second post, highlighting the key insights I’ve got from the EDA.\nLet’s export the current dataframe to a csv so I don’t need to clean it from scratch.\n\ndf_peak_hour |&gt; \n  write_csv('manggarai-station-schedule-2024.csv')\n\nSee you on the next post!",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/EDA_R.html#introduction",
    "href": "posts/analyzing-manggarai-train-station/EDA_R.html#introduction",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "in Indonesia, especially in Jakarta, train stations are popular methods for transportation. Out of over 50 train stations, the busiest among them is Manggarai, which also act as the main hub for trains from all different routes. If you’re an Indonesian, you know how busy it is during the morning.\nHere is the map of the train route from all stations.  You can see that Manggarai lies at the center of the map, being the hub of all major routes.\nWith a full day schedule from 00:00 to 23:00, let’s uncover what interesting insight we could find.",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/EDA_R.html#collecting-data",
    "href": "posts/analyzing-manggarai-train-station/EDA_R.html#collecting-data",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "There is an official national train company website, KAI Commuter, where you can query the schedule based on specified departure and destination. The schedule will then be displayed on the page.\n\nWe can then scrape the page to collect the data. However, I found something interesting when I opened up the dev tools and go to the network tab\n\nYou can see it got the data from an api. If you open the request url, you’ll see the full JSON file:\n\nUnfortunately, the trick no longer works. That was a week ago. Now, if you try the same trick, you’ll get a 401 unauthorized access instead.",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/EDA_R.html#parsing-data",
    "href": "posts/analyzing-manggarai-train-station/EDA_R.html#parsing-data",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "First, we need to transform them to a data frame format, where each key will be its own column. As usual, let’s load up the weapon, tidyverse!\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\n\njsondata &lt;- read_json('./manggarai-schedule.json')\njsondata &lt;- jsondata$data\ndf &lt;- tibble(schedule = jsondata)\ndf &lt;- df |&gt; unnest_wider(schedule)",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/EDA_R.html#cleaning-data",
    "href": "posts/analyzing-manggarai-train-station/EDA_R.html#cleaning-data",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "There isn’t any particular objective at hand. I just want to explore the data and find any interesting insights. So, any column might be needed later. However, there are 2 columns that are less likely to be used: train_id and color.\ncolor is obvious. It was used to differentiate the routes visually on KAI Commuterline. However, I’m thinking for using them here for the same purpose as well when drawing plots. So, let’s keep that for now.\ntrain_id is just the identifier of each train. It’s not a useful information for regular customers because any train is virtually the same. Nevertheless, I’ll keep that too, as I’m curious if any particular train stand out.\n\n\n\nIt’s good enough it is, with snake_case and quite descriptive names. But we can make it better by changing a few:\n\n‘ka’ in ka_name stands for ‘Kereta Api’, which means ‘trains’. It looks odd because the rest of the columns use English. So, we can rename it to train_name. Honestly, they don’t feel like names at all, they’re more like lines, so let’s rename them as train_lines\nsince there is dest_time, we can change dest to dest_city.\nwe can rename time_est to est_time for better consistency with dest_time. In fact, the word est itself is not quite clear, so we can rename it further as arrival_time\n‘destination’ would be much clearer than just ‘dest’, so let’s rename all ‘dest’ to ‘destination’\n\n\ndf |&gt; names()\n\n[1] \"train_id\"   \"ka_name\"    \"route_name\" \"dest\"       \"time_est\"  \n[6] \"color\"      \"dest_time\" \n\n\n\ndf_renamed &lt;- df |&gt; \n  rename(train_line = ka_name,\n         destination_city = dest,\n         arrival_time = time_est,\n         destination_time = dest_time)\n\n\ndf_renamed |&gt;  names()\n\n[1] \"train_id\"         \"train_line\"       \"route_name\"       \"destination_city\"\n[5] \"arrival_time\"     \"color\"            \"destination_time\"\n\n\n\n\n\nI noticed earlier that the train lines are a bit too long and redundant\n\ndf_renamed |&gt; \n  count(train_line, name = 'total_train')\n\n# A tibble: 5 × 2\n  train_line                                   total_train\n  &lt;chr&gt;                                              &lt;int&gt;\n1 COMMUTER LINE BOGOR                                  302\n2 COMMUTER LINE BST                                     28\n3 COMMUTER LINE CIKARANG                               211\n4 COMMUTER LINE RANGKASBITUNG                            1\n5 DINAS RANGKAIAN KRL (TIDAK ANGKUT PENUMPANG)           3\n\n\nLet’s cut the Commuter Line.\n\ndf_renamed &lt;- df_renamed |&gt; \n  mutate(train_line = str_remove(train_line, 'COMMUTER LINE '))\n\nDINAS Rangkaian is a special train that most people can’t get in anyway. So let’s remove that.\n\ndf_renamed &lt;- df_renamed |&gt; \n  filter(train_line != \"DINAS RANGKAIAN KRL (TIDAK ANGKUT PENUMPANG)\")\n\n\n\n\nAll the columns are currently in chr data type. So, we need to fix each one of them.\n\ndf_renamed |&gt; str()\n\ntibble [542 × 7] (S3: tbl_df/tbl/data.frame)\n $ train_id        : chr [1:542] \"1448B\" \"5182\" \"1452B\" \"5002\" ...\n $ train_line      : chr [1:542] \"BOGOR\" \"CIKARANG\" \"BOGOR\" \"CIKARANG\" ...\n $ route_name      : chr [1:542] \"JAKARTAKOTA-DEPOK\" \"ANGKE-CIKARANG\" \"JAKARTAKOTA-BOGOR\" \"MANGGARAI-CIKARANG\" ...\n $ destination_city: chr [1:542] \"DEPOK\" \"CIKARANG\" \"BOGOR\" \"CIKARANG\" ...\n $ arrival_time    : chr [1:542] \"00:01:00\" \"00:15:00\" \"00:17:00\" \"04:00:00\" ...\n $ color           : chr [1:542] \"#E30A16\" \"#0084D8\" \"#E30A16\" \"#0084D8\" ...\n $ destination_time: chr [1:542] \"00:34:00\" \"01:05:00\" \"01:17:00\" \"04:50:00\" ...\n\n\n\ntrain_id: It contains both numbers and letters, so let’s leave it as chr\nka_name: this column contains a limited set of labels, so factor would be more appropriate\nroute_name, dest: same reason as ka_name\ntime_est, dest_time: date objects. More specifically, a period object from lubridate\ncolor: kinda tricky. We know there are millions of possible color combinations. But, we also know the train station only uses around 5 of them. Well, let’s count it first. if there’s truly only few, we’ll convert them to factor.\n\n\ndf_renamed |&gt; \n  count(color, name = 'total')\n\n# A tibble: 3 × 2\n  color   total\n  &lt;chr&gt;   &lt;int&gt;\n1 #0084D8   212\n2 #E30A16   302\n3 #F76114    28\n\n\nAhh, that’s even less. Alright, factor it is.\n\ndf_correct_types &lt;- df_renamed |&gt; \n  mutate(train_line = as_factor(train_line),\n         route_name = as_factor(route_name),\n         destination_city = as_factor(destination_city),\n         arrival_time = hms(arrival_time),\n         color = as_factor(color),\n         destination_time = hms(destination_time)\n         )\n\n\ndf_correct_types\n\n# A tibble: 542 × 7\n   train_id train_line route_name            destination_city arrival_time color\n   &lt;chr&gt;    &lt;fct&gt;      &lt;fct&gt;                 &lt;fct&gt;            &lt;Period&gt;     &lt;fct&gt;\n 1 1448B    BOGOR      JAKARTAKOTA-DEPOK     DEPOK            1M 0S        #E30…\n 2 5182     CIKARANG   ANGKE-CIKARANG        CIKARANG         15M 0S       #008…\n 3 1452B    BOGOR      JAKARTAKOTA-BOGOR     BOGOR            17M 0S       #E30…\n 4 5002     CIKARANG   MANGGARAI-CIKARANG    CIKARANG         4H 0M 0S     #008…\n 5 5004     CIKARANG   MANGGARAI-BEKASI      BEKASI           4H 10M 0S    #008…\n 6 5017     CIKARANG   MANGGARAI-KAMPUNGBAN… KAMPUNGBANDAN    4H 10M 0S    #008…\n 7 5008     CIKARANG   MANGGARAI-CIKARANG    CIKARANG         4H 25M 0S    #008…\n 8 1000B    BOGOR      MANGGARAI-BOGOR       BOGOR            4H 33M 0S    #E30…\n 9 5010     CIKARANG   MANGGARAI-BEKASI      BEKASI           4H 36M 0S    #008…\n10 5023     CIKARANG   MANGGARAI-KAMPUNGBAN… KAMPUNGBANDAN    4H 41M 0S    #008…\n# ℹ 532 more rows\n# ℹ 1 more variable: destination_time &lt;Period&gt;\n\n\nAlright, everything looks great. Let’s move on to analysis.",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/EDA_R.html#analyzing-data",
    "href": "posts/analyzing-manggarai-train-station/EDA_R.html#analyzing-data",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "Since there’s no specific objectives, there’s many insights we could look for.\n\n\nThere’s one library that will be helpful to aid our analysis, ggpmisc. it’ll extend our ggplot quite nicely.\n\nlibrary(ggpmisc)\n\nLoading required package: ggpp\n\n\nRegistered S3 methods overwritten by 'ggpp':\n  method                  from   \n  heightDetails.titleGrob ggplot2\n  widthDetails.titleGrob  ggplot2\n\n\n\nAttaching package: 'ggpp'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n\n\n\n\nThe first thing that comes to my mind is finding the peak hours. Knowing this, we can anticipate when is the best time to go to the station.\nAlthough, the ideal way to deduce this is to have the data of the passengers’ traffic: how many that comes every day and every hour. Nevertheless, we could still infer it based on the amount of trains there are at the moment. It’s logical for the company to increases it as the traffic goes up.\nTo get the peak hours, we can count the number of trains per hour and aggregate the sum.\n\ndf_peak_hour &lt;- df_correct_types |&gt; \n  mutate(hour = hour(arrival_time),\n         .after = arrival_time)\n\n\ndf_peak_hour |&gt; \n  count(hour, name = 'total_trains')\n\n# A tibble: 20 × 2\n    hour total_trains\n   &lt;dbl&gt;        &lt;int&gt;\n 1     0            3\n 2     4            9\n 3     5           25\n 4     6           34\n 5     7           34\n 6     8           37\n 7     9           33\n 8    10           31\n 9    11           26\n10    12           28\n11    13           25\n12    14           25\n13    15           31\n14    16           36\n15    17           34\n16    18           32\n17    19           32\n18    20           28\n19    21           24\n20    22           15\n\n\nSo, the peak is on hour 7 and 16, with a total of 37 trains.\n\ndf_peak_hour |&gt; \n  count(hour, name = 'total_trains') |&gt; \n  ggplot(aes(hour, total_trains)) +\n  geom_line()\n\n\n\n\n\n\n\n\nWe can also group it by the lines.\n\ndf_peak_hour |&gt; \n  count(hour, train_line, name = 'total_trains') |&gt; \n  ggplot(aes(hour, total_trains, color = train_line)) +\n  geom_line() + \n  scale_color_manual(values = c(\n    \"BOGOR\" = \"red\",\n    \"BST\" = \"blue\",\n    \"RANGKASBITUNG\" = \"green\",\n    \"CIKARANG\" = \"cyan\"\n    # Add more mappings as needed\n  ))\n\n\n\n\n\n\n\n\nLet’s zoom in on each line\n\n\n\n\nCode\ndf_peak_hour |&gt; \n  count(hour, train_line, name = 'total_train') |&gt; \n  filter(train_line == 'BOGOR') |&gt; \n  ggplot(aes(hour, total_train)) +\n  geom_line(color='red') +\n  stat_peaks() +\n  stat_peaks(\n    geom = 'text',\n    x.label.fmt = 'hour:', position = position_nudge(x=-1.2, y = 1.2)\n  ) +\n  stat_peaks(geom = 'label', position = position_nudge(y=1.2)) +\n  labs(title = 'Bogor Line')\n\n\nWarning in sprintf(fmt, x): one argument not used by format 'hour:'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_peak_hour |&gt; \n  count(hour, train_line, name = 'total_train') |&gt; \n  filter(train_line == 'CIKARANG') |&gt; \n  ggplot(aes(hour, total_train)) +\n  geom_line(color='cyan') +\n  stat_peaks() +\n  stat_peaks(\n    geom = 'text',\n    x.label.fmt = 'hour:', position = position_nudge(x=-1.2, y = 1.2)\n  ) +\n  stat_peaks(geom = 'label', position = position_nudge(y=1.2)) +\n  labs(title = 'Cikarang Line')\n\n\nWarning in sprintf(fmt, x): one argument not used by format 'hour:'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_peak_hour |&gt; \n  count(hour, train_line, name = 'total_train') |&gt; \n  filter(train_line == 'BST') |&gt; \n  ggplot(aes(hour, total_train)) +\n  geom_line(color='blue') +\n  geom_point() +\n  labs(title = 'Soekarno-Hatta Line') +\n  scale_y_continuous(breaks = c(0,1,2)) +\n  scale_x_continuous(breaks = c(0:23))\n\n\n\n\n\n\n\n\n\nLooks like there is no pattern on this line. It’s basically just 1-2 train every hour\n\n\n\n\ndf_peak_hour |&gt; \n  filter(train_line == 'RANGKASBITUNG') |&gt; \n  select(train_line, arrival_time)\n\n# A tibble: 1 × 2\n  train_line    arrival_time\n  &lt;fct&gt;         &lt;Period&gt;    \n1 RANGKASBITUNG 14H 40M 0S  \n\n\nThis one is a bit special. There is only 1 time out of the whole day when this train arrives. Which is at 14:40. It will go straight to Rangkasbitung after arriving in Tanahabang, where you’d usually transit at Tanahabang to switch train.\nHere is the route again if you forget.\n\n\n\n\nThe first peaks happen around 06:00 = 07:00, whcih makes sense because that’s when most people are going to work. The socond peaks happen around 16:00-17:00, which is when most people are going home.\n\n\n\n\nThe next useful insight we could find is the interval between the trains. We could obtain this by lagging the arrival_time by 1, and subtract it from the original arrival_time for all rows.\n\ndf_train_intervals &lt;- df_peak_hour |&gt;\n  mutate(arrival_time_lagged = lag(arrival_time, default = first(arrival_time)), \n         .after = arrival_time)\n\ndf_train_intervals &lt;- df_train_intervals |&gt; \n  mutate(arrival_interval = arrival_time - arrival_time_lagged,\n         .after = arrival_time_lagged)\n\n\ndf_train_intervals |&gt; \n  select(arrival_time, arrival_time_lagged, arrival_interval)\n\n# A tibble: 542 × 3\n   arrival_time arrival_time_lagged arrival_interval\n   &lt;Period&gt;     &lt;Period&gt;            &lt;Period&gt;        \n 1 1M 0S        1M 0S               0S              \n 2 15M 0S       1M 0S               14M 0S          \n 3 17M 0S       15M 0S              2M 0S           \n 4 4H 0M 0S     17M 0S              4H -17M 0S      \n 5 4H 10M 0S    4H 0M 0S            10M 0S          \n 6 4H 10M 0S    4H 10M 0S           0S              \n 7 4H 25M 0S    4H 10M 0S           15M 0S          \n 8 4H 33M 0S    4H 25M 0S           8M 0S           \n 9 4H 36M 0S    4H 33M 0S           3M 0S           \n10 4H 41M 0S    4H 36M 0S           5M 0S           \n# ℹ 532 more rows\n\n\nPerfect. To make use of this column, we need to convert it to a numerical value, like seconds or minutes.\n\ndf_train_intervals &lt;-  df_train_intervals |&gt; \n  mutate(arrival_interval = as.numeric(arrival_interval, 'minutes'))\n\ndf_train_intervals |&gt; \n  select(arrival_time, arrival_time_lagged, arrival_interval)\n\n# A tibble: 542 × 3\n   arrival_time arrival_time_lagged arrival_interval\n   &lt;Period&gt;     &lt;Period&gt;                       &lt;dbl&gt;\n 1 1M 0S        1M 0S                              0\n 2 15M 0S       1M 0S                             14\n 3 17M 0S       15M 0S                             2\n 4 4H 0M 0S     17M 0S                           223\n 5 4H 10M 0S    4H 0M 0S                          10\n 6 4H 10M 0S    4H 10M 0S                          0\n 7 4H 25M 0S    4H 10M 0S                         15\n 8 4H 33M 0S    4H 25M 0S                          8\n 9 4H 36M 0S    4H 33M 0S                          3\n10 4H 41M 0S    4H 36M 0S                          5\n# ℹ 532 more rows\n\n\nNow, we can get to the fun part: analyzing it.\n\ndf_train_intervals |&gt; \n  pull(arrival_interval) |&gt; \n  mean()\n\n[1] 2.52214\n\n\nOn average, there’s a new train every 2.5 minutes. Wow.\nNext, we’ll use group by because combining all routes won’t make much sense unless we want to see how busy the station is, which we already did.\n\ndf_train_intervals &lt;-  df_train_intervals |&gt; \n  group_by(train_line) |&gt; \n  mutate(arrival_time_lagged = lag(arrival_time, default = first(arrival_time))) |&gt;\n  mutate(arrival_interval = as.numeric(arrival_time - arrival_time_lagged, \"minutes\")) |&gt; \n  ungroup()\n\n\ndf_train_intervals |&gt; \n  select(train_line, arrival_time, arrival_time_lagged, arrival_interval)\n\n# A tibble: 542 × 4\n   train_line arrival_time arrival_time_lagged arrival_interval\n   &lt;fct&gt;      &lt;Period&gt;     &lt;Period&gt;                       &lt;dbl&gt;\n 1 BOGOR      1M 0S        1M 0S                              0\n 2 CIKARANG   15M 0S       15M 0S                             0\n 3 BOGOR      17M 0S       1M 0S                             16\n 4 CIKARANG   4H 0M 0S     15M 0S                           225\n 5 CIKARANG   4H 10M 0S    4H 0M 0S                          10\n 6 CIKARANG   4H 10M 0S    4H 10M 0S                          0\n 7 CIKARANG   4H 25M 0S    4H 10M 0S                         15\n 8 BOGOR      4H 33M 0S    17M 0S                           256\n 9 CIKARANG   4H 36M 0S    4H 25M 0S                         11\n10 CIKARANG   4H 41M 0S    4H 36M 0S                          5\n# ℹ 532 more rows\n\n\nNow, let’s find some insight.\n\ndf_train_intervals |&gt; \n  group_by(train_line) |&gt; \n  summarize(arrival_mean = mean(arrival_interval),\n            arrival_mad = mad(arrival_interval))\n\n# A tibble: 4 × 3\n  train_line    arrival_mean arrival_mad\n  &lt;fct&gt;                &lt;dbl&gt;       &lt;dbl&gt;\n1 BOGOR                 4.52        2.97\n2 CIKARANG              6.41        4.45\n3 BST                  35.4         0   \n4 RANGKASBITUNG         0           0   \n\n\nExcept for BST and Rangkasbitung, the interval is very short. This means passengers don’t need to worry about getting late, because the next train will soon arrive in less than 10 minutes.\nUnfortunately, we combined all routes from the same line as if they’re the same routes. In reality, one line can be used for many different routes. So it would be more accurate to compute the arrival_mean and arrival_mad for each route, provided that each passenger would only focus on of these routes.",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/analyzing-manggarai-train-station/EDA_R.html#temporary-conclusion-and-recommendation",
    "href": "posts/analyzing-manggarai-train-station/EDA_R.html#temporary-conclusion-and-recommendation",
    "title": "Analyzing Indonesia’s Busiest Train Station: Manggarai",
    "section": "",
    "text": "First, let’s sum up what we’ve found so far.\n\n\nHighest spikes of train arrivals occur between on 06:00-07:00 and 16:00-17:00, where people are going to work and going home from work, respectively.\n\n\n\nOn average, new train arrives roughly every 5-10 min.\n\n\n\nRight now, our findings of Train Intervals isn’t helpful, because we combined all the routes as one. So, Peak Hours is our weapon for now. With this, we can say that if you want to avoid peak hours that are extremely crowded, avoid the peak hours. Conversely, if you don’t want to wait for a long time for the next train, come during peak hours.\n\n\n\nMy current analysis is still inadequate and lack depth. To be honest, I find it hard to write an article while doing EDA at the same time in the same place.\nNext time, I’ll first focus entirely on EDA, exploring all the interesting variables that come to mind without writing any explanations. After that, I’ll do a second post, highlighting the key insights I’ve got from the EDA.\nLet’s export the current dataframe to a csv so I don’t need to clean it from scratch.\n\ndf_peak_hour |&gt; \n  write_csv('manggarai-station-schedule-2024.csv')\n\nSee you on the next post!",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/index.html",
    "href": "posts/imdb-top-10000-movies/index.html",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDb",
    "section": "",
    "text": "The goal of this analysis is to find common characteristics among the most popular movies. These characteristics would be helpful to potential movie producers who want to identify trends and patterns that contribute to a film’s success.\n\n\n\n\n\nThe data was collected from the IMDb Non-Commercial Dataset, which is freely available to the public.\n\n\n\nThe dataset is given as a list of .tsv files. To process that, I set up a local SQLite database with appropriate tables and imported the .tsv into them.\nThe dataset contains a comprehensive amount of data, with a total of 39 columns that are spread across 7 tables. For this analysis, I selected 2 tables that are most relevant and joined them together: title.basics and title.ratings.\nTo get the most popular movies, I sorted the movies in descending order based on the number of votes, as it should be positively correlated with the amount of people that watched the movie.\nFinally, I took the top 10.000 rows and exported them into a .csv file.\nThe csv file is then loaded into R to check for data quality and for further analysis. The data is already clean. The data reflects what the live site shows, with 0.01% null values, no duplicates, no misspellings, and so on.\n\n\n\nThe analysis is also performed using R in RStudio, with the help of tidyverse libraries such as dplyr and ggplot2. The majority of the analysis is identifying relationships among the number of votes and the potential key variables using statistics and visualization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe most popular movies have moderate duration, which is 80-120 minutes, and very few of them are below 80 minutes. This suggests that most audience prefer movies with long enough duration to build depth, characters, plots, and other aspects; But not too long.\n\n\n\n\n\n\n\n\n\n\n\n\nEach movie can have multiple genres. The most popular ones are the one that has Drama, Comedy, and Romance altogether. This suggests that audience prefer movies that are easy to relate.\n\n\n\n\n\n\n\n\n\n\n\n\nIf we count each genre separately, we’ll see that Drama dominated the genre competition across top 10.000 movies. A possible explanation is because it’s one of the most flexible element. It can be applied to any kind of movies, and will relate to anyone across times and cultures.\n\n\n\n\n\nAim for a movie duration between 80-120 minutes: The data shows that 73% of the most popular movies fall within this range. This suggests audiences prefer movies of moderate length.\nFocus on key genre combinations: Consider producing movies that combine Comedy, Drama, and Romance, as this was the most common genre combination. Alternatively, pure Drama movies or Comedy-Drama combinations were also highly popular.\nPrioritize Drama, Comedy, and Action genres: When considering individual genres, these three were the most common among popular movies. Producers might want to ensure their projects incorporate at least one of these genres.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/index.html#objective",
    "href": "posts/imdb-top-10000-movies/index.html#objective",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDb",
    "section": "",
    "text": "The goal of this analysis is to find common characteristics among the most popular movies. These characteristics would be helpful to potential movie producers who want to identify trends and patterns that contribute to a film’s success.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/index.html#methodology",
    "href": "posts/imdb-top-10000-movies/index.html#methodology",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDb",
    "section": "",
    "text": "The data was collected from the IMDb Non-Commercial Dataset, which is freely available to the public.\n\n\n\nThe dataset is given as a list of .tsv files. To process that, I set up a local SQLite database with appropriate tables and imported the .tsv into them.\nThe dataset contains a comprehensive amount of data, with a total of 39 columns that are spread across 7 tables. For this analysis, I selected 2 tables that are most relevant and joined them together: title.basics and title.ratings.\nTo get the most popular movies, I sorted the movies in descending order based on the number of votes, as it should be positively correlated with the amount of people that watched the movie.\nFinally, I took the top 10.000 rows and exported them into a .csv file.\nThe csv file is then loaded into R to check for data quality and for further analysis. The data is already clean. The data reflects what the live site shows, with 0.01% null values, no duplicates, no misspellings, and so on.\n\n\n\nThe analysis is also performed using R in RStudio, with the help of tidyverse libraries such as dplyr and ggplot2. The majority of the analysis is identifying relationships among the number of votes and the potential key variables using statistics and visualization.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/index.html#results",
    "href": "posts/imdb-top-10000-movies/index.html#results",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDb",
    "section": "",
    "text": "The most popular movies have moderate duration, which is 80-120 minutes, and very few of them are below 80 minutes. This suggests that most audience prefer movies with long enough duration to build depth, characters, plots, and other aspects; But not too long.\n\n\n\n\n\n\n\n\n\n\n\n\nEach movie can have multiple genres. The most popular ones are the one that has Drama, Comedy, and Romance altogether. This suggests that audience prefer movies that are easy to relate.\n\n\n\n\n\n\n\n\n\n\n\n\nIf we count each genre separately, we’ll see that Drama dominated the genre competition across top 10.000 movies. A possible explanation is because it’s one of the most flexible element. It can be applied to any kind of movies, and will relate to anyone across times and cultures.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/index.html#recommendations",
    "href": "posts/imdb-top-10000-movies/index.html#recommendations",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDb",
    "section": "",
    "text": "Aim for a movie duration between 80-120 minutes: The data shows that 73% of the most popular movies fall within this range. This suggests audiences prefer movies of moderate length.\nFocus on key genre combinations: Consider producing movies that combine Comedy, Drama, and Romance, as this was the most common genre combination. Alternatively, pure Drama movies or Comedy-Drama combinations were also highly popular.\nPrioritize Drama, Comedy, and Action genres: When considering individual genres, these three were the most common among popular movies. Producers might want to ensure their projects incorporate at least one of these genres.",
    "crumbs": [
      "Executive Summary"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/EDA_R.html",
    "href": "posts/imdb-top-10000-movies/EDA_R.html",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDB",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(naniar)\nlibrary(summarytools)\n\nWarning: no DISPLAY variable so Tk is not available\n\n\nsystem might not have X11 capabilities; in case of errors when using dfSummary(), set st_options(use.x11 = FALSE)\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(DataExplorer)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n\n\n\nupdate_geom_defaults(\"bar\", list(fill = 'grey', color = 'black'))\n\n\n\n\n\ndf &lt;- read_csv('top_10_000_movies.csv', na = '\\\\N')\n\nRows: 10000 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tconst, primaryTitle, genres\ndbl (4): startYear, runtimeMinutes, averageRating, numVotes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\ndf\n\n# A tibble: 10,000 × 7\n   tconst    primaryTitle genres startYear runtimeMinutes averageRating numVotes\n   &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 tt0111161 The Shawsha… Drama       1994            142           9.3  2933616\n 2 tt0468569 The Dark Kn… Actio…      2008            152           9    2914022\n 3 tt1375666 Inception    Actio…      2010            148           8.8  2587592\n 4 tt0137523 Fight Club   Drama       1999            139           8.8  2365113\n 5 tt0109830 Forrest Gump Drama…      1994            142           8.8  2294287\n 6 tt0110912 Pulp Fiction Crime…      1994            154           8.9  2253218\n 7 tt0816692 Interstellar Adven…      2014            169           8.7  2149979\n 8 tt0133093 The Matrix   Actio…      1999            136           8.7  2083320\n 9 tt0068646 The Godfath… Crime…      1972            175           9.2  2044687\n10 tt0120737 The Lord of… Actio…      2001            178           8.9  2035850\n# ℹ 9,990 more rows\n\n\n\n\n\ndf |&gt; \n  vis_miss()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  plot_histogram(ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  ggplot(aes(averageRating)) +\n  geom_histogram(binwidth = 1) +\n  scale_x_continuous(breaks = seq(0,10,1))\n\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  ggplot(aes(runtimeMinutes)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  ggplot(aes(runtimeMinutes)) +\n  geom_histogram() +\n  ylim(0,20)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  pull(numVotes) |&gt; \n  cut(breaks = seq(1, 3000000, 100000), dig.lab = 50, right = FALSE, include.lowest = TRUE) |&gt; \n  tabyl() |&gt; \n  as.tibble() |&gt; \n  rename(range = 1) |&gt; \n  mutate(percent = label_percent(accuracy=0.001)(percent)) |&gt; \n  select(-valid_percent)\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n\n# A tibble: 30 × 3\n   range                n percent\n   &lt;fct&gt;            &lt;int&gt; &lt;chr&gt;  \n 1 [1,100001)        7749 77.490%\n 2 [100001,200001)   1130 11.300%\n 3 [200001,300001)    470 4.700% \n 4 [300001,400001)    209 2.090% \n 5 [400001,500001)    131 1.310% \n 6 [500001,600001)     73 0.730% \n 7 [600001,700001)     62 0.620% \n 8 [700001,800001)     50 0.500% \n 9 [800001,900001)     37 0.370% \n10 [900001,1000001)    21 0.210% \n# ℹ 20 more rows\n\n  # mutate(percent = label_percent(accuracy=0.01)(percent))\n  # mutate(range = format(range, big.mark = ','))\n\n\ndf |&gt; \n  ggplot(aes(numVotes)) +\n  geom_histogram() +\n  ylim(c(0, 100)) +\n  scale_x_continuous(labels = comma)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  ggplot(aes(startYear)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- df |&gt; \n  mutate(genres = fct_rev(fct_infreq(genres)))\n\n\ndf |&gt; \n  count(genres) |&gt; \n  arrange(desc(n)) |&gt;\n  slice_head(n = 10) |&gt; \n  ggplot(aes(n, genres)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\ndf_unnested_genre &lt;- df |&gt; \n  mutate(genres = str_split(genres, ',')) |&gt; \n  unnest_longer(genres) |&gt; \n  mutate(genres = fct_rev(fct_infreq(genres)))\n\n\ndf_unnested_genre |&gt; \n  count(genres) |&gt; \n  arrange(desc(n)) |&gt; \n  slice_head(n = 10) |&gt; \n  ggplot(aes(genres, n)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  ggplot(aes(runtimeMinutes)) +\n  geom_histogram(aes(fill = runtimeMinutes &gt;= 80 & runtimeMinutes &lt;= 120), \n                 binwidth = 40, boundary = 0) +\n  scale_x_continuous(breaks = seq(0, 240, 40)) +\n  scale_fill_manual(values = c(\"FALSE\" = \"grey\", \"TRUE\" = \"red\"),\n                    labels = c(\"FALSE\" = \"Other Intervals\", \"TRUE\" = \"Most Frequent Runtime Interval\")) +\n  labs(fill = \"\") +\n  theme(legend.position = \"bottom\")\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  pull(runtimeMinutes) |&gt; \n  cut(breaks = seq(1, 600, 40), dig.lab = 50) |&gt; \n  tabyl() |&gt; \n  as.tibble() |&gt; \n  rename(range = 1) |&gt; \n  mutate(percent = label_percent(accuracy=0.01)(percent)) |&gt; \n  select(-valid_percent)\n\n# A tibble: 15 × 3\n   range         n percent\n   &lt;fct&gt;     &lt;int&gt; &lt;chr&gt;  \n 1 (1,41]        0 0.00%  \n 2 (41,81]     281 2.81%  \n 3 (81,121]   7393 73.93% \n 4 (121,161]  2006 20.06% \n 5 (161,201]   280 2.80%  \n 6 (201,241]    26 0.26%  \n 7 (241,281]     4 0.04%  \n 8 (281,321]     2 0.02%  \n 9 (321,361]     0 0.00%  \n10 (361,401]     1 0.01%  \n11 (401,441]     1 0.01%  \n12 (441,481]     1 0.01%  \n13 (481,521]     0 0.00%  \n14 (521,561]     0 0.00%  \n15 &lt;NA&gt;          5 0.05%  \n\n\n\n\n\n\ndf |&gt; \n  count(genres) |&gt; \n  arrange(desc(n)) |&gt;\n  mutate(is_top3 = ifelse(row_number() &lt;= 3, \"Top 3\", \"Others\")) |&gt;  # Flag top 5\n  slice_head(n = 10) |&gt; \n  ggplot(aes(n, genres, fill = is_top3)) +  # Use fill aesthetic to differentiate\n  geom_col() +\n  scale_fill_manual(values = c(\"Top 3\" = \"#FFA07A\", \"Others\" = \"grey\")) +  # Highlight top 5\n  theme_minimal() +\n  labs(fill = \"Group\")  # Add legend\n\n\n\n\n\n\n\n\n\n\n\n\ndf_unnested_genre |&gt; \n  count(genres) |&gt; \n  arrange(desc(n)) |&gt; \n  slice_head(n = 10) |&gt; \n  mutate(rank = if_else(row_number() &lt;= 3, 'Top_3', 'Others')) |&gt; \n  ggplot(aes(genres, n, fill = rank)) +\n  geom_col() +\n  scale_fill_manual(values = c('Top_' = '#FFA07A', 'Others' = 'grey')) +\n  coord_flip()",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/EDA_R.html#setup",
    "href": "posts/imdb-top-10000-movies/EDA_R.html#setup",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDB",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(naniar)\nlibrary(summarytools)\n\nWarning: no DISPLAY variable so Tk is not available\n\n\nsystem might not have X11 capabilities; in case of errors when using dfSummary(), set st_options(use.x11 = FALSE)\n\nAttaching package: 'summarytools'\n\nThe following object is masked from 'package:tibble':\n\n    view\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(DataExplorer)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n\n\n\nupdate_geom_defaults(\"bar\", list(fill = 'grey', color = 'black'))\n\n\n\n\n\ndf &lt;- read_csv('top_10_000_movies.csv', na = '\\\\N')\n\nRows: 10000 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): tconst, primaryTitle, genres\ndbl (4): startYear, runtimeMinutes, averageRating, numVotes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/EDA_R.html#clean-data",
    "href": "posts/imdb-top-10000-movies/EDA_R.html#clean-data",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDB",
    "section": "",
    "text": "df\n\n# A tibble: 10,000 × 7\n   tconst    primaryTitle genres startYear runtimeMinutes averageRating numVotes\n   &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 tt0111161 The Shawsha… Drama       1994            142           9.3  2933616\n 2 tt0468569 The Dark Kn… Actio…      2008            152           9    2914022\n 3 tt1375666 Inception    Actio…      2010            148           8.8  2587592\n 4 tt0137523 Fight Club   Drama       1999            139           8.8  2365113\n 5 tt0109830 Forrest Gump Drama…      1994            142           8.8  2294287\n 6 tt0110912 Pulp Fiction Crime…      1994            154           8.9  2253218\n 7 tt0816692 Interstellar Adven…      2014            169           8.7  2149979\n 8 tt0133093 The Matrix   Actio…      1999            136           8.7  2083320\n 9 tt0068646 The Godfath… Crime…      1972            175           9.2  2044687\n10 tt0120737 The Lord of… Actio…      2001            178           8.9  2035850\n# ℹ 9,990 more rows\n\n\n\n\n\ndf |&gt; \n  vis_miss()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  plot_histogram(ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  ggplot(aes(averageRating)) +\n  geom_histogram(binwidth = 1) +\n  scale_x_continuous(breaks = seq(0,10,1))\n\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  ggplot(aes(runtimeMinutes)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  ggplot(aes(runtimeMinutes)) +\n  geom_histogram() +\n  ylim(0,20)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 8 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  pull(numVotes) |&gt; \n  cut(breaks = seq(1, 3000000, 100000), dig.lab = 50, right = FALSE, include.lowest = TRUE) |&gt; \n  tabyl() |&gt; \n  as.tibble() |&gt; \n  rename(range = 1) |&gt; \n  mutate(percent = label_percent(accuracy=0.001)(percent)) |&gt; \n  select(-valid_percent)\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n\n# A tibble: 30 × 3\n   range                n percent\n   &lt;fct&gt;            &lt;int&gt; &lt;chr&gt;  \n 1 [1,100001)        7749 77.490%\n 2 [100001,200001)   1130 11.300%\n 3 [200001,300001)    470 4.700% \n 4 [300001,400001)    209 2.090% \n 5 [400001,500001)    131 1.310% \n 6 [500001,600001)     73 0.730% \n 7 [600001,700001)     62 0.620% \n 8 [700001,800001)     50 0.500% \n 9 [800001,900001)     37 0.370% \n10 [900001,1000001)    21 0.210% \n# ℹ 20 more rows\n\n  # mutate(percent = label_percent(accuracy=0.01)(percent))\n  # mutate(range = format(range, big.mark = ','))\n\n\ndf |&gt; \n  ggplot(aes(numVotes)) +\n  geom_histogram() +\n  ylim(c(0, 100)) +\n  scale_x_continuous(labels = comma)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  ggplot(aes(startYear)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf &lt;- df |&gt; \n  mutate(genres = fct_rev(fct_infreq(genres)))\n\n\ndf |&gt; \n  count(genres) |&gt; \n  arrange(desc(n)) |&gt;\n  slice_head(n = 10) |&gt; \n  ggplot(aes(n, genres)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\ndf_unnested_genre &lt;- df |&gt; \n  mutate(genres = str_split(genres, ',')) |&gt; \n  unnest_longer(genres) |&gt; \n  mutate(genres = fct_rev(fct_infreq(genres)))\n\n\ndf_unnested_genre |&gt; \n  count(genres) |&gt; \n  arrange(desc(n)) |&gt; \n  slice_head(n = 10) |&gt; \n  ggplot(aes(genres, n)) +\n  geom_col() +\n  coord_flip()",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  },
  {
    "objectID": "posts/imdb-top-10000-movies/EDA_R.html#analyze-data",
    "href": "posts/imdb-top-10000-movies/EDA_R.html#analyze-data",
    "title": "Analyzing Top 10.000 Most Popular Movies on IMDB",
    "section": "",
    "text": "df |&gt; \n  ggplot(aes(runtimeMinutes)) +\n  geom_histogram(aes(fill = runtimeMinutes &gt;= 80 & runtimeMinutes &lt;= 120), \n                 binwidth = 40, boundary = 0) +\n  scale_x_continuous(breaks = seq(0, 240, 40)) +\n  scale_fill_manual(values = c(\"FALSE\" = \"grey\", \"TRUE\" = \"red\"),\n                    labels = c(\"FALSE\" = \"Other Intervals\", \"TRUE\" = \"Most Frequent Runtime Interval\")) +\n  labs(fill = \"\") +\n  theme(legend.position = \"bottom\")\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  pull(runtimeMinutes) |&gt; \n  cut(breaks = seq(1, 600, 40), dig.lab = 50) |&gt; \n  tabyl() |&gt; \n  as.tibble() |&gt; \n  rename(range = 1) |&gt; \n  mutate(percent = label_percent(accuracy=0.01)(percent)) |&gt; \n  select(-valid_percent)\n\n# A tibble: 15 × 3\n   range         n percent\n   &lt;fct&gt;     &lt;int&gt; &lt;chr&gt;  \n 1 (1,41]        0 0.00%  \n 2 (41,81]     281 2.81%  \n 3 (81,121]   7393 73.93% \n 4 (121,161]  2006 20.06% \n 5 (161,201]   280 2.80%  \n 6 (201,241]    26 0.26%  \n 7 (241,281]     4 0.04%  \n 8 (281,321]     2 0.02%  \n 9 (321,361]     0 0.00%  \n10 (361,401]     1 0.01%  \n11 (401,441]     1 0.01%  \n12 (441,481]     1 0.01%  \n13 (481,521]     0 0.00%  \n14 (521,561]     0 0.00%  \n15 &lt;NA&gt;          5 0.05%  \n\n\n\n\n\n\ndf |&gt; \n  count(genres) |&gt; \n  arrange(desc(n)) |&gt;\n  mutate(is_top3 = ifelse(row_number() &lt;= 3, \"Top 3\", \"Others\")) |&gt;  # Flag top 5\n  slice_head(n = 10) |&gt; \n  ggplot(aes(n, genres, fill = is_top3)) +  # Use fill aesthetic to differentiate\n  geom_col() +\n  scale_fill_manual(values = c(\"Top 3\" = \"#FFA07A\", \"Others\" = \"grey\")) +  # Highlight top 5\n  theme_minimal() +\n  labs(fill = \"Group\")  # Add legend\n\n\n\n\n\n\n\n\n\n\n\n\ndf_unnested_genre |&gt; \n  count(genres) |&gt; \n  arrange(desc(n)) |&gt; \n  slice_head(n = 10) |&gt; \n  mutate(rank = if_else(row_number() &lt;= 3, 'Top_3', 'Others')) |&gt; \n  ggplot(aes(genres, n, fill = rank)) +\n  geom_col() +\n  scale_fill_manual(values = c('Top_' = '#FFA07A', 'Others' = 'grey')) +\n  coord_flip()",
    "crumbs": [
      "Source Code",
      "Exploratory Data Analysis (EDA) with R"
    ]
  }
]